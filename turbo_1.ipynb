{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO with TuRBO-1 and TS/qEI\n",
    "\n",
    "In this tutorial, we show how to implement Trust Region Bayesian Optimization (TuRBO) [1] in a closed loop in BoTorch.\n",
    "\n",
    "This implementation uses one trust region (TuRBO-1) and supports either parallel expected improvement (qEI) or Thompson sampling (TS). We optimize the $10D$ Ackley function on the domain $[-10, 15]^{10}$ and show that TuRBO-1 outperforms qEI as well as Sobol.\n",
    "\n",
    "Since botorch assumes a maximization problem, we will attempt to maximize $-f(x)$ to achieve $\\max_x -f(x)=0$.\n",
    "\n",
    "[1]: [Eriksson, David, et al. Scalable global optimization via local Bayesian optimization. Advances in Neural Information Processing Systems. 2019](https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install torch -y\n",
    "#! conda install botorch -c pytorch -c gpytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.models import FixedNoiseGP, SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import HorseshoePrior\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "#other packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "dtype=torch.float \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the 10-dimensional Ackley function\n",
    "\n",
    "The goal is to minimize the popular Ackley function:\n",
    "\n",
    "$f(x_1,\\ldots,x_d) = -20\\exp\\left(-0.2 \\sqrt{\\frac{1}{d} \\sum_{j=1}^d x_j^2} \\right) -\\exp \\left( \\frac{1}{d} \\sum_{j=1}^d \\cos(2 \\pi x_j) \\right) + 20 + e$\n",
    "\n",
    "over the domain  $[-10, 15]^{10}$.  The global optimal value of $0$ is attained at $x_1 = \\ldots = x_d = 0$.\n",
    "\n",
    "As mentioned above, since botorch assumes a maximization problem, we instead maximize $-f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = Ackley(dim=10, negate=True).to(dtype=dtype, device=device)\n",
    "fun.bounds[0, :].fill_(-10)\n",
    "fun.bounds[1, :].fill_(15)\n",
    "dim = fun.dim\n",
    "lb, ub = fun.bounds\n",
    "\n",
    "\n",
    "#def eval_objective(x):\n",
    "#    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "#    return fun(unnormalize(x, fun.bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10., -10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
       "        [ 15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4560])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0,1, size = (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD4CAYAAADIH9xYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaz0lEQVR4nO3de5RdZX3/8feHSQIk4SoQQhK5aFRA5RYCFReSQjBQJaB2/UB+iKJOsaCAXT8by2op1brwBuqSGgPE4lKglYukmIaE/CqoFEhCAyQETIgoQ0Iid7kUmJlv/zg7eDg9M/ucOXvvzN7zebH2Ovv+fU4SvvPMs5/9PIoIzMxs+NtmaxfAzMxa44RtZlYSTthmZiXhhG1mVhJO2GZmJTEq7wB3TvxwId1QDvnKW4oIA8A2h88sLNbzF3y5sFhnPjyusFjXf35KIXH6f7exkDgAbDu6sFCXXbNdYbHe9UpxPclOfuIadXqP155c33KBR++2X8fxiuQatplZSeRewzYzK1R/39YuQW6csM2sWvp6t3YJcuOEbWaVEtG/tYuQGydsM6uWfidsM7NycA3bzKwk/NDRzKwkRnINW9I7gNnAJCCADcCCiFiTc9nMzNoWFe4lMuiLM5L+GrgOEHAPsCxZv1bSnPyLZ2bWpv7+1peSSathfxI4MCJeq98p6VJgNXBJs4skdQPdAF/Y8RBmj903g6KambWgwk0iaa+m9wN7Ndk/MTnWVETMi4hpETHNydrMCtXf1/pSMmk17POBpZLWAo8l+94MvBU4N8dymZkNTYVr2IMm7IhYJOltwHRqDx0F9ADLIqJ8P57MrPoq/NAxtZdI1N7zvKuAspiZda6EDxNb5X7YZlYpVf7l3wnbzKplpLZhm5mVjptEzMxKwjVsM7OS6Hst/ZyScsI2s2pxk8jQHfSZYmZ37ntoXSFxANh518JCFTmT+Q1fPbywWBd98deFxDn5leJmMv/BtsXNaT2V4ib7fv/3DyosViYybBKRNAv4NtAFXBkRlzQc/3/A6cnmKGB/YPeIeFrSo8AfgD6gNyKmdVoe17DNrFoyqmFL6gIuB2aSvDAoaUFEPLjlnIj4OvD15PwPAhdExNN1t5kREU9mUiCcsM2sarJrEpkOrIuI9QCSrqM21PSDA5x/GnBtVsGbKe53ODOzAkTfay0vkrolLa9buutuNYk/jqEEtVr2pGYxJY0FZgE31BcFWCxpRcN9h8w1bDOrljbasCNiHjBvgMPNHhTEAOd+EPhVQ3PIURGxQdIewBJJD0XEHS0XrgnXsM2sWrKbwKAHmFK3PZnajFvNnEpDc0hEbEg+NwM3UWti6YgTtplVS/S3vgxuGTBV0r6SxlBLygsaT5K0E/A+4Oa6feMk7bBlHTgeWNXpV3OTiJlVS0YPHSOiV9K5wK3UuvXNj4jVks5Ojs9NTj0FWBwRL9ZdPgG4SRLU8uw1EbGo0zI5YZtZtWTYDzsiFgILG/bNbdj+Z+CfG/atBzLvwO6EbWbV0lvdCQyG3IYt6RNZFsTMLBPZtWEPO508dLx4oAP1fRvnLy/wlXEzs+x6iQw7gzaJSLp/oEPUGtWbqu/b+OI/nD5Qv0Uzs+yVsObcqrQ27AnA+4FnGvYLuDOXEpmZdaKENedWpSXsW4DxEbGy8YCkn+dRIDOzjozUGnZEfHKQYx/NvjhmZh2qcC8Rd+szs2qJ6j42c8I2s2oZwW3YZmbl4oRtZlYSI/Who5lZ6fT1be0S5Cb3hD3qw2flHeJ1r3zna4XEOfLsnxYSB2D29vsVFosddyks1KT+rkLinPbq2kLiAKw4uLg/v7f9cqBhmbM34y9fTD8pI4fPzuAmbhIZ/opK1mY2zDlhm5mVhNuwzczKIfrdD9vMrBzcJGJmVhLuJWJmVhIVrmF71nQzq5YMJzCQNEvSw5LWSZrT5Pgxkp6TtDJZ/q7Va4fCNWwzq5aMBn+S1AVcDswEeoBlkhZExIMNp/4iIj4wxGvb4hq2mVVLdjXs6cC6iFgfEa8C1wGtvtrTybUDSk3Ykt4h6VhJ4xv2z+o0uJlZ5vqj5aV+/tlk6a670yTgsbrtnmRfoz+RdJ+kf5d0YJvXtiVtTsfPAecAa4CrJJ0XETcnh78CLOq0AGZmmWqjl0j9/LNNqNklDdv3AntHxAuSTgR+Ckxt8dq2pdWwPw0cFhEnA8cAfyvpvORYswLVDtT91LryJ7d0WkYzs5ZFf3/LS4oeYErd9mTgDYO4RMTzEfFCsr4QGC1pt1auHYq0h45ddYV5VNIxwPWS9maQhF3/U+uV1Uur+9qRmQ0/2b3puAyYKmlf4HHgVOANUyNK2hPYFBEhaTq1SvBTwLNp1w5FWsJ+QtLBWybhTar9HwDmA+/qNLiZWeYyGkskInolnQvcCnQB8yNitaSzk+NzgY8An5HUC7wMnBoRATS9ttMypSXsjwFvmNEyInqBj0n6fqfBzcwyl+FYIkkzx8KGfXPr1r8LfLfVazuVNmt6zyDHfpVlQczMMtHrV9PNzMrBw6uamZWEh1c1MyuHFrrrlZYTtplVi2vYZmYl4YQ9dIuOvSrvEABMGb1dIXEAzh2ze2GxdnuluH98Hzr71sJivdj/aiFxHjh5QiFxALa78B8Li7Xz9L8oLNZ7n1peWKxXsriJJzAwMysHz+loZlYWTthmZiXhXiJmZiXhGraZWUk4YZuZlUP0uUnEzKwcXMM2MysHd+szMyuLkZywk2lvIiKWSToAmAU8lAzObWY2vFS3CTt11vSLgBOAUZKWAEcAPwfmSDokIpq+i5tMFd8N8JkdDuf9Y9+aaaHNzAYSvdXN2Gk17I8ABwPbAk8AkyPieUlfB+4Gmibs+kl4b97zo9X9/cTMhp/q5mu2STneGxF9EfES8EhEPA8QES9T6T8WMyur6I+WlzSSZkl6WNI6SXOaHD9d0v3Jcqekg+qOPSrpAUkrJWUyglZaDftVSWOThH1YXUF2wgnbzIajjDKTpC7gcmAm0AMsk7QgIh6sO+03wPsi4hlJJ1BrWTii7viMiHgymxKlJ+yjI+IVgIg3TJQ2Gjgzq0KYmWUlw25904F1EbEeQNJ1wGzg9YQdEXfWnX8XMDmr4M0M2iSyJVk32f9kRDyQT5HMzDrQ3/oiqVvS8rqlu+5Ok4DH6rZ7kn0D+STw73XbASyWtKLhvkPmfthmVinR28a5dR0kmlCzS5qeKM2glrDfW7f7qIjYIGkPYImkhyLijtZL97+lPXQ0MyuV6G99SdEDTKnbngxsaDxJ0ruBK4HZEfHU6+WI2JB8bgZuotbE0hEnbDOrljaaRFIsA6ZK2lfSGOBUYEH9CZLeDNwInBERv67bP07SDlvWgeOBVZ1+NTeJmFmltFBzbu0+Eb2SzgVuBbqA+RGxWtLZyfG5wN8BbwL+SRLUukJPAyYANyX7RgHXRMSiTsvkhG1mlZJVwgZIhuBY2LBvbt36p4BPNbluPXBQ4/5O5Z6wf7Tti3mHAOCaZd8oJA7AO194prBYY99xSmGxNs0sbgiBFzaOLiTOtn99USFxAL414zuFxVo5e7fCYn126X6FxcpC9DV7VlgNrmGbWaVkWcMebpywzaxSot81bDOzUnAN28ysJCJcwzYzKwXXsM3MSqLfvUTMzMrBDx3NzEqiygm77bFEJP0wj4KYmWUhovWlbNIm4V3QuAuYIWlngIg4KadymZkNSZVr2GlNIpOpza5wJbVxYAVMA7452EX1s6Yfuuu72W/8Ph0X1MysFVXu1pfWJDINWAFcCDwXET8HXo6I2yPi9oEuioh5ETEtIqY5WZtZkfr61PJSNoPWsJN5HC+T9JPkc1PaNWZmW1OVa9gtJd+I6AH+XNKfAc/nWyQzs6EbyW3YbxARPwN+llNZzMw6VsbeH61y84aZVYpr2GZmJdHXX92pap2wzaxSqtwkUt0fRWY2IvWHWl7SSJol6WFJ6yTNaXJckr6THL9f0qGtXjsUTthmVikRankZjKQu4HLgBOAA4DRJBzScdgIwNVm6ge+1cW3bnLDNrFIyHEtkOrAuItZHxKvAdcDshnNmAz+MmruAnSVNbPHatuXehn3L5pV5hwDgsWPPKSQOwJf/sGNhsRbvclRhsW68b0xhsb7y8gOFxHnwFz8tJA7AZy8/NP2kjGin3QuLdfGKco331kpTxxb1w2gk5kXEvGR9EvBY3bEe4IiGWzQ7Z1KL17bNDx3NrFLa6SWSJOd5Axxulvkb6+UDndPKtW1zwjazSsmwk0gPMKVuezKwocVzxrRwbdvchm1mlZJhL5FlwFRJ+0oaA5wKNA45vQD4WNJb5Ehqg+RtbPHatrmGbWaVktXgTxHRK+lc4FagC5gfEaslnZ0cnwssBE4E1gEvAZ8Y7NpOy+SEbWaVkuWk6RGxkFpSrt83t249gKY9Hppd2yknbDOrlGj6vK8anLDNrFJ6R/p42GZmZeEadkLSe6m9wbMqIhbnUyQzs6HLsg17uBm0W5+ke+rWPw18F9gBuCirwUzMzLIUqOWlbNL6YY+uW+8GZkbExcDxwOkDXSSpW9JySct7e1/IoJhmZq3pb2Mpm7QmkW0k7UItsSsifg8QES9K6h3oovrXPbfffu8Kj05rZsNNXwlrzq1KS9g7ASuovRcfkvaMiCckjaf5u/JmZltVhWcIGzxhR8Q+AxzqB07JvDRmZh3qr3Bdckjd+iLiJeA3GZfFzKxjVW6DdT9sM6uUMj5MbJUTtplVSr/cJGJmVgp9W7sAOXLCNrNKGbG9RMzMysa9RDrQH8U8Ath/7Romjd+tkFjf7dq1kDgA/11YJPirZ+4sLNameQO+KJupwz+/pJA4APO2mVRYrGOevruwWJtOemthsbLgXiIlUFSyNrPhzU0iZmYl4W59ZmYl0VfhGrZnTTezSilqtD5Ju0paImlt8rlLk3OmSPoPSWskrZZ0Xt2xv5f0uKSVyXJiWkwnbDOrlAKHV50DLI2IqcDSZLtRL/BXEbE/cCRwjqQD6o5fFhEHJ0vqhL1O2GZWKaHWlw7NBq5O1q8GTv5fZYnYGBH3Jut/ANYAQ+5O5IRtZpXSTg27frKVZOluI9SEiNgItcQM7DHYyZL2AQ4B6vtknivpfknzmzWpNPJDRzOrlHZeTa+fbKUZSbcBezY5dGE7ZUrmELgBOD8ink92fw/4ErWu418CvgmcNdh9nLDNrFKy7IcdEccNdEzSJkkTI2KjpInA5gHOG00tWf84Im6su/emunOuAG5JK0/aJLxHSNoxWd9e0sWS/k3SVyXtlHZzM7OiFfjQcQFwZrJ+JnBz4wmSBFwFrImISxuOTazbPAVYlRYwrQ17PvBSsv5talOGfTXZ94O0m5uZFa3AhH0JMFPSWmBmso2kvSRt6fFxFHAG8KdNuu99TdIDku4HZgAXpAVMnYQ3IrZMtjstIg5N1n8paeVAFyUN990AXaN2pqtrfFo5zMwyUdRYIhHxFHBsk/0bgBOT9V8ywPy3EXFGuzHTatirJH0iWb9P0jQASW8DXhvoooiYFxHTImKak7WZFalfrS9lk5awPwW8T9IjwAHAf0paD1yRHDMzG1b62ljKJm3W9OeAj0vaAdgvOb+n/ummmdlw0l/hAVZb6taXvKFzX85lMTPrmEfrMzMrierWr52wzaxiXMM2MyuJXlW3ju2EbWaVUt107YRtZhXjJpEO/Gq3w/IOAcCBc48uJA7A0/+YOkZLZo5Z91xhsZ5c+2+FxZqy/4cLibP+C9MLiQMw4/LfFhbr2W+cVFislxevLixWFkZ8tz4zs7Kobrp2wjazinGTiJlZSfRVuI7thG1mleIatplZSYRr2GZm5eAatplZSbhbn5lZSVQ3XTthm1nF9FY4ZafNmv45SVOKKoyZWaeijf86IWlXSUskrU0+dxngvEeTyXZXSlre7vX10qYI+xJwt6RfSPpLSbu3+EW6JS2XtPzGFx9t5RIzs0wUOGv6HGBpREwFlibbA5kREQdHxLQhXg+kJ+z1wGRqifsw4EFJiySdmUwb1lT9JLwfGrdPWhnMzDJTVA0bmA1cnaxfDZyc9/VpCTsioj8iFkfEJ4G9gH8CZlFL5mZmw0o7Nez61oBk6W4j1ISI2AiQfO4xwHkBLJa0ouH+rV7/urSHjm+YCD4iXgMWAAskbZ92czOzovVF6zXniJgHzBvouKTbgD2bHLqwjSIdFREbJO0BLJH0UETc0cb1r0tL2P9noAMR8fJQApqZ5SnLftgRcdxAxyRtkjQxIjZKmghsHuAeG5LPzZJuAqYDdwAtXV9v0CaRiPh12g3MzIaTAtuwFwBnJutnAjc3niBp3JbnfZLGAccDq1q9vlFaG7aZWakU2EvkEmCmpLXAzGQbSXtJWpicMwH4paT7gHuAn0XEosGuH4xfnDGzSinq1fSIeAo4tsn+DcCJyfp64KB2rh+ME7aZVYpH6zMzK4l2eomUjRO2mVWKR+vrwFuOeDbvEABc/Zl7C4kDcNYvvlZYrAfH71pYrCUH/k1hsc7feVr6SRl4ZkFPIXEAbv9WW82RHel6zymFxdrhg68UFisLHg/bzKwk3IZtZlYSbhIxMyuJ8ENHM7Ny6HMN28ysHNwkYmZWEm4SMTMrCdewzcxKYsR265M0BjgV2BARt0n6KPAeYA0wL5nQwMxs2BjJr6b/IDlnrKQzgfHAjdRGmJrOH8dyNTMbFkZyk8i7IuLdkkYBjwN7RUSfpB8B9w10UTJvWTfApYdM5eP77pVZgc3MBjOSE/Y2SbPIOGAssBPwNLAtMHqgi+rnSXvmw8dU90/PzIadkdxL5CrgIaCL2qSTP5G0HjgSuC7nspmZtW3E1rAj4jJJ/5Ksb5D0Q+A44IqIuKeIApqZtWPE9hKBP874m6w/C1yfZ4HMzDrRF9UdYNWT8JpZpUREy0snJO0qaYmktcnnLk3OebuklXXL85LOT479vaTH646dmBbTCdvMKqWfaHnp0BxgaURMBZYm228QEQ9HxMERcTBwGPAScFPdKZdtOR4RCxuvb+SEbWaVEm3816HZwNXJ+tXAySnnHws8EhG/HWpAJ2wzq5T+iJYXSd2Sltct3W2EmhARGwGSzz1Szj8VuLZh37mS7pc0v1mTSiOPJWJmldJOzbn+nZFmJN0G7Nnk0IXtlCl5n+Uk4It1u78HfAmI5PObwFmD3ccJ28wqJcteIhFx3EDHJG2SNDEiNkqaCGwe5FYnAPdGxKa6e7++LukK4Ja08uSesMccNCnvEAC8f92m9JMycviRnyss1qv9vYXFuveKjxQW67TTv19InPe+ckghcQB2e8u7C4s15cA/LyzW3uMmFBbrng23d3yP/uLedFxAbTylS5LPmwc59zQamkO2JPtk8xRgVVpAt2GbWaUU+NDxEmCmpLXAzGQbSXtJer3Hh6SxyfEbG67/mqQHJN0PzAAuSAvoJhEzq5SiatgR8RS1nh+N+zcAJ9ZtvwS8qcl5Z7Qb0wnbzCplRL+abmZWJn3Rt7WLkBsnbDOrlJE8vKqZWamM2OFVzczKxjVsM7OSKLAfduFSE7akt1Dr1D0F6AXWAtdGxHM5l83MrG1V7iUy6Iszkj4HzAW2Aw4HtqeWuP9T0jF5F87MrF190d/yUjZpbzp+GpgVEV+mNjXYARFxITALuGygi+pHwJq/fF12pTUzS1HUBAZbQyuvpm9pNtkW2AEgIn5HyqzpETEtIqadNe2tnZfSzKxF7QyvWjZpbdhXAssk3QUcDXwVQNLuwNM5l83MrG1lrDm3Km3W9G8n48HuD1waEQ8l+39PLYGbmQ0rI7ofdkSsBlYXUBYzs46N2Bq2mVnZlLH3R6ucsM2sUsr4MLFVTthmViluEjEzK4kqv+nohG1mleIatplZSVS5Dbut1ziLXIDuKsVxrHLFquJ3qnKskbIM51nTuysWx7HKFauK36nKsUaE4ZywzcysjhO2mVlJDOeEPa9icRyrXLGq+J2qHGtEUPJwwMzMhrnhXMM2M7M6TthmZiUx7BK2pFmSHpa0TtKcHOPMl7RZ0qq8YtTFmiLpPyStkbRa0nk5xdlO0j2S7kviXJxHnIaYXZL+S9ItOcd5VNIDklZKWp5zrJ0lXS/poeTv7E9yivP25PtsWZ6XdH5OsS5I/k2sknStpO3yiJPEOi+Jszqv7zNibe2O4A0d7buAR4D9gDHAfdTmkcwj1tHAocCqAr7XRODQZH0H4Nd5fC9AwPhkfTRwN3Bkzt/t88A1wC05x3kU2C3vv6sk1tXAp5L1McDOBcTsAp4A9s7h3pOA3wDbJ9v/Cnw8p+/xTmAVMJbam9S3AVOL+HsbCctwq2FPB9ZFxPqIeBW4DpidR6CIuIOCpjmLiI0RcW+y/gdgDbX/ibKOExHxQrI5Ollye6osaTLwZ9SmkqsESTtS+2F+FUBEvBoRzxYQ+ljgkYj4bU73HwVsL2kUtWS6Iac4+wN3RcRLEdEL3A6cklOsEWe4JexJwGN12z3kkNi2Jkn7AIdQq/3mcf8uSSuBzcCSiMglTuJbwBeAIkaMD2CxpBWS8nyDbj/g98APkqaeKyWNyzHeFqcC1+Zx44h4HPgG8DtgI/BcRCzOIxa12vXRkt4kaSxwIjAlp1gjznBL2GqyrzL9DiWNB24Azo+I5/OIERF9EXEwMBmYLumdecSR9AFgc0SsyOP+TRwVEYcCJwDnSMprTtFR1JrKvhcRhwAvArk9SwGQNAY4CfhJTvffhdpvqvsCewHjJP3fPGJFxBpqk3UvARZRa9bszSPWSDTcEnYPb/xpPJn8fnUrlKTR1JL1jyPixrzjJb/G/xyYlVOIo4CTJD1KrenqTyX9KKdYRMSG5HMzcBO15rM89AA9db+ZXE8tgefpBODeiNiU0/2PA34TEb+PiNeAG4H35BSLiLgqIg6NiKOpNTuuzSvWSDPcEvYyYKqkfZNax6nAgq1cpo5JErU20TURcWmOcXaXtHOyvj21/1EfyiNWRHwxIiZHxD7U/p7+f0TkUmuTNE7SDlvWgeOp/eqduYh4AnhM0tuTXccCD+YRq85p5NQckvgdcKSkscm/xWOpPUfJhaQ9ks83Ax8i3+82ogyr8bAjolfSucCt1J6az4/arO2Zk3QtcAywm6Qe4KKIuCqPWNRqo2cADyTtywB/ExELM44zEbhaUhe1H8b/GhG5drcryATgplquYRRwTUQsyjHeZ4EfJ5WG9cAn8gqUtPPOBP4irxgRcbek64F7qTVP/Bf5vjZ+g6Q3Aa8B50TEMznGGlH8arqZWUkMtyYRMzMbgBO2mVlJOGGbmZWEE7aZWUk4YZuZlYQTtplZSThhm5mVxP8AzWMyBjM/JjUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random_state = torch.Generator()\n",
    "random_state = random_state.manual_seed(10)\n",
    "n = 10\n",
    "\n",
    "\n",
    "accept = torch.FloatTensor(n, n).uniform_(-1, 1, generator = random_state) \n",
    "reservoir_pre_weights = torch.FloatTensor(n, n).uniform_(-1, 1, generator = random_state)\n",
    "sns.heatmap(accept); plt.show(); sns.heatmap(reservoir_pre_weights); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def printn(param: torch.nn.parameter):\n",
    "    print(param._name_ + \"\\t \\t\", param.shape)\n",
    "    \n",
    "\n",
    "class EchoStateNetwork(nn.Module):\n",
    "    def __init__(self, spectral_radius=0.9, n_nodes = 1000, activation_f = nn.Tanh(), feedback = True,\n",
    "                 noise = 0, input_scaling = 0.5, leaking_rate = 0.99, regularization = 10 **-3, backprop = False,\n",
    "                 criterion = nn.NLLLoss(), classification = False, output_size = 50, feedback_scaling = 0.5,\n",
    "                 already_normalized = True, bias = \"uniform\", connectivity = 0.1, random_state = 123,\n",
    "                 exponential = False, activation_function = None, obs_idx = None, resp_idx = None,\n",
    "                 reservoir = None, model_type = \"uniform\", input_weight_type = None, approximate_reservoir = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.already_normalized = already_normalized\n",
    "        self.reservoir = reservoir\n",
    "        self.random_state = torch.Generator().manual_seed(random_state)\n",
    "\n",
    "        self.approximate_reservoir = approximate_reservoir\n",
    "        # hyper-parameters:\n",
    "        self.input_scaling = input_scaling\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.noise = noise\n",
    "        self.n_nodes = n_nodes\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.regularization = regularization\n",
    "        self.feedback_scaling = feedback_scaling\n",
    "        self.bias = bias\n",
    "        self.connectivity = connectivity\n",
    "        \n",
    "        #activation\n",
    "        self.activation_function = activation_f\n",
    "        \n",
    "        self.feedback = feedback\n",
    "        #self.state = torch.zeros(1,self.n_nodes)\n",
    "        #self.state.detach()\n",
    "        self.backprop = backprop\n",
    "        \n",
    "        #self.LinOut = pass\n",
    "        #https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19\n",
    "        self.classification = classification\n",
    "        if self.classification:\n",
    "            self.log_reg = torch.nn.Linear(self.n_nodes, 2)\n",
    "            self.criterion = criterion #torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        with torch.no_grad():\n",
    "            self.gen_reservoir()\n",
    "    \n",
    "    def plot_reservoir(self):\n",
    "        sns.histplot(self.weights.numpy().reshape(-1,))\n",
    "        \n",
    "    def forward(self, t, input_, current_state):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        \n",
    "        \"\"\"\n",
    "        #with torch.no_grad():\n",
    "        #input_ = input_.view(-1,1)\n",
    "        #current_state= current_state.view(-1,1)\n",
    "        \n",
    "        #printn(self.in_weights)\n",
    "        in_vec     = self.in_weights @ input_\n",
    "        weight_vec = self.weights @ current_state\n",
    "        vec    = in_vec + weight_vec\n",
    "        update = self.activation_function(vec)\n",
    "        \n",
    "        current_hidden_state = self.leaking_rate * update + (1 - self.leaking_rate) * current_state  # Leaking separate\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.state[t, :] = current_hidden_state.detach()\n",
    "\n",
    "        if self.classification:\n",
    "            output = self.ClassOut(current_hidden_state)\n",
    "        else:\n",
    "            output = self.LinOut(current_hidden_state)\n",
    "        return current_hidden_state, output #, hidden_dot, output_dot\n",
    "      \n",
    "    \"\"\"\n",
    "    def gen_reservoir(self, spectral_radius = 0.5):\n",
    "        #\n",
    "        #generate the reservoir.\n",
    "        #\n",
    "        with torch.no_grad():\n",
    "            res_weights = torch.FloatTensor(self.n_nodes, self.n_nodes).uniform_(-1, 1, generator = self.random_state)\n",
    "            accept = torch.FloatTensor(self.n_nodes, self.n_nodes).uniform_(0, 1, generator = self.random_state)\n",
    "            accept = accept<self.connectivity\n",
    "            #print(accept.sum()/(len(accept.view(-1,1))))\n",
    "            res = accept * res_weights\n",
    "            radius = (res.eig(eigenvectors = False)[0].abs().max())\n",
    "            weights = nn.Parameter(res*self.spectral_radius/radius, requires_grad = False).to(device)\n",
    "        return weights\n",
    "    \"\"\"\n",
    "    \n",
    "    def gen_reservoir(self, obs_idx = None, targ_idx = None, load_failed = None):\n",
    "        \"\"\"Generates random reservoir from parameters set at initialization.\"\"\"\n",
    "        # Initialize new random state\n",
    "        start = time.time()\n",
    "        #random_state = np.random.RandomState(self.random_state)\n",
    "        max_tries = 1000  # Will usually finish on the first iteration\n",
    "        n = self.n_nodes\n",
    "\n",
    "        #if the size of the reservoir has changed, reload it.\n",
    "        if self.reservoir:\n",
    "            if self.reservoir.n_nodes_ != self.n_nodes:\n",
    "                load_failed = 1\n",
    "        \n",
    "        book_index = 0\n",
    "        for i in range(max_tries):\n",
    "            if i > 0:\n",
    "                printc(str(i), 'fail')\n",
    "            #only initialize the reservoir and connectivity matrix if we have to for speed in esn_cv.\n",
    "            if not self.reservoir or not self.approximate_reservoir or load_failed == 1:\n",
    "                print(\"bad, building reservoir\")\n",
    "\n",
    "                self.accept = torch.FloatTensor(self.n_nodes, self.n_nodes).uniform_(0, 1, generator = self.random_state)\n",
    "                self.weights = torch.FloatTensor(self.n_nodes, self.n_nodes).uniform_(-1, 1, generator = self.random_state)\n",
    "                self.weights *= self.accept\n",
    "                #self.weights = csc_matrix(self.weights)\n",
    "            else:\n",
    "                #print(\"LOADING MATRIX\", load_failed)\n",
    "                if self.approximate_reservoir:\n",
    "                    try:   \n",
    "                        self.weights = self.reservoir.get_approx_preRes(self.connectivity, i) #np.random.choice([0,1]))\n",
    "                        #printc(\"reservoir successfully loaded (\" + str(self.weights.shape) , 'green') \n",
    "                    except:\n",
    "                        printc(\"approx reservoir \" + str(i) + \" failed to load... regenerating\", 'fail')\n",
    "\n",
    "                        #skip to the next iteration of the loop\n",
    "                        if i > self.reservoir.number_of_preloaded_sparse_sets:\n",
    "                            load_failed = 1\n",
    "                            printc(\"All preloaded reservoirs Nilpotent, generating random reservoirs.\" + \", connectivity =\" + str(round(self.connectivity,8)) + '...regenerating', 'fail')\n",
    "                        continue\n",
    "                else:\n",
    "                    assert 1 == 0, \"TODO, case not yet handled.\"\n",
    "                    \n",
    "            max_eigenvalue = (self.weights.eig(eigenvectors = False)[0].abs().max())\n",
    "            \n",
    "            if max_eigenvalue > 0:\n",
    "                break\n",
    "            else:\n",
    "                printc(\"Loaded Reservoir is Nilpotent (max_eigenvalue =\" + str(max_eigenvalue) + \"), connectivity =\" + str(round(self.connectivity,8))  + '...regenerating', 'fail')\n",
    "                #if we have run out of pre-loaded reservoirs to draw from :\n",
    "                if i == max_tries - 1:\n",
    "                    raise ValueError('Nilpotent reservoirs are not allowed. Increase connectivity and/or number of nodes.')\n",
    "\n",
    "        # Set spectral radius of weight matrix\n",
    "        self.weights = nn.Parameter(self.weights*self.spectral_radius/max_eigenvalue, requires_grad = False).to(device)\n",
    "        \n",
    "        if load_failed == 1 or not self.reservoir:\n",
    "            self.state = Variable(torch.zeros((1, self.n_nodes)), requires_grad = False).to(device)\n",
    "        else:\n",
    "            self.state = self.reservoir.state\n",
    "\n",
    "        # Set out to none to indicate untrained ESN\n",
    "        self.out_weights = None\n",
    "             \n",
    "    def set_Win(self, inputs):\n",
    "        \"\"\"\n",
    "        Build the input weights.\n",
    "        Currently only uniform implimented.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if not self.reservoir or 'in_weights' not in dir(self.reservoir): \n",
    "                print(\"BAD: GENERATING IN WEIGHTS\")\n",
    "\n",
    "                in_weights = torch.FloatTensor(self.n_nodes, self.n_inputs).uniform_(-1, 1, generator = self.random_state)\n",
    "                if self.bias == \"uniform\":\n",
    "                    #random uniform distributed bias\n",
    "                    bias = torch.FloatTensor(self.n_nodes, 1).uniform_(-1, 1, generator = self.random_state)\n",
    "                else:\n",
    "                    bias = torch.ones(self.n_nodes, 1)*self.bias\n",
    "\n",
    "                #if there is white noise add it in (this will be much more useful later with the exponential model)\n",
    "                if self.noise:\n",
    "                    white_noise = torch.normal(0, self.noise, size = (self.n_nodes, n_inputs))\n",
    "                    in_weights += white_noise\n",
    "\n",
    "                in_weights = torch.hstack((bias, in_weights)) * self.input_scaling\n",
    "\n",
    "                \n",
    "            else:\n",
    "                in_weights = self.reservoir.in_weights + self.noise * self.reservoir.noise_z\n",
    "        \n",
    "        #This should be externalized too.\n",
    "        if self.feedback:\n",
    "            # Add teacher forced signal (equivalent to y(t-1) as input)\n",
    "\n",
    "            feedback_weights = self.feedback_scaling * torch.FloatTensor(self.n_nodes, 1).uniform_(-1, 1, generator = self.random_state)\n",
    "            in_weights = torch.hstack((in_weights, feedback_weights)).reshape(self.n_nodes, -1)\n",
    "             \n",
    "        in_weights = nn.Parameter(in_weights, requires_grad = False).to(device)\n",
    "        in_weights._name_ = \"in_weights\"\n",
    "        return(in_weights)\n",
    "    \n",
    "        \"\"\"\n",
    "        if not self.reservoir or 'in_weights' not in dir(self.reservoir): \n",
    "            \n",
    "            if self.input_weight_type in [\"exponential\", \"uniform\"]:\n",
    "                n_inputs = (y.shape[1] - 1) if x is None else x.shape[1]\n",
    "                self.n_inputs = n_inputs\n",
    "                print('in weight type', self.input_weight_type)\n",
    "                if self.input_weight_type == \"exponential\":\n",
    "                    #print(\"EXP\")\n",
    "                    self.get_exp_weights(x.shape[1])\n",
    "                    \n",
    "                    if self.reservoir:\n",
    "                        uniform_bias = self.reservoir.uniform_bias\n",
    "                        #print(\"BUILDING EXPONENTIAL IN WEIGHTS\" )\n",
    "                    else:\n",
    "                        uniform_bias = random_state.uniform(-1, 1, size = (self.n_nodes, 1))\n",
    "                    self.in_weights =  self.exp_weights\n",
    "                else:\n",
    "                    print(\"BUILDING UNIFORM IN WEIGHTS\")\n",
    "                    \n",
    "                    self.in_weights = random_state.uniform(-1, 1, size=(self.n_nodes, n_inputs))\n",
    "                    uniform_bias = random_state.uniform(-1, 1, size = (self.n_nodes, 1))\n",
    "\n",
    "                \n",
    "                 #* self.bias_scaling\n",
    "                if self.noise:\n",
    "                    white_noise = random_state.normal(loc = 0, scale = self.noise, size = (self.n_nodes, n_inputs)) #self.in_weights.shape[1] - 1\n",
    "                    #print(self.in_weights.shape, \"in_weights\")\n",
    "                    #print(white_noise.shape, \"white noise\")\n",
    "                    self.in_weights += white_noise\n",
    "                self.in_weights = np.hstack((uniform_bias, self.in_weights)) * self.input_scaling\n",
    "\n",
    "                #self.reservoir.in_weights = self.in_weights\n",
    "\n",
    "            elif self.input_weight_type == \"cyclic\":\n",
    "\n",
    "                # Set and scale input weights (for memory length and non-linearity)\n",
    "                self.in_weights = np.full(shape=(self.n_nodes, inputs.shape[1] - 1), fill_value=self.cyclic_input_w, dtype=np.float32)\n",
    "                self.in_weights *= np.sign(random_state.uniform(low=-1.0, high=1.0, size=self.in_weights.shape))\n",
    "                self.in_weights *= self.input_scaling \n",
    "                \n",
    "                #add input bias\n",
    "                cyclic_bias = np.full(shape=(self.n_nodes, 1), fill_value=self.cyclic_bias, dtype=np.float32)\n",
    "                cyclic_bias *= np.sign(random_state.uniform(low=-1.0, high=1.0, size=self.cyclic_bias.shape))\n",
    "                \n",
    "                self.in_weights = np.hstack((cyclic_bias, self.in_weights)) \n",
    "                self.input_weights_from_cv = self.in_weights\n",
    "        else:\n",
    "            #print(\"loading in_weights from cv.\")\n",
    "            self.in_weights = self.reservoir.in_weights + self.noise * self.reservoir.noise_z\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "    def display_in_weights(self):\n",
    "        sns.heatmap(self.in_weights)\n",
    "    def display_out_weights(self):\n",
    "        sns.heatmap(self.out_weights)\n",
    "    def display_res_weights(self):\n",
    "        sns.heatmap(self.weights)\n",
    "    def plot_states(self, n= 10):\n",
    "        for i in range(n):\n",
    "            plt.plot(list(range(len(self.state[:,i]))), RC.state[:,i], alpha = 0.8)\n",
    "        \n",
    "    def train(self, y, x=None, burn_in=0, input_weight=None, verbose = False, epochs = 5):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Arguments:\n",
    "            y: response matrix\n",
    "            x: observer matrix\n",
    "            burn in: obvious\n",
    "            input_weight : ???\n",
    "            \n",
    "        \"\"\"\n",
    "        #h  = Variable(torch.zeros([Nt,self.reservoir_size]), requires_grad = False)\n",
    "        #zd = Variable(torch.zeros(Nt), requires_grad = False)\n",
    "        #z  = Variable(torch.zeros(Nt), requires_grad = False)\n",
    "        \n",
    "        #TODO : torch random state\n",
    "        \n",
    "        #build the state matrix:\n",
    "        #with torch.no_grad():\n",
    "        start_index = 1 if self.feedback else 0 \n",
    "        rows = y.shape[0] - start_index\n",
    "        \n",
    "        # Normalize inputs and outputs\n",
    "        y = self.normalize(outputs=y, keep=True)\n",
    "        \n",
    "        if not x is None:\n",
    "            x = self.normalize(inputs=x, keep=True)\n",
    "            self.n_inputs = (x.shape[1])\n",
    "    \n",
    "        if x is None and not self.feedback:\n",
    "            #raise ValueError(\"Error: provide x or enable feedback\")\n",
    "            self.already_normalized = True\n",
    "            inputs = torch.ones((y.shape[0], y.shape[1] + 1))\n",
    "            self.n_inputs  = y.shape[1]\n",
    "        \n",
    "        if x is None and self.feedback:\n",
    "            self.n_inputs  = y.shape[1] - 1\n",
    "            \n",
    "        self.state = Variable(torch.zeros((rows, self.n_nodes)), requires_grad = False).to(device)\n",
    "        self.state._name_ = \"state\"\n",
    "        #self.state = self.state.detach()\n",
    "        current_state = self.state[-1] \n",
    "        \n",
    "        #concatenate a column of ones to the input x for bias.\n",
    "        inputs = torch.ones(rows, 1).to(device)\n",
    "        inputs.requires_grad=False\n",
    "        \n",
    "        \n",
    "        #initialize the in_weights and bias tensors\n",
    "        if not x is None:\n",
    "            inputs = torch.hstack((inputs, x[start_index:]))\n",
    "        elif self.feedback:\n",
    "            inputs = torch.hstack((inputs, y[:-1])) \n",
    "        else:\n",
    "            inputs = torch.FloatTensor(inputs.shape[0], 2).uniform_(-1, 1, generator = self.random_state)#torch.hstack((inputs, inputs)) \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        self.in_weights = self.set_Win(inputs)\n",
    "        inputs._name_ = \"inputs\"\n",
    "        #printn(inputs)\n",
    "        #printn(self.in_weights)\n",
    "        #self.LinIn.bias =   self.set_bin()\n",
    "        \n",
    "        #output weights\n",
    "        self.LinOut = nn.Linear(self.n_nodes, y.shape[0]).to(device)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.003)\n",
    "        \n",
    "        for i in range(5):\n",
    "            try:\n",
    "                param = next(hi)\n",
    "                if param.requires_grad:\n",
    "                    print(f'parameter {i}',param.shape)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        #fast exact solution ie we don't want to run backpropogation (ie we aren't doing classification):\n",
    "        if not self.backprop:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for t in range(inputs.shape[0]):\n",
    "\n",
    "                    hidden, output = self.forward(t, inputs[t].T, current_state)\n",
    "\n",
    "                complete_data = torch.hstack((inputs, self.state))\n",
    "                complete_data._name_ = \"complete_data\"\n",
    "\n",
    "                train_x = complete_data[burn_in:]  # Include everything after burn_in\n",
    "                train_y = y[burn_in + 1:] if self.feedback else y[burn_in:]\n",
    "                \n",
    "                train_y = train_y.to(device)\n",
    "                \n",
    "                hi_X = train_x.T @ train_x\n",
    "\n",
    "                \n",
    "                \n",
    "                #print(\"inputs\", inputs[t].T.get_device(), \" current state\", current_state.get_device())\n",
    "                # Ridge regression\n",
    "                ridge_x = train_x.T @ train_x + self.regularization * torch.eye(train_x.shape[1]).to(device)\n",
    "                ridge_y = train_x.T @ train_y\n",
    "\n",
    "                # Solver solution (fast)\n",
    "                out_weights_sol = torch.solve(ridge_y, ridge_x)\n",
    "                self.out_weights = out_weights_sol.solution\n",
    "                self.out_weights._name_ = \"out_weights\"\n",
    "                #sns.histplot(self.out_weights.numpy())\n",
    "                #print(\"self.out_weights\", self.out_weights.shape)\n",
    "                #print(\"Training Loss: {:.3f}.. \".format(self.criterion(hidden@self.out_weights, y)))\n",
    "        else:\n",
    "            # backprop:\n",
    "            \n",
    "            running_loss = 0\n",
    "            train_losses = []\n",
    "            for e in range(epochs):\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "                #unnin\n",
    "                for t in range(inputs.shape[0]):\n",
    "                    input_ = inputs[t].T\n",
    "                    _, output = self.forward(t, input_, current_state)\n",
    "                    output = output.view(*y.shape)\n",
    "                    loss += self.criterion(output, y)\n",
    "                    if t % 500 == 0:\n",
    "                        print(\"timestep \", t)\n",
    "                if not e:   \n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                #running_loss += loss.item()\n",
    "\n",
    "                #if self.classification:\n",
    "                #    output = F.log_softmax(output, dim=1)\n",
    "                #   #z_targ = y#.view(-1,) #z_targ = z_targ.long()\n",
    "                #    running_loss += self.criterion(output, y) \n",
    "                #else:\n",
    "                #    print(self.criterion)\n",
    "                #    running_loss += self.criterion(output, y)\n",
    "                \n",
    "                #loss_history.append(loss.data.numpy())\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(loss))#/len(trainloader)),\n",
    "                      #\"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                      #\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "            \n",
    "            complete_data = torch.hstack((inputs, self.state))\n",
    "        \n",
    "        # Store last y value as starting value for predictions\n",
    "        self.y_last = y[-1, :]\n",
    "\n",
    "        # Return all data for computation or visualization purposes (Note: these are normalized)\n",
    "        return complete_data, (y[1:,:] if self.feedback else y), burn_in\n",
    "    \n",
    "    def normalize(self, inputs=None, outputs=None, keep=False):\n",
    "        \"\"\"Normalizes array by column (along rows) and stores mean and standard devation.\n",
    "\n",
    "        Set `store` to True if you want to retain means and stds for denormalization later.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array or None\n",
    "            Input matrix that is to be normalized\n",
    "        outputs : array or None\n",
    "            Output column vector that is to be normalized\n",
    "        keep : bool\n",
    "            Stores the normalization transformation in the object to denormalize later\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        transformed : tuple or array\n",
    "            Returns tuple of every normalized array. In case only one object is to be returned the tuple will be\n",
    "            unpacked before returning\n",
    "\n",
    "        \"\"\"\n",
    "        # Checks\n",
    "        if inputs is None and outputs is None:\n",
    "            raise ValueError('Inputs and outputs cannot both be None')\n",
    "\n",
    "        # Storage for transformed variables\n",
    "        transformed = []\n",
    "        #torch.std()\n",
    "        if not inputs is None:\n",
    "            inputs = inputs.to(device)\n",
    "            if keep:\n",
    "                # Store for denormalizationf\n",
    "                self._input_means = inputs.mean(axis=0)\n",
    "                print(\"input means\", self._input_means)\n",
    "                self._input_stds = inputs.std(dim = 0) #, ddof = 1)#, ddof=1)\n",
    "                print(\"input stds\", self._input_stds)\n",
    "            # Transform\n",
    "            transformed.append((inputs - self._input_means) / self._input_stds)\n",
    "            \n",
    "            self._input_means = self._input_means.to(device)\n",
    "            self._input_stds  = self._input_stds.to(device)\n",
    "\n",
    "        if not outputs is None:\n",
    "            outputs = outputs.to(device)\n",
    "            if keep:\n",
    "                # Store for denormalization\n",
    "                self._output_means = outputs.mean(axis=0)\n",
    "                self._output_stds = outputs.std(dim = 0)#, ddof=1)\n",
    "\n",
    "            # Transform\n",
    "            transformed.append((outputs - self._output_means) / self._output_stds)\n",
    "            \n",
    "            self._output_means = self._output_means.to(device)\n",
    "            self._output_stds = self._output_stds.to(device)\n",
    "        # Syntactic sugar\n",
    "        return tuple(transformed) if len(transformed) > 1 else transformed[0]\n",
    "    \n",
    "    def test(self, y, x=None, y_start=None, steps_ahead=None, scoring_method='nmse', alpha=1.):\n",
    "        \"\"\"Tests and scores against known output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array\n",
    "            Column vector of known outputs\n",
    "        x : array or None\n",
    "            Any inputs if required\n",
    "        y_start : float or None\n",
    "            Starting value from which to start testing. If None, last stored value from trainging will be used\n",
    "        steps_ahead : int or None\n",
    "            Computes average error on n steps ahead prediction. If `None` all steps in y will be used.\n",
    "        scoring_method : {'mse', 'rmse', 'nrmse', 'tanh'}\n",
    "            Evaluation metric used to calculate error\n",
    "        alpha : float\n",
    "            Alpha coefficient to scale the tanh error transformation: alpha * tanh{(1 / alpha) * error}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error : float\n",
    "            Error between prediction and knwon outputs\n",
    "\n",
    "        \"\"\"\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Run prediction\n",
    "        final_t =y.shape[0]\n",
    "        if steps_ahead is None:\n",
    "            y_predicted = self.predict(n_steps = y.shape[0], x=x, y_start=y_start)\n",
    "            #printc(\"predicting \"  + str(y.shape[0]) + \"steps\", 'blue')\n",
    "        else:\n",
    "            #printc(\"predicting \"  + str(y.shape[0]) + \"steps\", 'blue')\n",
    "            y_predicted = self.predict_stepwise(y, x, steps_ahead=steps_ahead, y_start=y_start)[:final_t,:]\n",
    "        \n",
    "        print('y', y.get_device(), 'y_predicted', y_predicted.get_device())\n",
    "        # Return error\n",
    "        return self.error(y_predicted, y, scoring_method, alpha=alpha), y_predicted\n",
    "    \n",
    "    def predict(self, n_steps, pure_prediction = True, x=None, y_start=None):\n",
    "        \"\"\"Predicts n values in advance.\n",
    "\n",
    "        Prediction starts from the last state generated in training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps : int\n",
    "            The number of steps to predict into the future (internally done in one step increments)\n",
    "        x : numpy array or None\n",
    "            If prediciton requires inputs, provide them here\n",
    "        y_start : float or None\n",
    "            Starting value from which to start prediction. If None, last stored value from training will be used\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_predicted : numpy array\n",
    "            Array of n_step predictions\n",
    "\n",
    "        \"\"\"\n",
    "        # Check if ESN has been trained\n",
    "        if self.out_weights is None or self.y_last is None:\n",
    "            raise ValueError('Error: ESN not trained yet')\n",
    "        \n",
    "        # Normalize the inputs (like was done in train)\n",
    "        if not self.already_normalized:\n",
    "            if not x is None:\n",
    "                x = self.normalize(inputs=x)\n",
    "\n",
    "        #initialize input:\n",
    "        inputs = Variable(torch.zeros((n_steps, 1)), requires_grad = False) #torch.ones((n_steps, 1), dtype=np.float32)  # Add bias term\n",
    "        \n",
    "\n",
    "        #Choose correct input\n",
    "        if x is None and not self.feedback:\n",
    "            #raise ValueError(\"Error: provide x or enable feedback\")\n",
    "            inputs = torch.ones((self.in_weights.shape[0], self.in_weights.shape[1]))\n",
    "        elif x is not None:\n",
    "            inputs = torch.hstack((inputs, x)) \n",
    "        inputs._name_ = \"inputs\"\n",
    "        \n",
    "        # Set parameters\n",
    "        if self.out_weights.shape[1] == 1:\n",
    "            y_predicted = torch.zeros((n_steps,), dtype=torch.float32)\n",
    "        else:\n",
    "            y_predicted = torch.zeros((n_steps, self.out_weights.shape[1]), dtype=torch.float32)#)\n",
    "\n",
    "        # Get last states\n",
    "        previous_y = self.y_last\n",
    "        #if not self.already_normalized:\n",
    "        if not y_start is None:\n",
    "            previous_y = self.normalize(outputs=y_start)[0]\n",
    "\n",
    "        # Initialize state from last availble in train\n",
    "        current_state = self.state[-1]\n",
    "        current_state._name_ = \"current state\"\n",
    "\n",
    "        # Predict iteratively\n",
    "        for t in range(n_steps):\n",
    "\n",
    "            # Get correct input based on feedback setting\n",
    "            current_input = inputs[t].T if not self.feedback else torch.hstack((inputs[t], previous_y))\n",
    "\n",
    "            # Update\n",
    "            update = self.activation_function(self.in_weights @ current_input + self.weights @ current_state)\n",
    "            #print(\"update: \" + str(update.shape))\n",
    "            current_state = self.leaking_rate * update + (1 - self.leaking_rate) * current_state\n",
    "\n",
    "            # Prediction. Order of concatenation is [1, inputs, y(n-1), state]\n",
    "            complete_row = torch.hstack((current_input, current_state))\n",
    "\n",
    "            if self.out_weights.shape[1] > 1:\n",
    "                y_predicted[t,:] = complete_row @ self.out_weights\n",
    "                previous_y = y_predicted[t,:]\n",
    "            else:\n",
    "                y_predicted[t] = complete_row @ self.out_weights\n",
    "                previous_y = y_predicted[t]\n",
    "                #print(\"t: \", t, \"y_predicted\", y_predicted[t].shape)\n",
    "            \n",
    "        #if not self.already_normalized:\n",
    "        # Denormalize predictions\n",
    "        y_predicted = self.denormalize(outputs=y_predicted)\n",
    "\n",
    "        return y_predicted.view(-1, self.out_weights.shape[1])\n",
    "    \n",
    "    def predict_stepwise(self, y, x=None, steps_ahead=1, y_start=None):\n",
    "        \"\"\"Predicts a specified number of steps into the future for every time point in y-values array.\n",
    "        E.g. if `steps_ahead` is 1 this produces a 1-step ahead prediction at every point in time.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            Array with y-values. At every time point a prediction is made (excluding the current y)\n",
    "        x : numpy array or None\n",
    "            If prediciton requires inputs, provide them here\n",
    "        steps_ahead : int (default 1)\n",
    "            The number of steps to predict into the future at every time point\n",
    "        y_start : float or None\n",
    "            Starting value from which to start prediction. If None, last stored value from training will be used\n",
    "        Returns\n",
    "        -------\n",
    "        y_predicted : numpy array\n",
    "            Array of predictions at every time step of shape (times, steps_ahead)\n",
    "        \"\"\"\n",
    "        print(\"predict stepwise\")\n",
    "        \n",
    "        # Check if ESN has been trained\n",
    "        if self.out_weights is None or self.y_last is None:\n",
    "            raise ValueError('Error: ESN not trained yet')\n",
    "\n",
    "        # Normalize the arguments (like was done in train)\n",
    "        y = self.normalize(outputs=y)\n",
    "        if not x is None:\n",
    "            x = self.normalize(inputs=x)\n",
    "\n",
    "        # Timesteps in y\n",
    "        t_steps = y.shape[0]\n",
    "\n",
    "        # Check input\n",
    "        if not x is None and not x.shape[0] == t_steps:\n",
    "            raise ValueError('x has the wrong size for prediction: x.shape[0] = {}, while y.shape[0] = {}'.format(\n",
    "                x.shape[0], t_steps))\n",
    "\n",
    "        # Choose correct input\n",
    "        if x is None and not self.feedback:\n",
    "            raise ValueError(\"Error: cannot run without feedback and without x. Enable feedback or supply x\")\n",
    "        elif not x is None:\n",
    "            # Initialize input\n",
    "            inputs = torch.ones((t_steps, 1), dtype=torch.float32)  # Add bias term\n",
    "            inputs = torch.hstack((inputs, x)).to(device)  # Add x inputs\n",
    "        else:\n",
    "            # x is None\n",
    "            inputs = torch.ones((t_steps + steps_ahead, 1), dtype=torch.float32).to(device)  # Add bias term\n",
    "\n",
    "        # Run until we have no further inputs\n",
    "        time_length = t_steps if x is None else t_steps - steps_ahead + 1\n",
    "\n",
    "        # Set parameters\n",
    "        y_predicted = torch.zeros((time_length, steps_ahead), dtype=torch.float32).to(device)\n",
    "        # Get last states\n",
    "        previous_y = self.y_last\n",
    "        if not y_start is None:\n",
    "            previous_y = self.normalize(outputs=y_start)[0].to(device)\n",
    "\n",
    "        # Initialize state from last availble in train\n",
    "        current_state = self.state[-1]\n",
    "\n",
    "        # Predict iteratively\n",
    "        with torch.no_grad():\n",
    "            for t in range(time_length):\n",
    "                # State_buffer for steps ahead prediction\n",
    "                prediction_state = current_state.clone().detach().to(device)#np.copy(current_state)\n",
    "\n",
    "                # Y buffer for step ahead prediction\n",
    "                prediction_y = previous_y.clone().detach().to(device)#np.copy(previous_y)\n",
    "            \n",
    "                # Predict stepwise at from current time step\n",
    "                for n in range(steps_ahead):\n",
    "                    \n",
    "                    # Get correct input based on feedback setting\n",
    "                    prediction_input = inputs[t + n] if not self.feedback else torch.hstack((inputs[t + n], prediction_y)).to(device)\n",
    "                    # Update\n",
    "                    prediction_update = torch.tanh(self.in_weights @ prediction_input.T + self.weights @ prediction_state)\n",
    "                    prediction_state = self.leaking_rate * prediction_update + (1 - self.leaking_rate) * prediction_state\n",
    "\n",
    "                    # Store for next iteration of t (evolves true state)\n",
    "                    if n == 0:\n",
    "                        current_state = prediction_state.clone().detach()\n",
    "\n",
    "                    # Prediction. Order of concatenation is [1, inputs, y(n-1), state]\n",
    "                    prediction_row = torch.hstack((prediction_input, prediction_state))\n",
    "                    y_predicted[t, n] = prediction_row @ self.out_weights\n",
    "                    prediction_y = y_predicted[t, n]\n",
    "\n",
    "                # Evolve true state\n",
    "                previous_y = y[t]\n",
    "\n",
    "        # Denormalize predictions\n",
    "        y_predicted = self.denormalize(outputs=y_predicted)\n",
    "        \n",
    "        # Return predictions\n",
    "        return y_predicted\n",
    "    \n",
    "    def error(self, predicted, target, method='nmse', alpha=1.):\n",
    "        \"\"\"Evaluates the error between predictions and target values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted : array\n",
    "            Predicted value\n",
    "        target : array\n",
    "            Target values\n",
    "        method : {'mse', 'tanh', 'rmse', 'nmse', 'nrmse', 'tanh-nmse', 'log-tanh', 'log'}\n",
    "            Evaluation metric. 'tanh' takes the hyperbolic tangent of mse to bound its domain to [0, 1] to ensure\n",
    "            continuity for unstable models. 'log' takes the logged mse, and 'log-tanh' takes the log of the squeezed\n",
    "            normalized mse. The log ensures that any variance in the GP stays within bounds as errors go toward 0.\n",
    "        alpha : float\n",
    "            Alpha coefficient to scale the tanh error transformation: alpha * tanh{(1 / alpha) * error}.\n",
    "            This squeezes errors onto the interval [0, alpha].\n",
    "            Default is 1. Suggestions for squeezing errors > n * stddev of the original series\n",
    "            (for tanh-nrmse, this is the point after which difference with y = x is larger than 50%,\n",
    "             and squeezing kicks in):\n",
    "             n  |  alpha\n",
    "            ------------\n",
    "             1      1.6\n",
    "             2      2.8\n",
    "             3      4.0\n",
    "             4      5.2\n",
    "             5      6.4\n",
    "             6      7.6\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error : float\n",
    "            The error as evaluated with the metric chosen above\n",
    "\n",
    "        \"\"\"\n",
    "        errors = predicted - target\n",
    "        # Adjust for NaN and np.inf in predictions (unstable solution)\n",
    "        #if not torch.all(torch.isfinite(predicted)):\n",
    "        #    # print(\"Warning: some predicted values are not finite\")\n",
    "        #    errors = torch.inf\n",
    "\n",
    "        # Compute mean error\n",
    "        if method == 'mse':\n",
    "            error = torch.mean(torch.square(errors))\n",
    "        elif method == 'tanh':\n",
    "            error = alpha * torch.tanh(torch.mean(torch.square(errors)) / alpha)  # To 'squeeze' errors onto the interval (0, 1)\n",
    "        elif method == 'rmse':\n",
    "            error = torch.sqrt(torch.mean(torch.square(errors)))\n",
    "        elif method == 'nmse':\n",
    "            error = torch.mean(torch.square(errors)) / torch.square(target.ravel().std(ddof=1))\n",
    "        elif method == 'nrmse':\n",
    "            error = torch.sqrt(torch.mean(torch.square(errors))) / target.ravel().std(ddof=1)\n",
    "        elif method == 'tanh-nrmse':\n",
    "            nrmse = torch.sqrt(torch.mean(torch.square(errors))) / target.ravel().std(ddof=1)\n",
    "            error = alpha * torch.tanh(nrmse / alpha)\n",
    "        elif method == 'log':\n",
    "            mse = torch.mean(torch.square(errors))\n",
    "            error = torch.log(mse)\n",
    "        elif method == 'log-tanh':\n",
    "            nrmse = torch.sqrt(torch.mean(torch.square(errors))) / target.ravel().std(ddof=1)\n",
    "            error = torch.log(alpha * torch.tanh((1. / alpha) * nrmse))\n",
    "        else:\n",
    "            raise ValueError('Scoring method not recognized')\n",
    "        return error\n",
    "    \n",
    "    def denormalize(self, inputs=None, outputs=None):\n",
    "        \"\"\"Denormalizes array by column (along rows) using stored mean and standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array or None\n",
    "            Any inputs that need to be transformed back to their original scales\n",
    "        outputs : array or None\n",
    "            Any output that need to be transformed back to their original scales\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        transformed : tuple or array\n",
    "            Returns tuple of every denormalized array. In case only one object is to be returned the tuple will be\n",
    "            unpacked before returning\n",
    "\n",
    "        \"\"\"\n",
    "        if inputs is None and outputs is None:\n",
    "            raise ValueError('Inputs and outputs cannot both be None')\n",
    "\n",
    "        # Storage for transformed variables\n",
    "        transformed = []\n",
    "        \n",
    "        #for tensor in [train_x, train_y]:\n",
    "        #     print('device',tensor.get_device())\n",
    "        if not inputs is None:\n",
    "            transformed.append((inputs * self._input_stds) + self._input_means)\n",
    "        if not outputs is None:\n",
    "            transformed.append((outputs * self._output_stds) + self._output_means)\n",
    "\n",
    "        # Syntactic sugar\n",
    "        return tuple(transformed) if len(transformed) > 1 else transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('MackeyGlass_t17.txt')\n",
    "\n",
    "tr_len = 4000\n",
    "te_end = 4100 #6000\n",
    "\n",
    "train = torch.FloatTensor(data[:tr_len].reshape(-1, 1))\n",
    "test = torch.FloatTensor(data[tr_len:te_end].reshape(-1, 1))\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad, building reservoir\n",
      "BAD: GENERATING IN WEIGHTS\n",
      "predict stepwise\n",
      "y 0 y_predicted 0\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#n_nodes=1000, connectivity=0.01, input_scaling=0.5, feedback_scaling=0.5, leaking_rate=0.3, spectral_radius=1.25, regularization=1e-8, feedback=True)\n",
    "torch.manual_seed(1)\n",
    "RC = EchoStateNetwork(n_nodes=1000, connectivity=0.1, input_scaling=1, feedback_scaling=1, \n",
    "         leaking_rate=0.1, spectral_radius=0.96, regularization=1e-3, feedback=True, backprop = False).to(device) #bias = -10,\n",
    "\n",
    "RC.train( train)#, epochs=epochs, lr = 5e-3)\n",
    "\n",
    "err, pred = RC.test(test, scoring_method = \"mse\", steps_ahead = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict stepwise is a beast that I admit I don't yet understand. It clearly is the secret to unlocking pure predictions and improving the RC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one step ahead prediction (t 0 was the last step of training): [t1, t2, t3, t4, t5]\n",
    "#two steps ahead prediction: [t2, t3, t4, t5, t6]\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict stepwise\n",
      "y 0 y_predicted 0\n",
      "reported error tensor(0.1466, device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-613668c28975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalc_err\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#print(i, \"error: \", )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-613668c28975>\u001b[0m in \u001b[0;36mcalc_err\u001b[1;34m(steps_ahead, pred, truth)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtest_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0msteps_ahead\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[pred_ {}] neq [test {} ]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtest_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "step = 10\n",
    "\n",
    "err, pred = RC.test(test, scoring_method = \"mse\", steps_ahead = step)\n",
    "print('reported error', err)\n",
    "end = len(test)\n",
    "\n",
    "def calc_err(steps_ahead = step, pred = pred, truth =test):\n",
    "    if steps_ahead == None:\n",
    "        pred_, test_ = pred, test\n",
    "    else:\n",
    "        end = len(test) - steps_ahead\n",
    "        pred_ = pred[ :end, steps_ahead].clone()\n",
    "        test_ = test[ steps_ahead : ].clone()\n",
    "    assert len(pred_) == len(test_), \"[pred_ {}] neq [test {} ]\".format(len(pred_), len(test_))\n",
    "    return torch.mean((pred_ - test_)**2)\n",
    "errors = []   \n",
    "for i in range(step):\n",
    "    errors.append(calc_err(i).item())\n",
    "    #print(i, \"error: \", )\n",
    "\n",
    "hi = pd.DataFrame({\"steps_ahead\":list(range(step)), \"error\": errors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"steps_ahead\", y = np.log(hi[\"error\"]), data = hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,4))\n",
    "plt.plot(list(range(len(test))), test, label = \"train\")\n",
    "for i in range(pred.shape[1]):\n",
    "    offset = [0] *(i+1)\n",
    "    #yy =  offset + pred[:,i].numpy().ravel().tolist()\n",
    "    yy = pred[:,i].numpy().ravel().tolist()\n",
    "    \n",
    "    xx = list(range(len(yy )))\n",
    "    \n",
    "    plt.plot(xx, yy, alpha = 0.1 * i, color = \"red\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.numpy()[0:3].reshape(-1,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,4))\n",
    "for i in range(pred.shape[1]):\n",
    "    offset = test.numpy()[0:(i+1)].reshape(-1,).tolist()#*(i+1)\n",
    "    yy = offset + pred[i,:].numpy().ravel().tolist()\n",
    "    \n",
    "    xx = list(range(len(yy )))\n",
    "    \n",
    "    plt.plot(xx, yy, alpha = 0.1 * i, color = \"red\")\n",
    "    #if i >5:\n",
    "    #    break\n",
    "plt.plot(list(range(len(test))), test, label = \"train\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " RC.predict_stepwise(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred__ = RC.test(train, scoring_method = \"mse\", steps_ahead = None)\n",
    "pred__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = pred__.numpy().ravel()\n",
    "plt.figure(figsize = (14,4))\n",
    "plt.plot(list(range(len(hi))), hi, label = \"prediction\")\n",
    "plt.plot(list(range(len(hi))), train, label = \"train\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bounds(bounds_dict):\n",
    "    \"\"\"\n",
    "    This function converts the bounds of a dictionary.\n",
    "    \"\"\"\n",
    "    var_list = [\"spectral_radius\", \"connectivity\", \"regularization\", \"leaking_rate\", \"bias\"]\n",
    "    log_params = [\"connectivity\", \"regularization\"]\n",
    "    for i, param in enumerate(var_list):\n",
    "        lb, ub = bounds_dict[param]\n",
    "        tens_ = torch.cat([torch.tensor(lb).view(1,1), torch.tensor(ub).view(1,1)], dim = 0)\n",
    "        if not i:\n",
    "            tensor_ = tens_\n",
    "        else:\n",
    "            tensor_ = torch.cat([tensor_, tens_], 1)\n",
    "    print('tensor', tensor_)\n",
    "    return(tensor_)\n",
    "    # build bound tensor:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintain the TuRBO state\n",
    "TuRBO needs to maintain a state, which includes the length of the trust region, success and failure counters, success and failure tolerance, etc. \n",
    "\n",
    "In this tutorial we store the state in a dataclass and update the state of TuRBO after each batch evaluation. \n",
    "\n",
    "**Note**: These settings assume that the domain has been scaled to $[0, 1]^d$ and that the same batch size is used for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5 ** 7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 3  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TurboState(success_tolerance = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TurboState(dim=dim, batch_size=10)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate initial points\n",
    "This generates an initial set of Sobol points that we use to start of the BO loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_points(dim, n_pts):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new batch\n",
    "Given the current `state` and a probabilistic (GP) `model` built from observations `X` and `Y`, we generate a new batch of points.  \n",
    "\n",
    "This method works on the domain $[0, 1]^d$, so make sure to not pass in observations from the true domain.  `unnormalize` is called before the true function is evaluated which will first map the points back to the original domain.\n",
    "\n",
    "We support either TS and qEI which can be specified via the `acqf` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size,\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\", \"ei\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the TR to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = (\n",
    "            torch.rand(n_candidates, dim, dtype=dtype, device=device)\n",
    "            <= prob_perturb\n",
    "        )\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask        \n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        thompson_sampling = MaxPosteriorSampling(model=model, replacement=False)\n",
    "        X_next = thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "    elif acqf == \"ei\":\n",
    "        ei = qExpectedImprovement(model, train_Y.max(), maximize=True)\n",
    "        X_next, acq_value = optimize_acqf(\n",
    "            ei,\n",
    "            bounds=torch.stack([tr_lb, tr_ub]),\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "        )\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization loop\n",
    "This simple loop runs one instance of TuRBO-1 with Thompson sampling until convergence.\n",
    "\n",
    "TuRBO-1 is a local optimizer that can be used for a fixed evaluation budget in a multi-start fashion.  Once TuRBO converges, `state[\"restart_triggered\"]` will be set to true and the run should be aborted.  If you want to run more evaluations with TuRBO, you simply generate a new set of initial points and then keep generating batches until convergence or when the evaluation budget has been exceeded.  It's important to note that evaluations from previous instances are discarded when TuRBO restarts.\n",
    "\n",
    "NOTE: We use a `SingleTaskGP` with a noise constraint to keep the noise from getting too large as the problem is noise-free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "global errorz\n",
    "global errorz_step\n",
    "hi = \"for i in range(10): \\\n",
    "    pl.plot(pl.randn(100)) \\\n",
    "    display.clear_output(wait=True) \\\n",
    "    display.display(pl.gcf()) \\\n",
    "    time.sleep(1.0)\"\n",
    "\n",
    "\n",
    "def HRC(parameterization, steps_ahead = 60, backprop = False, plot_type = \"error\", *args): #Hayden's RC or Hillary lol\n",
    "    \"\"\"\n",
    "    This version of the RC helper function\n",
    "    \n",
    "    Arguments:\n",
    "        parameterization\n",
    "        steps_ahead\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def expand(x): \n",
    "        if type(x) == torch.Tensor:\n",
    "            expanded = torch.squeeze(x.clone().detach()).view(1,1)\n",
    "        else:\n",
    "            expanded = torch.squeeze(torch.tensor(x)).view(1,1)\n",
    "        return expanded\n",
    "    \n",
    "    def rnd(x):\n",
    "        return torch.round(x*100).item()/100\n",
    "    \n",
    "    batch_size = parameterization.shape[0]\n",
    "    \n",
    "    #can be parrallelized\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        #print(parameterization)\n",
    "        spectr, connect, regul, leak_rate, bias = parameterization[i, :]\n",
    "        \n",
    "        #print(\"building ... spect\", spectr.item(), \",connect\", connect.item(),\n",
    "        #      \", reg\",regul.item(), end = \" \")\n",
    "        \n",
    "        RC = EchoStateNetwork(n_nodes=1000, connectivity=10**connect, input_scaling=1, feedback_scaling=1, \n",
    "             leaking_rate=leak_rate, spectral_radius=spectr, regularization=10**regul, \n",
    "             feedback=True, backprop = backprop, bias = bias, approximate_reservoir = False) #bias = -10,\n",
    "\n",
    "        RC.train(train)\n",
    "        #print(\" ... model trained\", end = \" \")\n",
    "        \n",
    "        err_, pred_  = RC.test(y=train, scoring_method='mse', steps_ahead = steps_ahead)\n",
    "        \n",
    "        steps_displayed = 500\n",
    "\n",
    "        #plotting\n",
    "        if len(errorz) % 5 == 0:\n",
    "            \n",
    "            ax[1].clear()\n",
    "            ax[1].plot(train[:steps_displayed], alpha = 0.4, color = \"blue\")\n",
    "            \n",
    "        #time.sleep(0.1)\n",
    "        # original code for how to display interactive plot.\n",
    "        #\"for i in range(10): \\\n",
    "        #pl.plot(pl.randn(100)) \\\n",
    "        #display.clear_output(wait=True) \\\n",
    "        #display.display(pl.gcf()) \\\n",
    "        #time.sleep(1.0)\"\n",
    "        \n",
    "        #plot 1:\n",
    "        log_resid = torch.log((train - pred_)**2)\n",
    "        ax[0].plot(np.log(errorz), alpha = 0.2, color = \"green\", label = \"all samples\")\n",
    "        ax[0].set_title(\"log error vs Bayes Opt. step\")\n",
    "        ax[0].set_ylabel(\"log(mse)\")\n",
    "        ax[0].set_xlabel(\"Bayesian Optimization step\")\n",
    "        \n",
    "        \n",
    "        #plot 2:\n",
    "        if pred_.shape[0] == 1:\n",
    "            ax[1].plot(pred_[:steps_displayed], alpha = 0.3, color = \"red\")\n",
    "        else:\n",
    "            ax[1].plot(pred_[pred_.shape[0] - 1, :], alpha = 0.3, color = \"red\")\n",
    "        ax[1].set_ylim(train.min().item() - 0.1, train.max().item() )\n",
    "        ax[1].set_title(\"all guesses\")\n",
    "        ax[1].set_ylabel(\"y\")\n",
    "        ax[1].set_xlabel(\"time step\")\n",
    "        \n",
    "        #plot 3:\n",
    "        ax[2].clear()\n",
    "        ax[2].plot(train[:steps_displayed], alpha = 0.5, color = \"blue\", label = \"train\")\n",
    "        \n",
    "        if pred_.shape[0] == 1:\n",
    "            ax[2].plot(pred_, alpha = 0.3, color = \"red\")\n",
    "        else:\n",
    "            ax[2].plot(pred_, alpha = 0.3, color = \"red\")\n",
    "        #ax[2].plot(pred_[:steps_displayed], alpha = 0.5, color = \"red\", label = \"pred\")\n",
    "        ax[2].set_ylim(train.min().item() - 0.1, train.max().item() )\n",
    "        ax[2].set_title(\"guess vs step\")\n",
    "        ax[2].set_ylabel(\"y\")\n",
    "        ax[2].set_xlabel(\"time step\")\n",
    "        pl.legend()\n",
    "        #print(pred_.shape)\n",
    "        \n",
    "        \n",
    "        display.clear_output(wait=True) \n",
    "        display.display(pl.gcf()) \n",
    "        #print(\"pred_[pred_.shape[0] - 1, :]\", pred_[pred_.shape[0] - 1, :].shape)\n",
    "        \n",
    "        #make the maximum value of the error 10000 to avoid inf.\n",
    "        err_ = min(err_, torch.tensor(1000))\n",
    "        \n",
    "        if torch.isnan(err_):\n",
    "            err_ = torch.tensor(1000)\n",
    "        \n",
    "        if not i:\n",
    "            error = err_\n",
    "        else:\n",
    "            error = torch.cat([error, err_], dim = 0)\n",
    "        errorz.append(err_.item())\n",
    "        ax[1].set_ylim(train.min().item() - 0.1, train.max().item() )\n",
    "    \n",
    "    return -error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim = fun.bounds.shape[1]\n",
    "batch_size = 4\n",
    "n_init = 5  # 2*dim, which corresponds to 5 batches of 4\n",
    "fun = HRC\n",
    "fun.bounds = convert_bounds(bounds_dict)\n",
    "fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))\n",
    "\n",
    "def get_initial_points(dim, n_pts):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the CV class, which will contain the BO optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .esn import *\n",
    "#from .scr import *\n",
    "#from .detail.robustgpmodel import *\n",
    "#from .detail.esn_bo import *\n",
    "import numpy as np\n",
    "import GPy\n",
    "import GPyOpt\n",
    "import copy\n",
    "import json\n",
    "import pyDOE\n",
    "from collections import OrderedDict\n",
    "#import multiprocessing\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "colorz = {\n",
    "  \"header\" : '\\033[95m',\n",
    "  \"blue\" : '\\033[94m',\n",
    "  'cyan' : '\\033[96m',\n",
    "  'green' : '\\033[92m',\n",
    "  'warning' : '\\033[93m',\n",
    "  'fail' : '\\033[91m',\n",
    "  'endc' : '\\033[0m',\n",
    "   'bold' :'\\033[1m',\n",
    "   \"underline\" : '\\033[4m'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def printc(string_, color_) :\n",
    "      print(colorz[color_] + string_ + colorz[\"endc\"] )\n",
    "\n",
    "#from scipy.sparse import csr_matrix\n",
    "#>>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5\n",
    "\n",
    "class SparseBooklet:\n",
    "    def __init__(self, book, keys):\n",
    "        self.sparse_book = book\n",
    "        self.sparse_keys_ = keys\n",
    "\n",
    "    def get_approx_preRes(self, connectivity_threshold):\n",
    "        \"\"\"\n",
    "        You can use the matrix returned instead of...\n",
    "        \"\"\"\n",
    "        key_ =  self.sparse_keys_[self.sparse_keys_ > connectivity_threshold][0]\n",
    "        val =  self.sparse_book[key_].clone()\n",
    "        return val\n",
    "\n",
    "class GlobalSparseLibrary:\n",
    "\n",
    "    def __init__(self, lb = -5, ub = 0, n_nodes = 1000, precision = None, flip_the_script = False):\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.n_nodes_ = n_nodes\n",
    "        self.library = {}\n",
    "        self.book_indices = []\n",
    "        self.precision = precision\n",
    "        self.flip_the_script = flip_the_script\n",
    "\n",
    "\n",
    "\n",
    "    def addBook(self, random_seed):\n",
    "        \"\"\"\n",
    "        Add a sparse reservoir set.\n",
    "        \"\"\"\n",
    "        book = {}\n",
    "        random_state = torch.Generator().manual_seed(random_seed)\n",
    "        n = self.n_nodes_\n",
    "\n",
    "        accept = torch.FloatTensor(n, n).uniform_(-1, 1, generator = random_state) \n",
    "        reservoir_pre_weights = torch.FloatTensor(n, n).uniform_(-1, 1, generator = random_state)\n",
    "\n",
    "        \"for now we're going to avoid sparse matrices\"\n",
    "        for connectivity in np.logspace(self.ub, self.lb, self.precision):\n",
    "            #book[connectivity] = csc_matrix((accept < connectivity ) * reservoir_pre_weights)\n",
    "            \n",
    "            book[connectivity] = (accept < connectivity ) * reservoir_pre_weights\n",
    "        sparse_keys_ = np.array(sorted(book))\n",
    "\n",
    "        self.library[random_seed] = SparseBooklet(book = book, keys = sparse_keys_)\n",
    "        self.book_indices.append(random_seed)\n",
    "\n",
    "    def getIndices(self):\n",
    "        return self.book_indices\n",
    "\n",
    "    def get_approx_preRes(self, connectivity_threshold, index = 0):\n",
    "        \"\"\"\n",
    "        You can use the matrix returned instead of...\n",
    "        \"\"\"\n",
    "        if self.flip_the_script:\n",
    "            index = np.random.randint(len(self.book_indices))\n",
    "        book = self.library[self.book_indices[index]]\n",
    "        if index != 0:\n",
    "            printc(\"retrieving book from library\" + str(self.book_indices[index]), 'green')\n",
    "        return book.get_approx_preRes(connectivity_threshold)\n",
    "\n",
    "\n",
    "class ReservoirBuildingBlocks:\n",
    "    \"\"\" An object that allows us to save reservoir components (independent of hyper-parameters) for faster optimization.\n",
    "    Parameters:\n",
    "        model_type: either random, cyclic or delay line\n",
    "        input_weight_type: exponential or uniform\n",
    "        random_seed: the random seed to set the reservoir\n",
    "        n_nodes: the nodes of the network\n",
    "        n_inputs: the number of observers in the case of a block experiment, the size of the output in the case of a pure prediction where teacher forcing is used.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type, input_weight_type, random_seed, n_nodes, n_inputs = None, Distance_matrix = None):\n",
    "        #print(\"INITIALING RESERVOIR\")\n",
    "\n",
    "        #initialize attributes\n",
    "        self.input_weight_type_ = input_weight_type\n",
    "        self.model_type_ = model_type\n",
    "        self.n_inputs_ = n_inputs\n",
    "        self.n_nodes_ = n_nodes\n",
    "        self.seed_ = random_seed\n",
    "        self.state = np.zeros((1, self.n_nodes_), dtype=np.float32)\n",
    "        \n",
    "        if model_type == \"random\":\n",
    "            self.gen_ran_res_params()\n",
    "            self.gen_sparse_accept_dict()\n",
    "\n",
    "    def gen_ran_res_params(self):\n",
    "        random_state = torch.Generator().manual_seed(self.seed_)\n",
    "        n = self.n_nodes_\n",
    "        self.accept = torch.FloatTensor(n, n).uniform_(-1, 1, generator = random_state)\n",
    "        self.reservoir_pre_weights = torch.FloatTensor(n, n).uniform_(-1, 1, generator = random_state)\n",
    "\n",
    "    def gen_sparse_accept_dict(self, reservoir_seeds = [123, 999], precision = 1000):\n",
    "        \"\"\"\n",
    "        Later change this so that you put in the real search bounds.\n",
    "        This will approximate the search for the sparcity hyper-parameter, which will dramatically speed up training of the network.\n",
    "        Later we can add in a final stage where the network does approximate sparcity to a point, then changes to computer precision search.\n",
    "        \"\"\"\n",
    "        printc(\"GENERATING SPARSE DICT\", 'cyan')\n",
    "        global sparse_dict\n",
    "        \n",
    "        #printc(\"Building approximate sparse reservoirs for faster optimization ...\",'fail')\n",
    "        #for connectivity in np.logspace(0, -5, precision):\n",
    "        #    sparse_dict[connectivity] = csc_matrix((self.accept < connectivity ) * self.reservoir_pre_weights)\n",
    "        #self.sparse_keys_ = np.array(sorted(sparse_dict))\n",
    "        self.number_of_preloaded_sparse_sets = len(reservoir_seeds)\n",
    "        sparse_dict = GlobalSparseLibrary(precision = precision)\n",
    "        for random_seed in reservoir_seeds:\n",
    "            printc(\"generated sparse reservoir library for random seed \" + str(random_seed), 'cyan')\n",
    "            sparse_dict.addBook(random_seed)\n",
    "\n",
    "    def get_approx_preRes(self, connectivity_threshold, i):\n",
    "        \"\"\"\n",
    "        You can use the matrix returned instead of...\n",
    "        \"\"\"\n",
    "        val = sparse_dict.get_approx_preRes(connectivity_threshold, index = i)\n",
    "        return val\n",
    "\n",
    "    def get_approx_preRes_old(self, connectivity_threshold, i):\n",
    "        \"\"\"\n",
    "        You can use the matrix returned instead of...\n",
    "        \"\"\"\n",
    "        key_ =  self.sparse_keys_[self.sparse_keys_ > connectivity_threshold][0]\n",
    "        val =  sparse_dict[key_].copy()\n",
    "        return val\n",
    "\n",
    "    def gen_in_weights(self):\n",
    "\n",
    "        random_state = torch.Generator().manual_seed(self.seed_)\n",
    "\n",
    "        n, m = self.n_nodes_, self.n_inputs_\n",
    "\n",
    "        #at the moment all input weight matrices use uniform bias.\n",
    "        self.uniform_bias = torch.FloatTensor(n, 1).uniform_(-1, 1, generator = random_state)\n",
    "\n",
    "        #weights\n",
    "        if self.input_weight_type_ == \"uniform\":\n",
    "            \n",
    "            self.in_weights_uniform = torch.FloatTensor(n, m).uniform_(-1, 1, generator = random_state)\n",
    "            \n",
    "            #add bias\n",
    "            self.in_weights = torch.hstack((self.uniform_bias, self.in_weights_uniform))\n",
    "\n",
    "        elif self.input_weight_type_ == \"exponential\":\n",
    "            #printc(\"BUILDING SIGN_\", 'fail')\n",
    "            sign1 = random_state.choice([-1, 1], size= (in_w_shape_[0], in_w_shape_[1]//2))\n",
    "            sign2 = random_state.choice([-1, 1], size= (in_w_shape_[0], in_w_shape_[1]//2))\n",
    "\n",
    "            self.sign_dual = (sign1, sign2)\n",
    "            self.sign = np.concatenate((sign1, sign2), axis = 1)\n",
    "\n",
    "        #regularization\n",
    "        self.noise_z = torch.normal(0, 1, size = (n, m + 1), generator = random_state)\n",
    "\n",
    "\n",
    "\n",
    "__all__ = ['EchoStateNetworkCV']\n",
    "\n",
    "\n",
    "class EchoStateNetworkCV:\n",
    "    \"\"\"A cross-validation object that automatically optimizes ESN hyperparameters using Bayesian optimization with\n",
    "    Gaussian Process priors.\n",
    "\n",
    "    Searches optimal solution within the provided bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bounds : dict\n",
    "        A dictionary specifying the bounds for optimization. The key is the parameter name and the value\n",
    "        is a tuple with minimum value and maximum value of that parameter. E.g. {'n_nodes': (100, 200), ...}\n",
    "    model : class: {EchoStateNetwork, SimpleCycleReservoir}\n",
    "            Model class to optimize\n",
    "    subsequence_length : int\n",
    "        Number of samples in one cross-validation sample\n",
    "    eps : float\n",
    "        The number specifying the maximum amount of change in parameters before considering convergence\n",
    "    initial_samples : int\n",
    "        The number of random samples to explore the  before starting optimization\n",
    "    validate_fraction : float\n",
    "        The fraction of the data that may be used as a validation set\n",
    "    steps_ahead : int or None\n",
    "        Number of steps to use in n-step ahead prediction for cross validation. `None` indicates prediction\n",
    "        of all values in the validation array.\n",
    "    max_iterations : int\n",
    "        Maximim number of iterations in optimization\n",
    "    batch_size : int\n",
    "        Batch size of samples used by GPyOpt\n",
    "    cv_samples : int\n",
    "        Number of samples of the objective function to evaluate for a given parametrization of the ESN\n",
    "    scoring_method : {'mse', 'rmse', 'tanh', 'nmse', 'nrmse', 'log', 'log-tanh', 'tanh-nrmse'}\n",
    "        Evaluation metric that is used to guide optimization\n",
    "    log_space : bool\n",
    "        Optimize in log space or not (take the logarithm of the objective or not before modeling it in the GP)\n",
    "    tanh_alpha : float\n",
    "        Alpha coefficient used to scale the tanh error function: alpha * tanh{(1 / alpha) * mse}\n",
    "    esn_burn_in : int\n",
    "        Number of time steps to discard upon training a single Echo State Network\n",
    "    acquisition_type : {'MPI', 'EI', 'LCB'}\n",
    "        The type of acquisition function to use in Bayesian Optimization\n",
    "    max_time : float\n",
    "        Maximum number of seconds before quitting optimization\n",
    "    n_jobs : int\n",
    "        Maximum number of concurrent jobs\n",
    "    esn_feedback : bool or None\n",
    "        Build ESNs with feedback ('teacher forcing') if available\n",
    "    update_interval : int (default 1)\n",
    "        After how many acquisitions the GPModel should be updated\n",
    "    verbose : bool\n",
    "        Verbosity on or off\n",
    "    plot : bool\n",
    "        Show convergence plot at end of optimization\n",
    "    target_score : float\n",
    "        Quit when reaching this target score\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bounds, subsequence_length, model=EchoStateNetwork, eps=1e-8, initial_samples=50,\n",
    "                 validate_fraction=0.2, steps_ahead=1, max_iterations=1000, batch_size=1, cv_samples=1,\n",
    "                 scoring_method='nrmse', log_space=True, tanh_alpha=1., esn_burn_in=0, acquisition_type='LCB',\n",
    "                 max_time=np.inf, n_jobs=1, random_seed=None, esn_feedback=None, update_interval=1, verbose=True,\n",
    "                 plot=True, target_score=0., exp_weights = False, obs_index = None, target_index = None, \n",
    "                 noise = 0,\n",
    "                 model_type = \"random\", activation_function = \"tanh\", input_weight_type = \"uniform\", \n",
    "                 Distance_matrix = None, n_res = 1, count = None, reservoir = None,\n",
    "                 backprop = False, interactive = False, approximate_reservoir = True,\n",
    "                 failure_tolerance = 1, length_min = None):\n",
    "        print(\"FEEDBACK\", esn_feedback)\n",
    "        #\n",
    "        self.length_min = length_min\n",
    "        self.failure_tolerance = failure_tolerance\n",
    "        self.approximate_reservoir = approximate_reservoir\n",
    "        self.interactive = interactive\n",
    "        if interactive:\n",
    "            self.fig, self.ax = pl.subplots(1,3, figsize = (16,4))\n",
    "        \n",
    "        self.backprop = backprop\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Bookkeeping\n",
    "        self.bounds = bounds\n",
    "        self.parameters = OrderedDict(bounds) \n",
    "        self.errorz, self.errorz_step = [], []\n",
    "        # Fix order\n",
    "        \n",
    "        self.free_parameters = []\n",
    "        self.fixed_parameters = []\n",
    "        self.n_res = n_res\n",
    "        \n",
    "        self.count_ = None\n",
    "\n",
    "        # Store settings\n",
    "        self.model = model\n",
    "        self.subsequence_length = subsequence_length\n",
    "        self.eps = eps\n",
    "        self.initial_samples = initial_samples\n",
    "        self.validate_fraction = validate_fraction\n",
    "        self.steps_ahead = steps_ahead\n",
    "        self.max_iterations = max_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.cv_samples = cv_samples\n",
    "        self.scoring_method = scoring_method\n",
    "        self.log_space = log_space\n",
    "        self.alpha = tanh_alpha\n",
    "        self.esn_burn_in =  torch.tensor(esn_burn_in, dtype=torch.int32).item()   #torch.adjustment required\n",
    "        self.acquisition_type = acquisition_type\n",
    "        self.max_time = max_time\n",
    "        self.n_jobs = n_jobs\n",
    "        self.seed = random_seed\n",
    "        self.feedback = esn_feedback\n",
    "        self.update_interval = update_interval\n",
    "        self.verbose = verbose\n",
    "        self.plot = plot\n",
    "        self.target_score = target_score\n",
    "\n",
    "        #Hayden modifications: varying the architectures\n",
    "        self.exp_weights = exp_weights\n",
    "        self.obs_index = obs_index\n",
    "        self.target_index = target_index\n",
    "        self.noise = noise\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.Distance_matrix = Distance_matrix\n",
    "\n",
    "        # Normalize bounds domains and remember transformation\n",
    "        self.scaled_bounds, self.bound_scalings, self.bound_intercepts = self.normalize_bounds(self.parameters)\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.input_weight_type = input_weight_type\n",
    "        \n",
    "        if self.seed != None and type(self.bounds[\"n_nodes\"]) == int:\n",
    "            self.reservoir_matrices = ReservoirBuildingBlocks(model_type = self.model_type, \n",
    "                                                              random_seed = self.seed,\n",
    "                                                              n_nodes = self.bounds[\"n_nodes\"],\n",
    "                                                              input_weight_type = self.input_weight_type)\n",
    "        self.iteration_durations = []\n",
    "\n",
    "    def normalize_bounds(self, bounds):\n",
    "        \"\"\"Makes sure all bounds feeded into GPyOpt are scaled to the domain [0, 1],\n",
    "        to aid interpretation of convergence plots.\n",
    "\n",
    "        Scalings are saved in instance parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bounds : dicts\n",
    "            Contains dicts with boundary information\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scaled_bounds, scalings, intercepts : tuple\n",
    "            Contains scaled bounds (list of dicts in GPy style), the scaling applied (numpy array)\n",
    "            and an intercept (numpy array) to transform values back to their original domain\n",
    "        \"\"\"\n",
    "        scaled_bounds = []\n",
    "        scalings = []\n",
    "        intercepts = []\n",
    "        \n",
    "        non_fixed_params = []\n",
    "        \n",
    "        for name, domain in self.bounds.items():\n",
    "            # Get any fixed parmeters\n",
    "            if type(domain) == int or type(domain) == float:\n",
    "                # Take note\n",
    "                self.fixed_parameters.append(name)\n",
    "\n",
    "            # Free parameters\n",
    "            elif type(domain) == tuple:\n",
    "                # Bookkeeping\n",
    "                self.free_parameters.append(name)\n",
    "\n",
    "                # Get scaling\n",
    "                lower_bound = min(domain)\n",
    "                upper_bound = max(domain)\n",
    "                scale = upper_bound - lower_bound\n",
    "\n",
    "                # Transform to [0, 1] domain\n",
    "                #scaled_bound = {'name': name, 'type': 'continuous', 'domain': (0., 1.)} #torch.adjustment required\n",
    "                non_fixed_params.append(name)\n",
    "                \n",
    "                # Store\n",
    "                #scaled_bounds.append(scaled_bound)\n",
    "                scalings.append(scale)\n",
    "                intercepts.append(lower_bound)\n",
    "            else:\n",
    "                raise ValueError(\"Domain bounds not understood\")\n",
    "        \n",
    "        n_hyperparams = len(non_fixed_params)\n",
    "\n",
    "        scaled_bounds = torch.cat([torch.zeros(1,n_hyperparams), torch.ones((1, n_hyperparams))], 0)\n",
    "        \n",
    "        return scaled_bounds, torch.tensor(scalings), torch.tensor(intercepts) #torch.adjustment required\n",
    "\n",
    "    def denormalize_bounds(self, normalized_arguments):\n",
    "        \"\"\"Denormalize arguments to feed into model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        normalized_arguments : numpy array\n",
    "            Contains arguments in same order as bounds\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        denormalized_arguments : 1-D numpy array\n",
    "            Array with denormalized arguments\n",
    "\n",
    "        \"\"\"\n",
    "        denormalized_bounds = (normalized_arguments * self.bound_scalings) + self.bound_intercepts  #torch.adjustment required\n",
    "        return denormalized_bounds\n",
    "\n",
    "    def construct_arguments(self, x):\n",
    "        \"\"\"Constructs arguments for ESN input from input array.\n",
    "\n",
    "        Does so by denormalizing and adding arguments not involved in optimization,\n",
    "        like the random seed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 1-D numpy array\n",
    "            Array containing normalized parameter values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        arguments : dict\n",
    "            Arguments that can be fed into an ESN\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Denormalize free parameters\n",
    "        denormalized_values = self.denormalize_bounds(x)\n",
    "        arguments = dict(zip(self.free_parameters, denormalized_values.flatten()))\n",
    "\n",
    "        # Add fixed parameters\n",
    "        for name in self.fixed_parameters:\n",
    "            value = self.bounds[name]\n",
    "            arguments[name] = value\n",
    "\n",
    "        # Specific additions\n",
    "        #arguments['random_seed'] = self.seed\n",
    "        if 'regularization' in arguments:\n",
    "            arguments['regularization'] = 10. ** arguments['regularization']  # Log scale correction\n",
    "\n",
    "        log_vars = [\"cyclic_res_w\", \"cyclic_input_w\", \"cyclic_bias\"]\n",
    "\n",
    "        for var in log_vars:\n",
    "            if var in arguments:\n",
    "                arguments[var] = 10. ** arguments[var]  # Log scale correction\n",
    "\n",
    "        if 'connectivity' in arguments:\n",
    "            arguments['connectivity'] = 10. ** arguments['connectivity']  # Log scale correction\n",
    "\n",
    "        if 'llambda' in arguments:\n",
    "            arguments['llambda'] = 10. ** arguments['llambda']  # Log scale correction\n",
    "\n",
    "        if 'llambda2' in arguments:\n",
    "            arguments['llambda2'] = 10. ** arguments['llambda2']  # Log scale correction\n",
    "\n",
    "        if 'noise' in arguments:\n",
    "            arguments['noise'] = 10. ** arguments['noise']  # Log scale correction\n",
    "\n",
    "        if 'n_nodes' in arguments:\n",
    "            arguments['n_nodes'] = torch.tensor(arguments['n_nodes'], dtype = torch.int32).item()  # Discretize #torch.adjustment required\n",
    "\n",
    "        if not self.feedback is None:\n",
    "            arguments['feedback'] = self.feedback\n",
    "        \n",
    "        for argument, val_tensor in arguments.items():\n",
    "            \n",
    "            try:\n",
    "                arguments[argument] = arguments[argument].item()\n",
    "            except:\n",
    "                arguments[argument] = arguments[argument]\n",
    "        return arguments\n",
    "\n",
    "    def validate_data(self, y, x=None, verbose=True):\n",
    "        \"\"\"Validates inputted data against errors in shape and common mistakes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            A y-array to be checked (should be 2-D with series in columns)\n",
    "        x : numpy array or None\n",
    "            Optional x-array to be checked (should have same number of rows as y)\n",
    "        verbose: bool\n",
    "            Toggle to flag printed messages about common shape issues\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Throws ValueError when data is not in the correct format.\n",
    "\n",
    "        \"\"\"\n",
    "        # Check dimensions\n",
    "        if not y.ndim == 2:\n",
    "            raise ValueError(\"y-array is not 2 dimensional\")\n",
    "\n",
    "        if verbose and y.shape[0] < y.shape[1]:\n",
    "            print(\"Warning: y-array has more series (columns) than samples (rows). Check if this is correct\")\n",
    "\n",
    "        # Checks for x\n",
    "        if not x is None:\n",
    "\n",
    "            # Check dimensions\n",
    "            if not x.ndim == 2:\n",
    "                raise ValueError(\"x-array is not 2 dimensional\")\n",
    "\n",
    "            # Check shape equality\n",
    "            if x.shape[0] != y.shape[0]:\n",
    "                raise ValueError(\"y-array and x-array have different number of samples (rows)\")\n",
    "    \n",
    "    def eval_objective(self, x):\n",
    "        \"\"\"This is a helper function we use to unnormalize and evaluate a point\"\"\"\n",
    "\n",
    "        return self.HRC(x)#unnormalize(x, self.scaled_bounds))\n",
    "\n",
    "    \n",
    "\n",
    "    def objective_function(self, parameters, train_y, validate_y, train_x=None, validate_x=None, random_seed=None):\n",
    "        \"\"\"Returns selected error metric on validation set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters : array\n",
    "            Parametrization of the Echo State Network, in column vector shape: (n, 1).\n",
    "        train_y : array\n",
    "            Dependent variable of the training set\n",
    "        validate_y : array\n",
    "            Dependent variable of the validation set\n",
    "        train_x : array or None\n",
    "            Independent variable(s) of the training set\n",
    "        validate_x : array or None\n",
    "            Independent variable(s) of the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Score on provided validation set\n",
    "\n",
    "        \"\"\"\n",
    "        # Get arguments\n",
    "        arguments = self.construct_arguments(self.range_bounds)\n",
    "        #print(\"args\" + str(arguments))\n",
    "        #if pure_prediction = True:\n",
    "\n",
    "        # Build network\n",
    "        esn = self.model(**arguments, exponential = self.exp_weights, activation_function = self.activation_function,\n",
    "                obs_idx = self.obs_index, resp_idx = self.target_index, plot = False, model_type = self.model_type,\n",
    "                input_weight_type = self.input_weight_type)\n",
    "                #random_seed = self.random_seed) Distance_matrix = self.Distance_matrix)\n",
    "\n",
    "        # Train\n",
    "        esn.train(x=train_x, y=train_y, burn_in=self.esn_burn_in)\n",
    "\n",
    "        # Validation score\n",
    "\n",
    "        score = esn.test(x=validate_x, y=validate_y, scoring_method=self.scoring_method,\n",
    "                                         alpha=self.alpha)#, steps_ahead=self.steps_ahead) #validate_y.shape[0]) \n",
    "                         #steps_ahead=self.steps_ahead, alpha=self.alpha)\n",
    "        return score\n",
    "\n",
    "    ### Hayden Edit\n",
    "    def define_tr_val(self, inputs):\n",
    "        \"\"\"\n",
    "        Get indices\n",
    "        start_index = np.random.randint(viable_start, viable_stop)\n",
    "        train_stop_index = start_index + train_length\n",
    "        validate_stop_index = train_stop_index + validate_length\n",
    "\n",
    "        # Get samples\n",
    "        train_y = training_y[start_index: train_stop_index]\n",
    "        validate_y = training_y[train_stop_index: validate_stop_index]\n",
    "\n",
    "        if not training_x is None:\n",
    "            train_x = training_x[start_index: train_stop_index]\n",
    "            validate_x = training_x[train_stop_index: validate_stop_index]\n",
    "        else:\n",
    "            train_x = None\n",
    "            validate_x = None\n",
    "        \"\"\"\n",
    "        ####\n",
    "        #print(\"Hayden edit: parameters: \" + str(parameters))\n",
    "        #print(\"Hayden edit: fixed parameters: \" + str(self.fixed_parameters))\n",
    "        #print(\"Hayden edit: free parameters: \" + str(self.free_parameters))\n",
    "        ####\n",
    "        \n",
    "        start_index, random_seed = inputs[\"start_index\"], inputs[\"random_seed\"]\n",
    "        \n",
    "\n",
    "        # Get indices\n",
    "        #start_index = np.random.randint(self.viable_start, self.viable_stop)\n",
    "        train_stop_index = start_index + self.train_length\n",
    "        validate_stop_index = train_stop_index + self.validate_length\n",
    "        \n",
    "        \n",
    "\n",
    "        # Get samples\n",
    "        train_y = self.y[start_index: train_stop_index]\n",
    "        validate_y = self.y[train_stop_index: validate_stop_index]\n",
    "\n",
    "        if not self.x is None:\n",
    "            train_x = self.x[start_index: train_stop_index]\n",
    "            validate_x = self.x[train_stop_index: validate_stop_index]\n",
    "        else:\n",
    "            train_x = None\n",
    "            validate_x = None\n",
    "        #return(train_x, train_y, validate_x, validate_y)\n",
    "        # Loop through series and score result\n",
    "        #scores_ = []\n",
    "\n",
    "        \n",
    "        ###PARRALLEL LINES:\n",
    "        #score_ = self.objective_function(self.parameters, train_y, validate_y, train_x, validate_x, random_seed=random_seed)\n",
    "        #return([score_])\n",
    "        ### Lines of unknown utility\n",
    "        \n",
    "        #scores_.append(score_)\n",
    "        #for n in range(self.n_series):\n",
    "        #    score_ = self.objective_function(self.parameters, train_y[:, n].reshape(-1, 1),\n",
    "        #                                           validate_y[:, n].reshape(-1, 1), train_x, validate_x)\n",
    "        #    scores_.append(score_)\n",
    "        #print(scores_)\n",
    "        \n",
    "        return train_x, train_y, validate_x, validate_y\n",
    "        \n",
    "        \n",
    "    ###\n",
    "    def build_unq_dict_lst(self, lst1, lst2, key1 = \"start_index\", key2 = \"random_seed\"):\n",
    "        dict_lst = []\n",
    "        for i in range(len(lst1)):\n",
    "            for j in range(len(lst2)):\n",
    "                dictt = {}\n",
    "                dictt[key1] =  lst1[i]\n",
    "                dictt[key2] =  lst2[j]\n",
    "                dict_lst.append(dictt)\n",
    "\n",
    "        return dict_lst\n",
    "\n",
    "    def objective_sampler(self): #, parameters\n",
    "        \"\"\"Splits training set into train and validate sets, and computes multiple samples of the objective function.\n",
    "\n",
    "        This method also deals with dispatching multiple series to the objective function if there are multiple,\n",
    "        and aggregates the returned scores by averaging.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters : array\n",
    "            Parametrization of the Echo State Network\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean_score : 2-D array\n",
    "            Column vector with mean score(s), as required by GPyOpt\n",
    "\n",
    "        \"\"\"\n",
    "        # Get data\n",
    "        #self.parameters = parameters\n",
    "        training_y = self.y\n",
    "        training_x = self.x\n",
    "\n",
    "        # Get number of series\n",
    "        self.n_series = training_y.shape[1]\n",
    "\n",
    "        # Set viable sample range\n",
    "        viable_start = self.esn_burn_in\n",
    "        viable_stop = training_y.shape[0] - self.subsequence_length\n",
    "\n",
    "        # Get sample lengths\n",
    "        self.validate_length = torch.round(torch.tensor(self.subsequence_length * self.validate_fraction)).type(torch.int32).item()\n",
    "        self.train_length = self.subsequence_length - self.validate_length\n",
    "\n",
    "        # Score storage\n",
    "        scores = torch.zeros((self.cv_samples, self.n_res), dtype = torch.float32)\n",
    "\n",
    "        ### TORCH\n",
    "        #start_indices = np.random.randint(viable_start, viable_stop, size = self.cv_samples)\n",
    "        start_indices = torch.randint(low = viable_start, high = viable_stop, size = (self.cv_samples,))\n",
    "        start_indices = [index_tensor.item() for index_tensor in start_indices]\n",
    "        \n",
    "        if self.seed == None:\n",
    "            random_seeds  = torch.randint(0, 100000, size = (self.n_res,)) #TORCH\n",
    "        else:\n",
    "            random_seeds = [self.seed]\n",
    "\n",
    "        objective_inputs = self.build_unq_dict_lst(start_indices, random_seeds)\n",
    "\n",
    "       \n",
    "        \"\"\"\n",
    "        # Get samples\n",
    "        if (self.cv_samples * self.n_res)  > 1:\n",
    "            Pool = mp.Pool(self.cv_samples * self.n_res)\n",
    "\n",
    "            #get the asynch object:\n",
    "            results = list(zip(*mp.Pool.map(self.define_tr_val, objective_inputs)))\n",
    "\n",
    "            mp.Pool.close()\n",
    "            mp.Pool.join()\n",
    "        else:\n",
    "            results = self.define_tr_val(objective_inputs[0])\n",
    "\n",
    "        self.scores = torch.tensor(results).view(-1,1)\n",
    "\n",
    "        #.reshape(scores.shape)\n",
    "\n",
    "        # He is perhaps overlooking something essential to LCB here, it's hard to tell.\n",
    "        # Why do we only pass back a mean?\n",
    "        # Pass back as a column vector (as required by GPyOpt)\n",
    "        #mean_score = self.scores.mean()\n",
    "        mean_score = self.scores.mean()\n",
    "        \n",
    "        print('Score \\u03BC:' + str(round(np.log10(mean_score),4)), \", \\u03C3: \",  round(np.log10(self.scores.std()),4), \"seed\", random_seeds, \"n\", self.scores.shape[0])#, \"scores\", self.scores)#str(self.scores.std()),)\n",
    "            # pars = self.construct_arguments(parameters)\n",
    "\n",
    "        # Return scores\n",
    "        return mean_score #.reshape(-1, 1)\n",
    "        \"\"\"\n",
    "        return self.define_tr_val(objective_inputs[0])\n",
    "    \n",
    "    def HRC(self, parameters, backprop = False, plot_type = \"error\", *args): #Hayden's RC or Hillary lol\n",
    "        \"\"\"\n",
    "        This version of the RC helper function\n",
    "\n",
    "        Arguments:\n",
    "            parameterization\n",
    "            steps_ahead\n",
    "\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        def expand(x): \n",
    "            if type(x) == torch.Tensor:\n",
    "                expanded = torch.squeeze(x.clone().detach()).view(1,1)\n",
    "            else:\n",
    "                expanded = torch.squeeze(torch.tensor(x)).view(1,1)\n",
    "            return expanded\n",
    "\n",
    "        def rnd(x):\n",
    "            return torch.round(x*100).item()/100\n",
    "        \n",
    "        #print('params', parameters)\n",
    "        \n",
    "        batch_size = parameters.shape[0]\n",
    "\n",
    "        #can be parrallelized\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            #print(parameterization)\n",
    "            #spectr, connect, regul, leak_rate, bias = parameterization[i, :]\n",
    "            \n",
    "            # Get arguments\n",
    "            #params = {}\n",
    "            #for j, param in enumerate(self.params2optimize):\n",
    "            #    params[param] = parameters[i,j]\n",
    "            #args = dict(zip(self.params2optimize, parameters[i,:].tolist()))\n",
    "            arguments = self.construct_arguments(parameters)\n",
    "            \n",
    "            try:\n",
    "                res_args  = {\"reservoir\" : self.reservoir_matrices}\n",
    "                arguments = {**arguments, **res_args}\n",
    "            except:\n",
    "                assert 1 == 0\n",
    "                print(\"failed to load\")\n",
    "\n",
    "\n",
    "            # Build network\n",
    "            RC = self.model(**arguments, exponential = self.exp_weights, activation_function = self.activation_function,\n",
    "                obs_idx = self.obs_index, resp_idx = self.target_index,  model_type = self.model_type,\n",
    "                input_weight_type = self.input_weight_type, approximate_reservoir = self.approximate_reservoir)\n",
    "                # Distance_matrix = self.Distance_matrix)\n",
    "                # random_seed = random_seed) plot = False,\n",
    "\n",
    "            train_x, train_y, validate_x, validate_y = self.objective_sampler()\n",
    "            \n",
    "            RC.train(x = train_x, y = train_y,  burn_in=self.esn_burn_in) #y=train_y,\n",
    "\n",
    "            # Validation score\n",
    "            score, pred_ = RC.test(x=validate_x, y=validate_y, scoring_method='mse', steps_ahead = self.steps_ahead)\n",
    "\n",
    "            steps_displayed = pred_.shape[0]\n",
    "            \n",
    "            #plotting\n",
    "            if self.interactive:\n",
    "                \n",
    "                if len(self.errorz) % 5 == 0:\n",
    "                    self.ax[1].clear()\n",
    "                    self.ax[1].plot(validate_y, alpha = 0.4, color = \"blue\") #[:steps_displayed]\n",
    "\n",
    "                #plot 1:\n",
    "                log_resid = torch.log((validate_y - pred_)**2)\n",
    "                self.ax[0].plot(np.log(self.errorz)/np.log(10), alpha = 0.2, color = \"green\", label = \"all samples\")\n",
    "                self.ax[0].set_title(\"log error vs Bayes Opt. step\")\n",
    "                self.ax[0].set_ylabel(\"log(mse)\")\n",
    "                self.ax[0].set_xlabel(\"Bayesian Optimization step\")\n",
    "\n",
    "                #plot 2:\n",
    "                if pred_.shape[0] == 1:\n",
    "                    self.ax[1].plot(pred_, alpha = 0.3, color = \"red\") #[:steps_displayed]\n",
    "                else:\n",
    "                    self.ax[1].plot(pred_, alpha = 0.3, color = \"red\") #[pred_.shape[0] - 1, :]\n",
    "                #pred_[pred_.shape[0] - 1, :]\n",
    "                #self.ax[1].set_ylim(validate_y.min().item() - 0.1, validate_y.max().item() + 0.1 )\n",
    "                self.ax[1].set_ylim(-1, 1)\n",
    "                self.ax[1].set_title(\"all guesses\")\n",
    "                self.ax[1].set_ylabel(\"y\")\n",
    "                self.ax[1].set_xlabel(\"time step\")\n",
    "                self.ax[1].set_ylim(self.y.min().item() - 0.1, self.y.max().item() )\n",
    "\n",
    "                #plot 3:\n",
    "                self.ax[2].clear()\n",
    "                self.ax[2].plot(validate_y[:steps_displayed], alpha = 0.5, color = \"blue\", label = \"train\")\n",
    "\n",
    "                if pred_.shape[0] == 1:\n",
    "                    self.ax[2].plot(pred_[:steps_displayed], alpha = 0.3, color = \"red\")\n",
    "                else:\n",
    "                    self.ax[2].plot(pred_, alpha = 0.3, color = \"red\") #[pred_.shape[0] - 1, :]\n",
    "                #ax[2].plot(pred_[:steps_displayed], alpha = 0.5, color = \"red\", label = \"pred\")\n",
    "                self.ax[2].set_ylim(self.y.min().item() - 0.1, self.y.max().item() )\n",
    "                self.ax[2].set_title(\"guess vs step\")\n",
    "                self.ax[2].set_ylabel(\"y\")\n",
    "                self.ax[2].set_xlabel(\"time step\")\n",
    "                pl.legend()\n",
    "\n",
    "                display.clear_output(wait=True) \n",
    "                display.display(pl.gcf()) \n",
    "            \n",
    "            #make the maximum value of the error 10000 to avoid inf.\n",
    "            score = min(score, torch.tensor(1000))\n",
    "\n",
    "\n",
    "            if torch.isnan(score):\n",
    "                score = torch.tensor(1000)\n",
    "            \n",
    "            ###### Confusing use of score and error here, fix later\n",
    "            if not i:\n",
    "                error = score\n",
    "            else:\n",
    "                error = torch.cat([error, score], dim = 0)\n",
    "            self.errorz.append(score.item())\n",
    "            \n",
    "        \n",
    "        mean_score = score#self.scores.mean()\n",
    "        #log_mu = torch.log10(mean_score)#,4#round()\n",
    "        \n",
    "        #score_ = mean_score #- 0.1 * std_score\n",
    "        #print('success_tolerance', self.state.success_counter)\n",
    "        #print('failure_tolerance', self.state.failure_counter)\n",
    "        score_str = 'iter ' + str(self.count_) +': Score ' + f'{mean_score:.4f}'# +', log(\\u03BC):' + f'{log_mu:.4f}' \n",
    "        \n",
    "        #if scores.shape[0] > 1:\n",
    "            \n",
    "        #    std_score = np.log10(self.scores.std())\n",
    "\n",
    "        #    score_str += \", \\u03C3: \" + str(round(std_score, 4))\n",
    "        \"\"\"\n",
    "        score_str += \" seed \" + str(random_seeds) + \" n \" + str(self.scores.shape[0])\n",
    "\n",
    "        if not self.count_:\n",
    "            self.count_ = 1\n",
    "        else:\n",
    "            self.count_ +=1\n",
    "        if log_mu > -0.5:\n",
    "            printc(score_str, \"fail\")\n",
    "        elif log_mu  > -0.8:\n",
    "            printc(score_str, \"warning\")\n",
    "        elif log_mu > -1:\n",
    "            print(score_str)\n",
    "        elif log_mu > -1.5:\n",
    "            printc(score_str, \"blue\")\n",
    "        elif log_mu > -2 :\n",
    "            printc(score_str, \"cyan\")\n",
    "        elif log_mu > -2.5:\n",
    "            printc(score_str, \"green\")\n",
    "        \"\"\"\n",
    "        \n",
    "        stop = time.time()\n",
    "        self.iteration_durations.append(stop - start)\n",
    "\n",
    "        return -error\n",
    "    \n",
    "    \n",
    "    \n",
    "    def optimize(self, y, x=None, store_path=None):\n",
    "        \"\"\"Performs optimization (with cross-validation).\n",
    "\n",
    "        Uses Bayesian Optimization with Gaussian Process priors to optimize ESN hyperparameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            Column vector with target values (y-values)\n",
    "\n",
    "        x : numpy array or None\n",
    "            Optional array with input values (x-values)\n",
    "\n",
    "        store_path : str or None\n",
    "            Optional path where to store best found parameters to disk (in JSON)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        best_arguments : numpy array\n",
    "            The best parameters found during optimization\n",
    "\n",
    "        \"\"\"\n",
    "        # Checks\n",
    "        self.validate_data(y, x, self.verbose)\n",
    "        \n",
    "        # Initialize new random state\n",
    "        print(\"n inputs\")\n",
    "        self.reservoir_matrices.n_inputs_ = (y.shape[1] - 1) if type(x) == type(None) else x.shape[1]\n",
    "        print(\"n inputs\", self.reservoir_matrices.n_inputs_)\n",
    "        if self.approximate_reservoir:\n",
    "            self.reservoir_matrices.gen_in_weights()\n",
    "\n",
    "        self.random_state = torch.Generator().manual_seed(self.seed + 2)\n",
    "\n",
    "        # Temporarily store the data\n",
    "        self.x = x.type(torch.float32) if x is not None else None #torch.adjustment required\n",
    "\n",
    "        self.y = y.type(torch.float32)                            #torch.adjustment required\n",
    "\n",
    "        # Inform user\n",
    "        if self.verbose:\n",
    "            print(\"Model initialization and exploration run...\")\n",
    "            \n",
    "        if self.interactive:\n",
    "            self.fig, self.ax = pl.subplots(1,3, figsize = (16,4))\n",
    "            \n",
    "        self.errorz, self.errorz_step = [], []\n",
    "        \n",
    "        self.state = TurboState(dim, length_min = self.length_min, \n",
    "                                batch_size=self.batch_size, success_tolerance = self.eps)\n",
    "        #\n",
    "        \n",
    "        #assert 1 == 2\n",
    "        \n",
    "        \"\"\"\n",
    "        dim: int\n",
    "        batch_size: int\n",
    "        length: float = 0.8\n",
    "        length_min: float = 0.5 ** 7\n",
    "        length_max: float = 1.6\n",
    "        failure_counter: int = 0\n",
    "        failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "        success_counter: int = 0\n",
    "        success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "        best_value: float = -float(\"inf\")\n",
    "        restart_triggered: bool = False\n",
    "        \"\"\"\n",
    "        \n",
    "        #assert 1 == 2\n",
    "        \n",
    "        X_turbo = get_initial_points(self.scaled_bounds.shape[1], self.initial_samples)\n",
    "        \n",
    "        Y_turbo = torch.tensor(\n",
    "            [self.eval_objective(x.view(1,-1)) for x in X_turbo], dtype=dtype, device=device).unsqueeze(-1)\n",
    "        \n",
    "        count = 1\n",
    "        n_init = self.initial_samples\n",
    "        #append the errorz to errorz_step\n",
    "        self.errorz_step += [max(self.errorz)] * n_init\n",
    "\n",
    "        while not self.state.restart_triggered:  # Run until TuRBO converges\n",
    "            self.state.restart_triggered = True\n",
    "            # Fit a GP model\n",
    "            count +=1\n",
    "            train_Y = (Y_turbo - Y_turbo.mean()) / Y_turbo.std()\n",
    "            likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "            model = SingleTaskGP(X_turbo, train_Y, likelihood=likelihood)\n",
    "            mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "            fit_gpytorch_model(mll)\n",
    "\n",
    "            # Create a batch\n",
    "            X_next = generate_batch(\n",
    "                state=self.state,\n",
    "                model=model,\n",
    "                X=X_turbo,\n",
    "                Y=train_Y,\n",
    "                batch_size=self.batch_size,\n",
    "                n_candidates=min(5000, max(2000, 200 * dim)),\n",
    "                num_restarts=10,\n",
    "                raw_samples=512,\n",
    "                acqf=\"ts\",\n",
    "            )\n",
    "\n",
    "            #can be parallelized:\n",
    "            Y_next = torch.tensor(\n",
    "                                  [self.eval_objective(x.view(1,-1)) for x in X_next], \n",
    "                                   dtype=dtype, device=device).unsqueeze(-1)\n",
    "\n",
    "            # Update state\n",
    "            self.state = update_state(state=self.state, Y_next=Y_next)\n",
    "\n",
    "            # Append data\n",
    "            X_turbo = torch.cat((X_turbo, X_next), dim=0)\n",
    "            Y_turbo = torch.cat((Y_turbo, Y_next), dim=0)\n",
    "            \n",
    "\n",
    "            # Print current status\n",
    "            print( \n",
    "                f\"{len(X_turbo)}) Best score: {max(Y_next).item():.4f},  TR length: {self.state.length:.2e}\" + \n",
    "                f\" length {self.state.length}\"# Best value:.item() {state.best_value:.2e},\n",
    "            )\n",
    "            \n",
    "            \n",
    "\n",
    "            print( \n",
    "                f\"TR length: {self.state.length:.2e},\" +  f\" min length {self.state.length_min:.2e}\"# Best value:.item() {state.best_value:.2e},\n",
    "            )\n",
    "\n",
    "            self.errorz_step += [min(self.errorz)] * self.batch_size\n",
    "\n",
    "            assert len(self.errorz) == len(self.errorz_step), \"err len: {}, err step: {}\".format(len(self.errorz), len(self.errorz_step) )\n",
    "            \n",
    "            #best score so far.\n",
    "            if self.interactive:\n",
    "                self.ax[0].clear()\n",
    "                self.ax[0].plot(np.log(self.errorz_step)/np.log(10), alpha = 0.5, color = \"blue\", label = \"best value\")\n",
    "                \n",
    "                if len(self.errorz_step) == n_init + self.batch_size:\n",
    "                    self.ax[0].legend()\n",
    "\n",
    "        \n",
    "        # Save to disk if desired\n",
    "        if not store_path is None:\n",
    "            with open(store_path, 'w+') as output_file:\n",
    "                json.dump(best_arguments, output_file, indent=4)\n",
    "        \n",
    "        best_vals = x_temp[torch.argmax(y_temp)]\n",
    "        \n",
    "        try:\n",
    "            denormed_ = denormalize_bounds(best_vals)\n",
    "        except:\n",
    "            print(\"FAIL\")\n",
    "            \n",
    "        #####Bad temporary code to change it back into a dictionary\n",
    "        denormed_free_parameters = list(zip(esn_cv.free_parameters, denormed))\n",
    "        denormed_free_parameters = dict([ (item[0], item[1].item()) for item in denormed_free_parameters])\n",
    "        best_hyper_parameters = denormed_free_parameters\n",
    "        for fixed_parameter in esn_cv.fixed_parameters:\n",
    "            best_parameters = {fixed_parameter : esn_cv.bounds[fixed_parameter], **best_hyper_parameters }\n",
    "        \n",
    "        \n",
    "        # Return best parameters\n",
    "        return best_hyper_parameters #X_turbo, Y_turbo, state, best_vals, denormed_ #best_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "#### eps isn't what it used to be.\n",
    "\n",
    "bounds_dict = {\"spectral_radius\" : (0,1), \n",
    "               \"connectivity\" : (-5,0), \n",
    "               \"regularization\": (-4,4),\n",
    "               \"leaking_rate\" : (0, 1),\n",
    "               \"bias\" : (-3,3),\n",
    "               \"n_nodes\" : 1000, \n",
    "               \"feedback\": 1}\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 1000, \n",
    "                            steps_ahead = 1, scoring_method = \"mse\", interactive = False, random_seed = 123,\n",
    "                            eps = 3, initial_samples = 10, approximate_reservoir = True, length_min = 2**-1,\n",
    "                            batch_size = 50)\n",
    "vals = esn_cv.optimize(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "#### eps isn't what it used to be.\n",
    "\n",
    "bounds_dict = {\"spectral_radius\" : (0,1), \n",
    "               \"connectivity\" : (-5,0), \n",
    "               \"regularization\": (-4,4),\n",
    "               \"leaking_rate\" : 0.95,#(0, 1),\n",
    "               \"bias\" : (-3,3),\n",
    "               \"n_nodes\" : 1000, \n",
    "               \"feedback\": 1}\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 1000, \n",
    "                            steps_ahead = 1, scoring_method = \"mse\", interactive = False, random_seed = 123,\n",
    "                            eps = 3, initial_samples = 10, approximate_reservoir = False, length_min = 2 ** - 1,\n",
    "                            batch_size = 50)\n",
    "vals = esn_cv.optimize(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_temp, y_temp, state_temp, best_vals, denormed = vals\n",
    "denormed_free_parameters = list(zip(esn_cv.free_parameters, denormed))\n",
    "denormed_free_parameters = dict([ (item[0], item[1].item()) for item in denormed_free_parameters])\n",
    "best_parameters = denormed_free_parameters\n",
    "for fixed_parameter in esn_cv.fixed_parameters:\n",
    "    best_parameters = {fixed_parameter : esn_cv.bounds[fixed_parameter], **best_parameters}\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Later: multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "import torch.multiprocessing as mp\n",
    "#from torch.multiprocessing import Pool, Process, set_start_method\n",
    "#from model import MyModel\n",
    "\n",
    "def bs_func(tensor1, tensor2):\n",
    "    tensor3 = tensor1 * tensor2\n",
    "    return tensor3\n",
    "num_processes = 2\n",
    "processes = []\n",
    "for rank in range(num_processes):\n",
    "    p = mp.Process(target =bs_func, args=([1, 2],[2, 3],[3, 4],[4, 5],[5, 6]))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()\n",
    "processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = pl.subplots(1,3, figsize = (16,4))\n",
    "errorz, errorz_step = [], []\n",
    "X_turbo = get_initial_points(dim, n_init)\n",
    "Y_turbo = torch.tensor(\n",
    "    [eval_objective(x.view(1,-1)) for x in X_turbo], dtype=dtype, device=device).unsqueeze(-1)\n",
    "\n",
    "state = TurboState(dim, batch_size=batch_size)\n",
    "count = 1\n",
    "\n",
    "#append the errorz to errorz_step\n",
    "errorz_step += [np.max(errorz)] * n_init\n",
    "\n",
    "while not state.restart_triggered:  # Run until TuRBO converges\n",
    "    # Fit a GP model\n",
    "    count +=1\n",
    "    train_Y = (Y_turbo - Y_turbo.mean()) / Y_turbo.std()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    model = SingleTaskGP(X_turbo, train_Y, likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    \n",
    "    # Create a batch\n",
    "    X_next = generate_batch(\n",
    "        state=state,\n",
    "        model=model,\n",
    "        X=X_turbo,\n",
    "        Y=train_Y,\n",
    "        batch_size=batch_size,\n",
    "        n_candidates=min(5000, max(2000, 200 * dim)),\n",
    "        num_restarts=10,\n",
    "        raw_samples=512,\n",
    "        acqf=\"ts\",\n",
    "    )\n",
    "    \n",
    "    #can be parallelized:\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x.view(1,-1)) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Update state\n",
    "    state = update_state(state=state, Y_next=Y_next)\n",
    "\n",
    "    # Append data\n",
    "    X_turbo = torch.cat((X_turbo, X_next), dim=0)\n",
    "    Y_turbo = torch.cat((Y_turbo, Y_next), dim=0)\n",
    "\n",
    "    # Print current status\n",
    "    print(\n",
    "        f\"{len(X_turbo)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "    )\n",
    "    \n",
    "    errorz_step += [-state.best_value] * batch_size\n",
    "    \n",
    "    assert len(errorz) == len(errorz_step), \"err len: {}, err step: {}\".format(len(errorz), len(errorz_step) )\n",
    "    #best score so far.\n",
    "    ax[0].clear()\n",
    "    ax[0].plot(np.log(errorz_step), alpha = 0.5, color = \"blue\", label = \"best value\")\n",
    "    if len(errorz_step) == n_init + batch_size:\n",
    "        ax[0].legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EI\n",
    "As a baseline, we compare TuRBO to qEI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_ei = get_initial_points(dim, n_init)\n",
    "Y_ei = torch.tensor(\n",
    "    [eval_objective(x.view(1,-1)) for x in X_ei], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "while len(Y_ei) < len(Y_turbo):\n",
    "    train_Y = (Y_ei - Y_ei.mean()) / Y_ei.std()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    model = SingleTaskGP(X_ei, train_Y, likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    # Create a batch\n",
    "    ei = qExpectedImprovement(model, train_Y.max(), maximize=True)\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        ei,\n",
    "        bounds=torch.stack(\n",
    "            [\n",
    "                torch.zeros(dim, dtype=dtype, device=device),\n",
    "                torch.ones(dim, dtype=dtype, device=device),\n",
    "            ]\n",
    "        ),\n",
    "        q=batch_size,\n",
    "        num_restarts=10,\n",
    "        raw_samples=512,\n",
    "    )\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x.view(1,-1)) for x in candidate], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Append data\n",
    "    X_ei = torch.cat((X_ei, candidate), axis=0)\n",
    "    Y_ei = torch.cat((Y_ei, Y_next), axis=0)\n",
    "\n",
    "    # Print current status\n",
    "    print(f\"{len(X_ei)}) Best value: {Y_ei.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Sobol = (SobolEngine(dim, scramble=True).draw(len(X_turbo)).to(dtype=dtype, device=device))\n",
    "Y_Sobol = torch.tensor([eval_objective(x.view(1,-1)) for x in X_Sobol], dtype=dtype, device=device).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "\n",
    "\n",
    "\n",
    "names = [\"TuRBO-1\"]#, \"EI\", \"Sobol\"]\n",
    "runs = [Y_turbo]#, Y_ei, Y_Sobol]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, run in zip(names, runs):\n",
    "    fx = np.maximum.accumulate(run.cpu())\n",
    "    plt.plot(fx, marker=\"\", lw=3)\n",
    "\n",
    "#plt.plot([0, len(Y_turbo)], [fun.optimal_value, fun.optimal_value], \"k--\", lw=3)\n",
    "plt.xlabel(\"Function value\", fontsize=18)\n",
    "plt.xlabel(\"Number of evaluations\", fontsize=18)\n",
    "plt.title(\"10D Ackley\", fontsize=24)\n",
    "plt.xlim([0, len(Y_turbo)])\n",
    "plt.ylim([-20, 1])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend(\n",
    "    names + [\"Global optimal value\"],\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0, -0.08, 1, 1),\n",
    "    bbox_transform=plt.gcf().transFigure,\n",
    "    ncol=4,\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params():\n",
    "    \n",
    "    best_normalized_params = X_turbo[Y_turbo.numpy().argmax()].view(1,-1)\n",
    "    best_params = unnormalize(best_normalized_params, HRC.bounds)\n",
    "    #spectr, connect, regul, leak_rate, bias = parameterization[i, :]\n",
    "    best_params_dict = {}\n",
    "    param_list = \"spectral_radius\", \"connectivity\", \"regularization\", \"leaking_rate\", \"bias\" \n",
    "    exp_params = \"connectivity\", \"regularization\"\n",
    "    \n",
    "    best_params_dict = { param_list[i] : best_params[0,i].item() for i in range(len(param_list))}\n",
    "    \n",
    "    for log_param in exp_params:\n",
    "        best_params_dict[log_param] = 10 ** best_params_dict[log_param]\n",
    "        \n",
    "   \n",
    "    return best_params_dict\n",
    "best_params = get_best_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RC = esn(**best_params, feedback=True, backprop = False) #bias = -10,\n",
    "_ = RC.train( train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_len = 4000\n",
    "te_end = 4600 #6000\n",
    "\n",
    "#train = torch.FloatTensor(data[:tr_len].reshape(-1, 1))\n",
    "test = torch.FloatTensor(data[tr_len:te_end].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err, pred_ = RC.test(test, scoring_method = \"mse\", steps_ahead = 60)\n",
    "pl.figure(figsize = (16,4))\n",
    "pl.plot(pred_[len(pred_) - 1,:])\n",
    "pl.plot(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize = (14,6))\n",
    "\n",
    "plt.sca(axs[0]); RC.display_res_weights(); plt.title(\"res_weights\")\n",
    "plt.sca(axs[1]); RC.display_in_weights(); plt.title(\"in_weights\")\n",
    "plt.sca(axs[2]); RC.display_out_weights(); plt.title(\"out_weights\")\n",
    "\n",
    "plt.show()\n",
    "plt.figure(figsize = (14, 4))\n",
    "RC.plot_states()\n",
    "plt.title(\"states\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC.in_weights.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
