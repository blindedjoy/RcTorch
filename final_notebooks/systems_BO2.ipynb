{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8d2d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hayden/Desktop/RcTorchAsIs\n"
     ]
    }
   ],
   "source": [
    "cd /Users/hayden/Desktop/RcTorchAsIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57862fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./cp_rctorchprivate.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coastal-manual",
   "metadata": {
    "id": "coastal-manual"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RcTorch.esn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpylab\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRcTorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m odeint\n",
      "File \u001b[0;32m~/Desktop/RcTorchAsIs/RcTorch/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesn_cv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'RcTorch.esn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from RcTorch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e-rXL3fLBDU",
   "metadata": {
    "id": "4e-rXL3fLBDU"
   },
   "outputs": [],
   "source": [
    "# pip install rctorch==0.7162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-panel",
   "metadata": {
    "id": "needed-panel"
   },
   "outputs": [],
   "source": [
    "#this method will ensure that the notebook can use multiprocessing on jupyterhub or any other linux based system.\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\")\n",
    "except:\n",
    "    pass\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-albert",
   "metadata": {
    "id": "limiting-albert"
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def pltTr(x,y,clr='cyan', mark='o'):\n",
    "    plt.plot(x.detach().numpy(), y.detach().numpy(),\n",
    "             marker=mark, color=clr, markersize=8, label='truth', alpha = 0.9)\n",
    "\n",
    "def pltPred(x,y,clr='red', linS='-'):\n",
    "    plt.plot(x.detach().numpy(), y.detach().numpy(),\n",
    "             color=clr, marker='.', linewidth=2, label='RC')\n",
    "from decimal import Decimal\n",
    "\n",
    "def convert2pd(tensor1, tensor2):\n",
    "    pd_ = pd.DataFrame(np.hstack((tensor1.detach().cpu().numpy(), tensor2.detach().cpu().numpy())))\n",
    "    pd_.columns = [\"t\", \"y\"]\n",
    "    return pd_\n",
    "'%.2E' % Decimal('40800000000.00000000000000')\n",
    "\n",
    "def param(t,N,y0):\n",
    "    f = 1 - torch.exp(-t)\n",
    "    f_dot = 1 - f\n",
    "    #f = t\n",
    "    #f_dot=1\n",
    "    return y0 + f*N\n",
    "\n",
    "#define a reparameterization function\n",
    "def reparam(t, y0 = None, N = None, dN_dt = None, t_only = False):\n",
    "    f = 1 - torch.exp(-t)\n",
    "    f_dot = 1 - f\n",
    "    \n",
    "    if t_only:\n",
    "        return f, f_dot\n",
    "\n",
    "    y = y0 + N*f \n",
    "    if dN_dt:\n",
    "        ydot = dN_dt * f + f_dot * N\n",
    "    else:\n",
    "        ydot = None\n",
    "    return y, ydot\n",
    "\n",
    "def reparam(t, order = 1):\n",
    "    exp_t = torch.exp(-t)\n",
    "    \n",
    "    derivatives_of_g = []\n",
    "    \n",
    "    g = 1 - exp_t\n",
    "    \n",
    "    #0th derivative\n",
    "    derivatives_of_g.append(g)\n",
    "    \n",
    "    g_dot = 1 - g\n",
    "    return g, g_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-prescription",
   "metadata": {
    "id": "enhanced-prescription"
   },
   "outputs": [],
   "source": [
    "def force(X, A = 0):\n",
    "    return torch.zeros_like(X)\n",
    "lam =1\n",
    "def hamiltonian(x, p, lam = lam):\n",
    "    return (1/2)*(x**2 + p**2) + lam*x**4/4\n",
    "\n",
    "def custom_loss(X , y, ydot, out_weights, force_t = None, \n",
    "                reg = True, ode_coefs = None, mean = True,\n",
    "               enet_strength = None, enet_alpha = None, init_conds = None, lam = 1):\n",
    "    y, p = y[:,0].view(-1,1), y[:,1].view(-1,1)\n",
    "    ydot, pdot = ydot[:,0].view(-1,1), ydot[:,1].view(-1,1)\n",
    "    \n",
    "    #with paramization\n",
    "    L =  (ydot - p)**2 + (pdot + y + lam * y**3   - force_t)**2\n",
    "    \n",
    "    #if mean:\n",
    "    L = torch.mean(L)\n",
    "    \n",
    "    if reg:\n",
    "        #assert False\n",
    "        weight_size_sq = torch.mean(torch.square(out_weights))\n",
    "        weight_size_L1 = torch.mean(torch.abs(out_weights))\n",
    "        L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)\n",
    "        L = L + 0.1 * L_reg \n",
    "\n",
    "    y0, p0 = init_conds\n",
    "    ham = hamiltonian(y, p)\n",
    "    ham0 = hamiltonian(y0, p0)\n",
    "    L_H = (( ham - ham0).pow(2)).mean()\n",
    "    assert L_H >0\n",
    "\n",
    "    L = L +  0.1 * L_H\n",
    "    \n",
    "    #print(\"L1\", hi, \"L_elastic\", L_reg, \"L_H\", L_H)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-preparation",
   "metadata": {
    "id": "practical-preparation"
   },
   "outputs": [],
   "source": [
    "lineW = 3\n",
    "lineBoxW=2\n",
    "\n",
    "def optimize_last_layer(esn, \n",
    "                        SAVE_AFTER_EPOCHS = 1,\n",
    "                        epochs = 45000,\n",
    "                        custom_loss = custom_loss,\n",
    "                        EPOCHS_TO_TERMINATION = None,\n",
    "                        f = force,\n",
    "                        lr = 0.05, \n",
    "                        reg = None,\n",
    "                        plott = False,\n",
    "                        plot_every_n_epochs = 2000):#gamma 0.1, spikethreshold 0.07 works\n",
    "    with torch.enable_grad():\n",
    "        #define new_x\n",
    "        new_X = esn.extended_states.detach()\n",
    "        spikethreshold = esn.spikethreshold\n",
    "\n",
    "        #force detach states_dot\n",
    "        esn.states_dot = esn.states_dot.detach().requires_grad_(False)\n",
    "\n",
    "        #define criterion\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        #assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        #assert not new_X.requires_grad\n",
    "\n",
    "        #define previous_loss (could be used to do a convergence stop)\n",
    "        previous_loss = 0\n",
    "\n",
    "        #define best score so that we can save the best weights\n",
    "        best_score = 0\n",
    "\n",
    "        #define the optimizer\n",
    "        optimizer = optim.Adam(esn.parameters(), lr = lr)\n",
    "\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "        if esn.gamma_cyclic:\n",
    "            cyclic_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 10**-6, 0.01,\n",
    "                                            gamma = esn.gamma_cyclic,#0.9999,\n",
    "                                            mode = \"exp_range\", cycle_momentum = False)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=esn.gamma)\n",
    "        lrs = []\n",
    "\n",
    "        #define the loss history\n",
    "        loss_history = []\n",
    "\n",
    "        if plott:\n",
    "          #use pl for live plotting\n",
    "          fig, ax = pl.subplots(1,3, figsize = (16,4))\n",
    "\n",
    "        t = esn.X#.view(*N.shape).detach()\n",
    "        force_t = force(t)\n",
    "        g, g_dot = esn.G\n",
    "        y0  = esn.init_conds[0]\n",
    "\n",
    "        flipped = False\n",
    "        flipped2 = False\n",
    "        pow_ = -4\n",
    "        floss_last = 0\n",
    "\n",
    "\n",
    "        try:\n",
    "            assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        except:\n",
    "            esn.LinOut.weight.requires_grad_(True)\n",
    "            esn.LinOut.bias.requires_grad_(True)\n",
    "\n",
    "        #bail\n",
    "\n",
    "        #begin optimization loop\n",
    "        for e in range(epochs):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            N = esn.forward( esn.extended_states )\n",
    "            N_dot = esn.calc_Ndot(esn.states_dot)\n",
    "\n",
    "            y = g *N \n",
    "\n",
    "            ydot = g_dot * N + g * N_dot\n",
    "\n",
    "            y[:,0] = y[:,0] + esn.init_conds[0]\n",
    "            y[:,1] = y[:,1] + esn.init_conds[1]\n",
    "\n",
    "            #assert N.shape == N_dot.shape, f'{N.shape} != {N_dot.shape}'\n",
    "\n",
    "            #assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "\n",
    "            #total_ws = esn.LinOut.weight.shape[0] + 1\n",
    "            #weight_size_sq = torch.mean(torch.square(esn.LinOut.weight))\n",
    "\n",
    "            loss = custom_loss(esn.X, y, ydot, esn.LinOut.weight, reg = reg, ode_coefs = esn.ode_coefs,\n",
    "                    init_conds = esn.init_conds, enet_alpha= esn.enet_alpha, enet_strength = esn.enet_strength, force_t = force_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if esn.gamma_cyclic and e > 100 and e <5000:\n",
    "                cyclic_scheduler.step()\n",
    "                lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "\n",
    "            floss = float(loss)\n",
    "            loss_history.append(floss)\n",
    "\n",
    "            # if e == 10**3:\n",
    "            #     if floss > 10**(5):\n",
    "            #         EPOCHS_TO_TERMINATION = e + 50\n",
    "\n",
    "            # if e == 10**4:\n",
    "            #     if floss > 10**(2.5):\n",
    "            #         EPOCHS_TO_TERMINATION = e + 50\n",
    "                    \n",
    "            if e > 0:\n",
    "                loss_delta = float(np.log(floss_last) - np.log(floss)) \n",
    "                if loss_delta > esn.spikethreshold:# or loss_delta < -3:\n",
    "                    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "                    scheduler.step()\n",
    "\n",
    "\n",
    "            if not e and not best_score:\n",
    "                best_bias, best_weight, best_fit = esn.LinOut.bias.detach(), esn.LinOut.weight.detach(), y.clone()\n",
    "\n",
    "            if e > SAVE_AFTER_EPOCHS:\n",
    "                if not best_score:\n",
    "                    best_score = min(loss_history)\n",
    "                if floss < best_score:  \n",
    "                    best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "                    best_score = float(loss)\n",
    "                    best_fit = y.clone()\n",
    "                    best_ydot = ydot.clone()\n",
    "            # else:\n",
    "            #     if floss < best_score:\n",
    "            #         best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "            #         best_score = float(loss)\n",
    "            #         best_fit = y.clone()\n",
    "            #         best_ydot = ydot.clone()\n",
    "            \n",
    "            # if e >= EPOCHS_TO_TERMINATION and EPOCHS_TO_TERMINATION:\n",
    "            #     return {\"weights\": best_weight, \"bias\" : best_bias, \"y\" : best_fit, \n",
    "            #           \"loss\" : {\"loss_history\" : loss_history},  \"best_score\" : torch.tensor(best_score),\n",
    "            #           \"RC\" : esn}\n",
    "            floss_last = floss\n",
    "            if plott and e:\n",
    "\n",
    "                if e % plot_every_n_epochs == 0:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        print('lr', param_group['lr'])\n",
    "                    ax[0].clear()\n",
    "                    logloss_str = 'Log(L) ' + '%.2E' % Decimal((loss).item())\n",
    "                    delta_loss  = ' delta Log(L) ' + '%.2E' % Decimal((loss-previous_loss).item())\n",
    "\n",
    "                    print(logloss_str + \", \" + delta_loss)\n",
    "                    ax[0].plot(y.detach().cpu())\n",
    "                    ax[0].set_title(f\"Epoch {e}\" + \", \" + logloss_str)\n",
    "                    ax[0].set_xlabel(\"t\")\n",
    "\n",
    "                    ax[1].set_title(delta_loss)\n",
    "                    ax[1].plot(ydot.detach().cpu(), label = \"ydot\")\n",
    "                    #ax[0].plot(y_dot.detach(), label = \"dy_dx\")\n",
    "                    ax[2].clear()\n",
    "                    #weight_size = str(weight_size_sq.detach().item())\n",
    "                    #ax[2].set_title(\"loss history \\n and \"+ weight_size)\n",
    "\n",
    "                    ax[2].loglog(loss_history)\n",
    "                    ax[2].set_xlabel(\"t\")\n",
    "\n",
    "                    #[ax[i].legend() for i in range(3)]\n",
    "                    previous_loss = loss.item()\n",
    "\n",
    "                    #clear the plot outputt and then re-plot\n",
    "                    display.clear_output(wait=True) \n",
    "                    display.display(pl.gcf())\n",
    "\n",
    "\n",
    "        return {\"weights\": best_weight, \"bias\" : best_bias, \"y\" : best_fit, \"ydot\" : best_ydot, \n",
    "              \"loss\" : {\"loss_history\" : loss_history}, \"best_score\" : torch.tensor(best_score),\n",
    "              \"RC\" : esn}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-contractor",
   "metadata": {
    "id": "expensive-contractor"
   },
   "outputs": [],
   "source": [
    "#y0s = array([-1.  , -0.25,  0.5 ,  1.25])\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-exclusive",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "artificial-exclusive",
    "outputId": "2477f113-cd49-481b-9235-9eea5874d727"
   },
   "outputs": [],
   "source": [
    "log_vars = ['connectivity', 'llambda', 'llambda2', 'noise', 'regularization', 'dt', 'enet_strength']\n",
    "\n",
    "#trained to 20*pi\n",
    "hps = {'dt': 0.001,\n",
    "       'n_nodes': 500,\n",
    "       'connectivity': 0.019946997092875757,\n",
    "       'spectral_radius': 2.4289157390594482,\n",
    "       'regularization': 49.04219249279563,\n",
    "       'leaking_rate': 0.0032216429244726896,\n",
    "       'bias': 0.3808490037918091,\n",
    "       'enet_alpha': 0.2040003091096878,\n",
    "       'enet_strength': 0.07488961475845243,\n",
    "       'spikethreshold': 0.4231834411621094,\n",
    "       'gamma': .09350859373807907,\n",
    "       'gamma_cyclic' : 0.9999}\n",
    "\n",
    "\n",
    "\n",
    "for key, val in hps.items():\n",
    "    if key in log_vars:\n",
    "        print(key, np.log10(val))\n",
    "    else:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-liberal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "historic-liberal",
    "outputId": "6ca6dfca-0e06-41dc-d491-1ecac296d804"
   },
   "outputs": [],
   "source": [
    "BURN_IN = 500\n",
    "\n",
    "#declare the bounds dict. See above for which variables are optimized in linear vs logarithmic space.\n",
    "bounds_dict = {\"connectivity\" : (-2, -1.4), #(-2, -0.5), \n",
    "               \"spectral_radius\" : (2.2, 2.6),#(0.01, 1),\n",
    "               \"n_nodes\" : 500,\n",
    "               \"regularization\" : 1.69, #(-4.4, 2.6),\n",
    "               \"leaking_rate\" : (0.00322 - 0.002, 0.00322 + 0.002),\n",
    "               \"dt\" : -3,#-3,\n",
    "               \"bias\": (-0.5, 0.5),\n",
    "               \"enet_alpha\": (0.18, 0.22), #(0,1.0),\n",
    "               \"enet_strength\": (-1.32,-0.92),\n",
    "               \"spikethreshold\" : (0.35,0.45),\n",
    "               \"gamma\" : (0.08,0.12),\n",
    "               \"gamma_cyclic\" : (float(np.log10(0.9997)), float(np.log10(0.99999))),#(-0.002176919254274547, 0)\n",
    "               }\n",
    "#set up data\n",
    "x0, xf = 0, 4*np.pi\n",
    "nsteps = int(abs(xf - x0)/(10**bounds_dict[\"dt\"]))\n",
    "xtrain = torch.linspace(x0, xf, nsteps, requires_grad=False).view(-1,1)\n",
    "int(xtrain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-coordination",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "living-coordination",
    "outputId": "440ede74-3e13-4d21-a2ea-e2a98dc07165",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#declare the esn_cv optimizer: this class will run bayesian optimization to optimize the bounds dict.\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict,\n",
    "                            interactive = True, \n",
    "                            batch_size = 1, \n",
    "                            cv_samples = 1, \n",
    "                            initial_samples = 100,  #200\n",
    "                            subsequence_length = int(xtrain.shape[0] * 0.98),\n",
    "                            validate_fraction = 0.2,\n",
    "                            random_seed = 209, \n",
    "                            success_tolerance = 10,\n",
    "                            ODE_order = 1, \n",
    "                            length_min = 2 **(-8),\n",
    "                            esn_burn_in = BURN_IN, \n",
    "                            log_score = True,\n",
    "                            activation_function = torch.sin,\n",
    "                        act_f_prime = torch.cos,\n",
    "                            )\n",
    "#optimize:\n",
    "opt = True\n",
    "if opt:\n",
    "    opt_hps = esn_cv.optimize(\n",
    "                              x = xtrain.view(-1,1),\n",
    "                              reparam_f = reparam, \n",
    "                              ODE_criterion = custom_loss,\n",
    "                              init_conditions = [[1.1, 1.3], 1],#[[0,1], [0,1]], \n",
    "                              force = force,\n",
    "                              ode_coefs = [1, 1],\n",
    "                              rounds =1,\n",
    "                              backprop_f = optimize_last_layer, \n",
    "                              solve =  True, \n",
    "                              eq_system = True,\n",
    "                              n_outputs = 2,\n",
    "                              epochs =  5000,\n",
    "                              reg_type = \"ham\",\n",
    "                              tr_score_prop = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-oxford",
   "metadata": {
    "id": "instrumental-oxford"
   },
   "outputs": [],
   "source": [
    "if opt:\n",
    "    opt_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bZX9Yo1gLL8J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZX9Yo1gLL8J",
    "outputId": "c9ca4585-7b01-41be-dbb8-a660dd708807"
   },
   "outputs": [],
   "source": [
    "opt_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ncQX8tKLdMW",
   "metadata": {
    "id": "2ncQX8tKLdMW"
   },
   "outputs": [],
   "source": [
    "def f(u, t ,lam=0,A=0,W=1):\n",
    "            x,  px = u      # unpack current values of u\n",
    "            derivs = [px, -x - lam*x**3 +A*np.sin(W*t)]     # you write the derivative here\n",
    "            return derivs\n",
    "\n",
    "def convert_ode_coefs(t, ode_coefs):\n",
    "    \"\"\" converts coefficients from the string 't**n' or 't^n' where n is any float\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.tensor\n",
    "        input time tensor\n",
    "    ode_coefs: list\n",
    "        list of associated floats. List items can either be (int/floats) or ('t**n'/'t^n')\n",
    "    \"\"\"\n",
    "    type_t = type(t)\n",
    "    for i, coef in enumerate(ode_coefs):\n",
    "        if type(coef) == str:\n",
    "            if coef[0] == \"t\" and (coef[1] == \"*\" or (coef[1] == \"*\" and coef[2] == \"*\")):\n",
    "                pow_ = float(re.sub(\"[^0-9.-]+\", \"\", coef))\n",
    "                ode_coefs[i]  = t ** pow_\n",
    "                print(\"alterning ode_coefs\")\n",
    "        elif type(coef) in [float, int, type_t]:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, \"ode_coefs must be a list floats or strings of the form 't^pow', where pow is a real number.\"\n",
    "    return ode_coefs\n",
    "        \n",
    "# Scipy Solver   \n",
    "def NLosc_solution(t, x0,  px0, lam=0, A=0,W=1):\n",
    "    u0 = [x0, px0]\n",
    "    # Call the ODE solver\n",
    "    solPend = odeint(f, u0, t.cpu(), args=(lam,A,W,))\n",
    "    xP = solPend[:,0];        pxP = solPend[:,1];   \n",
    "    return xP, pxP\n",
    "\n",
    "def plot_predictions(RC, results, integrator_model, y0s, ax = None,  \n",
    "                     int_color = \"maroon\", RC_color = \"aquamarine\", RC_linestyle =':'):\n",
    "    \"\"\"plots a RC prediction and integrator model prediction for comparison\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    integrator model: function\n",
    "        the model to be passed to odeint which is a gold standard integrator numerical method\n",
    "        for solving ODE's written in Fortran. You may find the documentation here:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    X = RC.X.cpu().detach()\n",
    "    #int_sols = []\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "    \n",
    "    for i, y in enumerate(results[\"ys\"]):\n",
    "        y = y.cpu().detach()\n",
    "        if not i:\n",
    "            labels = [\"RC solver\",\"RC solver\", \"integrator\", \"integrator\"]\n",
    "        else:\n",
    "            labels = [None, None, None, None]\n",
    "        try:\n",
    "            labels\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #calculate the integrator prediction:\n",
    "        y_truth, p_truth  = NLosc_solution(RC.X.squeeze().data,y0s[i],1,lam=1, A=0, W= 0) \n",
    "        \n",
    "        #p = y[:,1].cpu()# + v0\n",
    "        #yy = y[:,0].cpu()# + y0\n",
    "        \n",
    "        #plot the integrator prediction\n",
    "        ax.plot(y_truth, p_truth,  color = int_color , linewidth = lineW+7, \n",
    "                label = labels[2])\n",
    "        \n",
    "        ax.plot(y[:,0], y[:,1], label = labels[0], \n",
    "                linewidth =lineW, color = RC_color, linestyle = RC_linestyle)#\"dodgerblue\")\n",
    "#         ax.plot(X, p, color = \"red\", alpha = 1.0, linewidth =3, \n",
    "#                 label = labels[3])\n",
    "        \n",
    "    ax.set_xlabel(r'$x(t)$')\n",
    "    ax.set_ylabel(r'$y(t)$')\n",
    "    ax.legend();\n",
    "    #return int_sols\n",
    "\n",
    "def force(X, A = 0):\n",
    "    return torch.zeros_like(X)\n",
    "\n",
    "def plot_rmsr(RC, results, force, ax = None):\n",
    "    \"\"\"plots the residuals of a RC prediction directly from the loss function\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    force: function\n",
    "        the force function describing the force term in the population equation\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10, 4))\n",
    "    X = RC.X.cpu().detach()\n",
    "    ys, ydots = results[\"ys\"], results[\"ydots\"]\n",
    "    \n",
    "    residuals = []\n",
    "    force_t = force(X)\n",
    "    for i, y in enumerate(ys):\n",
    "        ydot = ydots[i]\n",
    "        y = y.cpu().detach()\n",
    "        ydot = ydot.cpu().detach()\n",
    "        \n",
    "        ode_coefs = convert_ode_coefs(t = X, ode_coefs = RC.ode_coefs)\n",
    "        \n",
    "        resids = custom_loss(X, y, ydot, None, \n",
    "                             force_t = force_t, \n",
    "                             ode_coefs = RC.ode_coefs,\n",
    "                             mean = False,\n",
    "                             ham = False,\n",
    "                             init_conds = RC.init_conds)\n",
    "        if not i:\n",
    "            resids_tensor = resids\n",
    "            # label = r'{Individual Trajectory RMSR}'\n",
    "            label = 'Individual Trajectory Residuals'\n",
    "        else:\n",
    "            resids_tensor = torch.cat((resids_tensor, resids), axis = 1)\n",
    "            label = None\n",
    "        resids_specific_rmsr = torch.sqrt(resids/1) \n",
    "            \n",
    "        ax.plot(X, resids_specific_rmsr, color = \"orangered\", alpha = 0.4, label = label, linewidth = lineW-1)\n",
    "        residuals.append(resids)\n",
    "    \n",
    "    mean_resid = torch.mean(resids_tensor, axis =1)\n",
    "    rmsr = torch.sqrt(mean_resid)\n",
    "    ax.plot(X, rmsr, \n",
    "               color = \"blue\", \n",
    "               alpha = 0.9, \n",
    "               label = 'RMSR',\n",
    "               linewidth = lineW-0.5)\n",
    "\n",
    "    ax.legend(prop={\"size\":16});\n",
    "    \n",
    "    ax.set_xlabel(r'$t$')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel('RMSR')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def driven_force(X, A = 1):\n",
    "    return A * torch.sin(X)\n",
    "\n",
    "def no_force(X, A = 0):\n",
    "    return A\n",
    "\n",
    "#define a reparameterization function, empirically we find that g= 1-e^(-t) works well)\n",
    "def reparam(t, order = 1):\n",
    "    \n",
    "    exp_t = torch.exp(-t)\n",
    "    derivatives_of_g = []\n",
    "    \n",
    "    g = 1 - exp_t\n",
    "    g_dot = 1 - g\n",
    "    return g, g_dot\n",
    "    \n",
    "    #first derivative\n",
    "    \n",
    "    \n",
    "    #example code for higher derivatives:\n",
    "    #####################################\n",
    "    \n",
    "    #derivatives_of_g.append(g_dot)\n",
    "    #derivatives_of_g.append(g)\n",
    "#     for i in range(order):\n",
    "#         if i %2 == 0:\n",
    "#             #print(\"even\")\n",
    "#             derivatives_of_g.append(g_dot)\n",
    "#         else:\n",
    "#             #print(\"odd\")\n",
    "#             derivatives_of_g.append(-g_dot)\n",
    "#    return derivatives_of_g\n",
    "\n",
    "\n",
    "def custom_loss(X, y, ydot, out_weights, force_t = force, \n",
    "                reg = False, ode_coefs = None, mean = True,\n",
    "               enet_strength = None, enet_alpha = None, init_conds = None, lam = 1, ham = True):\n",
    "    \"\"\" The loss function of the ODE (in this case the population equation loss)\n",
    "    X: torch.tensor\n",
    "        The input (in the case of ODEs this is time t)\n",
    "    y: torch.tensor\n",
    "        The response variable\n",
    "    ydot: torch.tensor\n",
    "        The time derivative of the response variable\n",
    "    enet_strength: float\n",
    "        the magnitude of the elastic net regularization parameter. In this case there is no e-net regularization\n",
    "    enet_alpha: float\n",
    "        the proportion of the loss that is L2 regularization (ridge). 1-alpha is the L1 proportion (lasso).\n",
    "    ode_coefs: list\n",
    "        this list represents the ODE coefficients. They can be numbers or t**n where n is some real number.\n",
    "    force: function\n",
    "        this function needs to take the input time tensor and return a new tensor f(t)\n",
    "    reg: bool\n",
    "        if applicable (not in the case below) this will toggle the elastic net regularization on and off\n",
    "    reparam: function\n",
    "        a reparameterization function which needs to take in the time tensor and return g and gdot, which \n",
    "        is the reparameterized time function that satisfies the initial conditions.\n",
    "    init_conds: list\n",
    "        the initial conditions of the ODE.\n",
    "    mean: bool\n",
    "        if true return the cost (0 dimensional float tensor) else return the residuals (1 dimensional tensor)\n",
    "    ham : bool\n",
    "        if true use hamiltonian regularization.\n",
    "    lam : float\n",
    "        coefficient affecting the strength of the nonlinearity term.\n",
    "        \n",
    "    Returns:\n",
    "        the residuals or the cost depending on the mean argument (see above)\n",
    "    \"\"\"\n",
    "    \n",
    "    y, p = y[:,0].view(-1,1), y[:,1].view(-1,1)\n",
    "    ydot, pdot = ydot[:,0].view(-1,1), ydot[:,1].view(-1,1)\n",
    "    \n",
    "    #with paramization\n",
    "    L =  (ydot - p)**2 + (pdot + y + lam * y**3   - force(X))**2\n",
    "    \n",
    "    if mean:\n",
    "        L = torch.mean(L)\n",
    "    \n",
    "    if reg:\n",
    "        #assert False\n",
    "        weight_size_sq = torch.mean(torch.square(out_weights))\n",
    "        weight_size_L1 = torch.mean(torch.abs(out_weights))\n",
    "        L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)\n",
    "        L = L + 0.1 * L_reg \n",
    "    if ham:\n",
    "        y0, p0 = init_conds\n",
    "        ham = hamiltonian(y, p)\n",
    "        ham0 = hamiltonian(y0, p0)\n",
    "        L_H = (( ham - ham0).pow(2)).mean()\n",
    "        assert L_H >0\n",
    "\n",
    "        L = L +  0.1 * L_H\n",
    "    \n",
    "    #print(\"L1\", hi, \"L_elastic\", L_reg, \"L_H\", L_H)\n",
    "    return L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JJYn2M4HLknj",
   "metadata": {
    "id": "JJYn2M4HLknj"
   },
   "outputs": [],
   "source": [
    "nl_oscillator_hp_set = opt_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rpq-00HDLNIL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpq-00HDLNIL",
    "outputId": "536c7dee-a47f-44b1-c807-0a07dfcd969b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y0s = np.arange(0.7, 1.8, 0.2)\n",
    "v0 = 1\n",
    "\n",
    "RC = EchoStateNetwork(**nl_oscillator_hp_set, \n",
    "                       random_state = 209, \n",
    "                       feedback = False, \n",
    "                       id_ = 10,\n",
    "                       activation_f = torch.sin,\n",
    "                       act_f_prime = torch.cos,\n",
    "                       dtype = torch.float32, n_outputs = 2)\n",
    "\n",
    "train_args = {\"burn_in\" : int(BURN_IN), \n",
    "              \"ODE_order\" : 1,\n",
    "              \"force\" : force,\n",
    "              \"reparam_f\" : reparam,\n",
    "              \"init_conditions\" : [y0s, float(v0)],\n",
    "              \"ode_coefs\" :       [1, 1],\n",
    "              \"X\" :   xtrain.view(-1, 1),\n",
    "              \"eq_system\" : True,\n",
    "              #\"out_weights\" : out_weights\n",
    "              }\n",
    "#fit\n",
    "results = RC.fit(**train_args, \n",
    "                 SOLVE = True,\n",
    "                 train_score = True,\n",
    "                 backprop_f = optimize_last_layer, \n",
    "                 epochs = 10000,\n",
    "                 ODE_criterion = custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7sgGClYbMf_Q",
   "metadata": {
    "id": "7sgGClYbMf_Q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_ha1x8ALcDj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "P_ha1x8ALcDj",
    "outputId": "21730022-54ef-4617-daff-b4070eb28cfd"
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "fig = plt.figure(figsize = (7,9)); gs1 = gridspec.GridSpec(3, 3);\n",
    "ax = plt.subplot(gs1[:-1, :])\n",
    "\n",
    "plot_predictions(RC, results, NLosc_solution, y0s, ax = ax, int_color = \"black\", RC_color = \"cyan\")\n",
    "\n",
    "ax = plt.subplot(gs1[-1, :])\n",
    "# plot_data = plot_rmsr(pop_RC, \n",
    "#                           results, \n",
    "#                           force = no_force, \n",
    "#                           ax = ax)\n",
    "\n",
    "plot_data = plot_rmsr(RC,\n",
    "                      results, \n",
    "                      force = no_force, \n",
    "                      ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yM3WRr9RLydk",
   "metadata": {
    "id": "yM3WRr9RLydk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "systems_BO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "arm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
