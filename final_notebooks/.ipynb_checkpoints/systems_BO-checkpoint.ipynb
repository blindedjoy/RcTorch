{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0710c58",
   "metadata": {},
   "source": [
    "# RcTorch 2022 AITOOLs submission Notebook\n",
    "\n",
    "## Bayesian Optimization: 1st order system of ODE's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coastal-manual",
   "metadata": {
    "id": "coastal-manual"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from rctorch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e-rXL3fLBDU",
   "metadata": {
    "id": "4e-rXL3fLBDU"
   },
   "outputs": [],
   "source": [
    "# pip install rctorch==0.7162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "needed-panel",
   "metadata": {
    "id": "needed-panel"
   },
   "outputs": [],
   "source": [
    "#this method will ensure that the notebook can use multiprocessing on jupyterhub or any other linux based system.\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\")\n",
    "except:\n",
    "    pass\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "limiting-albert",
   "metadata": {
    "id": "limiting-albert"
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def pltTr(x,y,clr='cyan', mark='o'):\n",
    "    plt.plot(x.detach().numpy(), y.detach().numpy(),\n",
    "             marker=mark, color=clr, markersize=8, label='truth', alpha = 0.9)\n",
    "\n",
    "def pltPred(x,y,clr='red', linS='-'):\n",
    "    plt.plot(x.detach().numpy(), y.detach().numpy(),\n",
    "             color=clr, marker='.', linewidth=2, label='RC')\n",
    "from decimal import Decimal\n",
    "\n",
    "def convert2pd(tensor1, tensor2):\n",
    "    pd_ = pd.DataFrame(np.hstack((tensor1.detach().cpu().numpy(), tensor2.detach().cpu().numpy())))\n",
    "    pd_.columns = [\"t\", \"y\"]\n",
    "    return pd_\n",
    "'%.2E' % Decimal('40800000000.00000000000000')\n",
    "\n",
    "def param(t,N,y0):\n",
    "    f = 1 - torch.exp(-t)\n",
    "    f_dot = 1 - f\n",
    "    #f = t\n",
    "    #f_dot=1\n",
    "    return y0 + f*N\n",
    "\n",
    "#define a reparameterization function\n",
    "def reparam(t, y0 = None, N = None, dN_dt = None, t_only = False):\n",
    "    f = 1 - torch.exp(-t)\n",
    "    f_dot = 1 - f\n",
    "    \n",
    "    if t_only:\n",
    "        return f, f_dot\n",
    "\n",
    "    y = y0 + N*f \n",
    "    if dN_dt:\n",
    "        ydot = dN_dt * f + f_dot * N\n",
    "    else:\n",
    "        ydot = None\n",
    "    return y, ydot\n",
    "\n",
    "def reparam(t, order = 1):\n",
    "    exp_t = torch.exp(-t)\n",
    "    \n",
    "    derivatives_of_g = []\n",
    "    \n",
    "    g = 1 - exp_t\n",
    "    \n",
    "    #0th derivative\n",
    "    derivatives_of_g.append(g)\n",
    "    \n",
    "    g_dot = 1 - g\n",
    "    return g, g_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enhanced-prescription",
   "metadata": {
    "id": "enhanced-prescription"
   },
   "outputs": [],
   "source": [
    "def force(X, A = 0):\n",
    "    return torch.zeros_like(X)\n",
    "lam =1\n",
    "def hamiltonian(x, p, lam = lam):\n",
    "    return (1/2)*(x**2 + p**2) + lam*x**4/4\n",
    "\n",
    "def custom_loss(X , y, ydot, out_weights, force_t = None, \n",
    "                reg = True, ode_coefs = None, mean = True,\n",
    "               enet_strength = None, enet_alpha = None, init_conds = None, lam = 1):\n",
    "    y, p = y[:,0].view(-1,1), y[:,1].view(-1,1)\n",
    "    ydot, pdot = ydot[:,0].view(-1,1), ydot[:,1].view(-1,1)\n",
    "    \n",
    "    #with paramization\n",
    "    L =  (ydot - p)**2 + (pdot + y + lam * y**3   - force_t)**2\n",
    "    \n",
    "    #if mean:\n",
    "    L = torch.mean(L)\n",
    "    \n",
    "    if reg:\n",
    "        #assert False\n",
    "        weight_size_sq = torch.mean(torch.square(out_weights))\n",
    "        weight_size_L1 = torch.mean(torch.abs(out_weights))\n",
    "        L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)\n",
    "        L = L + 0.1 * L_reg \n",
    "\n",
    "    y0, p0 = init_conds\n",
    "    ham = hamiltonian(y, p)\n",
    "    ham0 = hamiltonian(y0, p0)\n",
    "    L_H = (( ham - ham0).pow(2)).mean()\n",
    "    assert L_H >0\n",
    "\n",
    "    L = L +  0.1 * L_H\n",
    "    \n",
    "    #print(\"L1\", hi, \"L_elastic\", L_reg, \"L_H\", L_H)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "practical-preparation",
   "metadata": {
    "id": "practical-preparation"
   },
   "outputs": [],
   "source": [
    "lineW = 3\n",
    "lineBoxW=2\n",
    "\n",
    "def optimize_last_layer(esn, \n",
    "                        SAVE_AFTER_EPOCHS = 1,\n",
    "                        epochs = 45000,\n",
    "                        custom_loss = custom_loss,\n",
    "                        EPOCHS_TO_TERMINATION = None,\n",
    "                        f = force,\n",
    "                        lr = 0.05, \n",
    "                        reg = None,\n",
    "                        plott = False,\n",
    "                        force_t = None,\n",
    "                        plot_every_n_epochs = 2000):#gamma 0.1, spikethreshold 0.07 works\n",
    "    with torch.enable_grad():\n",
    "        #define new_x\n",
    "        new_X = esn.extended_states.detach()\n",
    "        spikethreshold = esn.spikethreshold\n",
    "\n",
    "        #force detach states_dot\n",
    "        esn.states_dot = esn.states_dot.detach().requires_grad_(False)\n",
    "\n",
    "        #define criterion\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        #assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        #assert not new_X.requires_grad\n",
    "\n",
    "        #define previous_loss (could be used to do a convergence stop)\n",
    "        previous_loss = 0\n",
    "\n",
    "        #define best score so that we can save the best weights\n",
    "        best_score = 0\n",
    "\n",
    "        #define the optimizer\n",
    "        optimizer = optim.Adam(esn.parameters(), lr = lr)\n",
    "\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "        if esn.gamma_cyclic:\n",
    "            cyclic_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 10**-6, 0.01,\n",
    "                                            gamma = esn.gamma_cyclic,#0.9999,\n",
    "                                            mode = \"exp_range\", cycle_momentum = False)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=esn.gamma)\n",
    "        lrs = []\n",
    "\n",
    "        #define the loss history\n",
    "        loss_history = []\n",
    "\n",
    "        if plott:\n",
    "          #use pl for live plotting\n",
    "          fig, ax = pl.subplots(1,3, figsize = (16,4))\n",
    "\n",
    "        t = esn.X#.view(*N.shape).detach()\n",
    "        force_t = force(t)\n",
    "        g, g_dot = esn.G\n",
    "        y0  = esn.init_conds[0]\n",
    "\n",
    "        flipped = False\n",
    "        flipped2 = False\n",
    "        pow_ = -4\n",
    "        floss_last = 0\n",
    "\n",
    "\n",
    "        try:\n",
    "            assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        except:\n",
    "            esn.LinOut.weight.requires_grad_(True)\n",
    "            esn.LinOut.bias.requires_grad_(True)\n",
    "\n",
    "        #bail\n",
    "\n",
    "        #begin optimization loop\n",
    "        for e in range(epochs):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            N = esn.forward( esn.extended_states )\n",
    "            N_dot = esn._calc_Ndot(esn.states_dot)\n",
    "\n",
    "            y = g *N \n",
    "\n",
    "            ydot = g_dot * N + g * N_dot\n",
    "\n",
    "            y[:,0] = y[:,0] + esn.init_conds[0]\n",
    "            y[:,1] = y[:,1] + esn.init_conds[1]\n",
    "\n",
    "            #assert N.shape == N_dot.shape, f'{N.shape} != {N_dot.shape}'\n",
    "\n",
    "            #assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "\n",
    "            #total_ws = esn.LinOut.weight.shape[0] + 1\n",
    "            #weight_size_sq = torch.mean(torch.square(esn.LinOut.weight))\n",
    "\n",
    "            loss = custom_loss(esn.X, y, ydot, esn.LinOut.weight, reg = reg, ode_coefs = esn.ode_coefs,\n",
    "                    init_conds = esn.init_conds, enet_alpha= esn.enet_alpha, enet_strength = esn.enet_strength, force_t = force_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if esn.gamma_cyclic and e > 100 and e <5000:\n",
    "                cyclic_scheduler.step()\n",
    "                lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "\n",
    "            floss = float(loss)\n",
    "            loss_history.append(floss)\n",
    "\n",
    "            # if e == 10**3:\n",
    "            #     if floss > 10**(5):\n",
    "            #         EPOCHS_TO_TERMINATION = e + 50\n",
    "\n",
    "            # if e == 10**4:\n",
    "            #     if floss > 10**(2.5):\n",
    "            #         EPOCHS_TO_TERMINATION = e + 50\n",
    "                    \n",
    "            if e > 0:\n",
    "                loss_delta = float(np.log(floss_last) - np.log(floss)) \n",
    "                if loss_delta > esn.spikethreshold:# or loss_delta < -3:\n",
    "                    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "                    scheduler.step()\n",
    "\n",
    "\n",
    "            if not e and not best_score:\n",
    "                best_bias, best_weight, best_fit = esn.LinOut.bias.detach(), esn.LinOut.weight.detach(), y.clone()\n",
    "\n",
    "            if e > SAVE_AFTER_EPOCHS:\n",
    "                if not best_score:\n",
    "                    best_score = min(loss_history)\n",
    "                if floss < best_score:  \n",
    "                    best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "                    best_score = float(loss)\n",
    "                    best_fit = y.clone()\n",
    "                    best_ydot = ydot.clone()\n",
    "            # else:\n",
    "            #     if floss < best_score:\n",
    "            #         best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "            #         best_score = float(loss)\n",
    "            #         best_fit = y.clone()\n",
    "            #         best_ydot = ydot.clone()\n",
    "            \n",
    "            # if e >= EPOCHS_TO_TERMINATION and EPOCHS_TO_TERMINATION:\n",
    "            #     return {\"weights\": best_weight, \"bias\" : best_bias, \"y\" : best_fit, \n",
    "            #           \"loss\" : {\"loss_history\" : loss_history},  \"best_score\" : torch.tensor(best_score),\n",
    "            #           \"RC\" : esn}\n",
    "            floss_last = floss\n",
    "            if plott and e:\n",
    "\n",
    "                if e % plot_every_n_epochs == 0:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        print('lr', param_group['lr'])\n",
    "                    ax[0].clear()\n",
    "                    logloss_str = 'Log(L) ' + '%.2E' % Decimal((loss).item())\n",
    "                    delta_loss  = ' delta Log(L) ' + '%.2E' % Decimal((loss-previous_loss).item())\n",
    "\n",
    "                    print(logloss_str + \", \" + delta_loss)\n",
    "                    ax[0].plot(y.detach().cpu())\n",
    "                    ax[0].set_title(f\"Epoch {e}\" + \", \" + logloss_str)\n",
    "                    ax[0].set_xlabel(\"t\")\n",
    "\n",
    "                    ax[1].set_title(delta_loss)\n",
    "                    ax[1].plot(ydot.detach().cpu(), label = \"ydot\")\n",
    "                    #ax[0].plot(y_dot.detach(), label = \"dy_dx\")\n",
    "                    ax[2].clear()\n",
    "                    #weight_size = str(weight_size_sq.detach().item())\n",
    "                    #ax[2].set_title(\"loss history \\n and \"+ weight_size)\n",
    "\n",
    "                    ax[2].loglog(loss_history)\n",
    "                    ax[2].set_xlabel(\"t\")\n",
    "\n",
    "                    #[ax[i].legend() for i in range(3)]\n",
    "                    previous_loss = loss.item()\n",
    "\n",
    "                    #clear the plot outputt and then re-plot\n",
    "                    display.clear_output(wait=True) \n",
    "                    display.display(pl.gcf())\n",
    "\n",
    "\n",
    "        return {\"weights\": best_weight, \"bias\" : best_bias, \"y\" : best_fit, \"ydot\" : best_ydot, \n",
    "              \"loss\" : {\"loss_history\" : loss_history}, \"best_score\" : torch.tensor(best_score),\n",
    "              \"RC\" : esn}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "expensive-contractor",
   "metadata": {
    "id": "expensive-contractor"
   },
   "outputs": [],
   "source": [
    "#y0s = array([-1.  , -0.25,  0.5 ,  1.25])\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "artificial-exclusive",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "artificial-exclusive",
    "outputId": "2477f113-cd49-481b-9235-9eea5874d727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt -3.0\n",
      "n_nodes 500\n",
      "connectivity -1.7001224756240845\n",
      "spectral_radius 2.4289157390594482\n",
      "regularization 1.6905698776245117\n",
      "leaking_rate 0.0032216429244726896\n",
      "bias 0.3808490037918091\n",
      "enet_alpha 0.2040003091096878\n",
      "enet_strength -1.1255784034729004\n",
      "spikethreshold 0.4231834411621094\n",
      "gamma 0.09350859373807907\n",
      "gamma_cyclic 0.9999\n"
     ]
    }
   ],
   "source": [
    "log_vars = ['connectivity', 'llambda', 'llambda2', 'noise', 'regularization', 'dt', 'enet_strength']\n",
    "\n",
    "#trained to 20*pi\n",
    "hps = {'dt': 0.001,\n",
    "       'n_nodes': 500,\n",
    "       'connectivity': 0.019946997092875757,\n",
    "       'spectral_radius': 2.4289157390594482,\n",
    "       'regularization': 49.04219249279563,\n",
    "       'leaking_rate': 0.0032216429244726896,\n",
    "       'bias': 0.3808490037918091,\n",
    "       'enet_alpha': 0.2040003091096878,\n",
    "       'enet_strength': 0.07488961475845243,\n",
    "       'spikethreshold': 0.4231834411621094,\n",
    "       'gamma': .09350859373807907,\n",
    "       'gamma_cyclic' : 0.9999}\n",
    "\n",
    "\n",
    "\n",
    "for key, val in hps.items():\n",
    "    if key in log_vars:\n",
    "        print(key, np.log10(val))\n",
    "    else:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "historic-liberal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "historic-liberal",
    "outputId": "6ca6dfca-0e06-41dc-d491-1ecac296d804"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12566"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BURN_IN = 500\n",
    "\n",
    "#declare the bounds dict. See above for which variables are optimized in linear vs logarithmic space.\n",
    "bounds_dict = {\"log_connectivity\" : (-2, -1.4), #(-2, -0.5), \n",
    "               \"spectral_radius\" : (2.2, 2.6),#(0.01, 1),\n",
    "               \"n_nodes\" : 500,\n",
    "               \"regularization\" : 1.69, #(-4.4, 2.6),\n",
    "               \"leaking_rate\" : (0.00322 - 0.002, 0.00322 + 0.002),\n",
    "               \"log_dt\" : -3,#-3,\n",
    "               \"bias\": (-0.5, 0.5),\n",
    "               \"enet_alpha\": (0.18, 0.22), #(0,1.0),\n",
    "               \"log_enet_strength\": (-1.32,-0.92),\n",
    "               \"spikethreshold\" : (0.35,0.45),\n",
    "               \"gamma\" : (0.08,0.12),\n",
    "               \"gamma_cyclic\" : (float(np.log10(0.9997)), float(np.log10(0.99999))),#(-0.002176919254274547, 0)\n",
    "               }\n",
    "#set up data\n",
    "x0, xf = 0, 4*np.pi\n",
    "nsteps = int(abs(xf - x0)/(10**bounds_dict[\"log_dt\"]))\n",
    "xtrain = torch.linspace(x0, xf, nsteps, requires_grad=False).view(-1,1)\n",
    "int(xtrain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "living-coordination",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "living-coordination",
    "outputId": "440ede74-3e13-4d21-a2ea-e2a98dc07165",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEEDBACK: None , device: None\n",
      "parameters OrderedDict([('log_connectivity', (-2, -1.4)), ('spectral_radius', (2.2, 2.6)), ('n_nodes', 500), ('regularization', 1.69), ('leaking_rate', (0.0012200000000000002, 0.005220000000000001)), ('log_dt', -3), ('bias', (-0.5, 0.5)), ('enet_alpha', (0.18, 0.22)), ('log_enet_strength', (-1.32, -0.92)), ('spikethreshold', (0.35, 0.45)), ('gamma', (0.08, 0.12)), ('gamma_cyclic', (-0.00013030789173217684, -4.342966533881614e-06))])\n",
      "leaking_rate 0 1 (0.0012200000000000002, 0.005220000000000001)\n",
      "log_connectivity None 0.0 (-2, -1.4)\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 09:17:03,105\tINFO services.py:1470 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m,n 1 500\n",
      "in_weights torch.Size([500, 1]) m 1 n_in 1\n",
      "Model initialization and exploration run...\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::execute_objective()\u001b[39m (pid=78828, ip=127.0.0.1)\n  File \"/Users/hayden/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc_bayes.py\", line 726, in execute_objective\n    results = RC.fit(**fit_input, train_score = True, **train_args)\n  File \"/Users/hayden/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc.py\", line 1272, in fit\n    weight_dict = backprop_f(self, force_t = self.force_t, custom_loss = ODE_criterion, epochs = epochs)\n  File \"/var/folders/j8/qxt2xjjs43jc313rqwthwn780000gq/T/ipykernel_78808/491698057.py\", line 79, in optimize_last_layer\n  File \"/Users/hayden/miniforge3/envs/arm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1177, in __getattr__\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\nAttributeError: 'RcNetwork' object has no attribute 'calc_Ndot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc_bayes.py:1974\u001b[0m, in \u001b[0;36mRcBayesOpt.optimize\u001b[0;34m(self, n_trust_regions, max_evals, y, x, store_path, scoring_method, criterion, epochs, learning_rate, reparam_f, ODE_criterion, init_conditions, scale, force, backprop_f, backprop, ode_coefs, solve, rounds, tr_score_prop, q, eq_system, nonlinear_ode, reg_type, solve_sample_prop)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trust_regions):\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorz[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorz_step[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length_progress[i] \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m-> 1974\u001b[0m best_hyper_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_turbo_m\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_hyper_parameters\n",
      "File \u001b[0;32m~/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc_bayes.py:2567\u001b[0m, in \u001b[0;36mRcBayesOpt._turbo_m\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;66;03m#set up dict of turbo states\u001b[39;00m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 2567\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_initial_parallel_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2570\u001b[0m n_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_samples\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;66;03m# Run until TuRBO converges\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc_bayes.py:2248\u001b[0m, in \u001b[0;36mRcBayesOpt._execute_initial_parallel_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2246\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m objective_input \u001b[38;5;129;01min\u001b[39;00m objective_inputs:\n\u001b[0;32m-> 2248\u001b[0m     result_i \u001b[38;5;241m=\u001b[39m \u001b[43meval_objective_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_args_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2249\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result_i)\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_samples \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trust_regions\n",
      "File \u001b[0;32m~/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc_bayes.py:808\u001b[0m, in \u001b[0;36meval_objective_remote\u001b[0;34m(parallel_args_id, parameters, dtype, device, plot_type, *args)\u001b[0m\n\u001b[1;32m    798\u001b[0m num_processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameter_lst)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;66;03m#job_ids = [id(parameter_lst[i]) for i in range(num_processes)]\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m#What is returned by execute_objective:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    806\u001b[0m \n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m#parallel_arguments, parameters, X_turbo_spec\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexecute_objective\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_args_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_turbo_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_region_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameter_lst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m#the old way of sorting orgainizing to avoid problems with parallel job asynch, new method is id internal.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m#results = sorted(results, key=lambda x: x[1][\"trust_region_id\"]) \u001b[39;00m\n\u001b[1;32m    813\u001b[0m scores, result_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)) \n",
      "File \u001b[0;32m~/miniforge3/envs/arm/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/arm/lib/python3.9/site-packages/ray/worker.py:1831\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1829\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 1831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::execute_objective()\u001b[39m (pid=78828, ip=127.0.0.1)\n  File \"/Users/hayden/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc_bayes.py\", line 726, in execute_objective\n    results = RC.fit(**fit_input, train_score = True, **train_args)\n  File \"/Users/hayden/miniforge3/envs/arm/lib/python3.9/site-packages/rctorch/rc.py\", line 1272, in fit\n    weight_dict = backprop_f(self, force_t = self.force_t, custom_loss = ODE_criterion, epochs = epochs)\n  File \"/var/folders/j8/qxt2xjjs43jc313rqwthwn780000gq/T/ipykernel_78808/491698057.py\", line 79, in optimize_last_layer\n  File \"/Users/hayden/miniforge3/envs/arm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1177, in __getattr__\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\nAttributeError: 'RcNetwork' object has no attribute 'calc_Ndot'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "#declare the esn_cv optimizer: this class will run bayesian optimization to optimize the bounds dict.\n",
    "esn_cv = RcBayesOpt(bounds = bounds_dict,\n",
    "                            interactive = True, \n",
    "                            #batch_size = 1, \n",
    "                            cv_samples = 1, \n",
    "                            initial_samples = 100,  #200\n",
    "                            subsequence_prop = 0.98,\n",
    "                            validate_fraction = 0.2,\n",
    "                            random_seed = 209, \n",
    "                            success_tolerance = 10,\n",
    "                            ODE_order = 1, \n",
    "                            length_min = 2 **(-8),\n",
    "                            esn_burn_in = BURN_IN, \n",
    "                            log_score = True,\n",
    "                            activation_function = \"sin\",#torch.sin,\n",
    "                            #act_f_prime = torch.cos,\n",
    "                            n_outputs = 2,\n",
    "                            )\n",
    "#optimize:\n",
    "opt = True\n",
    "if opt:\n",
    "    opt_hps = esn_cv.optimize(\n",
    "                              x = xtrain.view(-1,1),\n",
    "                              reparam_f = reparam, \n",
    "                              ODE_criterion = custom_loss,\n",
    "                              init_conditions = [[1.1, 1.3], 1],#[[0,1], [0,1]], \n",
    "                              force = force,\n",
    "                              ode_coefs = [1, 1],\n",
    "                              rounds =1,\n",
    "                              backprop_f = optimize_last_layer, \n",
    "                              solve =  True, \n",
    "                              eq_system = True,\n",
    "                              max_evals = 200,\n",
    "                              epochs =  5000,\n",
    "                              reg_type = \"ham\",\n",
    "                              tr_score_prop = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "instrumental-oxford",
   "metadata": {
    "id": "instrumental-oxford"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt_hps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mopt_hps\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt_hps' is not defined"
     ]
    }
   ],
   "source": [
    "if opt:\n",
    "    opt_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ncQX8tKLdMW",
   "metadata": {
    "id": "2ncQX8tKLdMW"
   },
   "outputs": [],
   "source": [
    "def f(u, t ,lam=0,A=0,W=1):\n",
    "            x,  px = u      # unpack current values of u\n",
    "            derivs = [px, -x - lam*x**3 +A*np.sin(W*t)]     # you write the derivative here\n",
    "            return derivs\n",
    "\n",
    "def convert_ode_coefs(t, ode_coefs):\n",
    "    \"\"\" converts coefficients from the string 't**n' or 't^n' where n is any float\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.tensor\n",
    "        input time tensor\n",
    "    ode_coefs: list\n",
    "        list of associated floats. List items can either be (int/floats) or ('t**n'/'t^n')\n",
    "    \"\"\"\n",
    "    type_t = type(t)\n",
    "    for i, coef in enumerate(ode_coefs):\n",
    "        if type(coef) == str:\n",
    "            if coef[0] == \"t\" and (coef[1] == \"*\" or (coef[1] == \"*\" and coef[2] == \"*\")):\n",
    "                pow_ = float(re.sub(\"[^0-9.-]+\", \"\", coef))\n",
    "                ode_coefs[i]  = t ** pow_\n",
    "                print(\"alterning ode_coefs\")\n",
    "        elif type(coef) in [float, int, type_t]:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, \"ode_coefs must be a list floats or strings of the form 't^pow', where pow is a real number.\"\n",
    "    return ode_coefs\n",
    "        \n",
    "# Scipy Solver   \n",
    "def NLosc_solution(t, x0,  px0, lam=0, A=0,W=1):\n",
    "    u0 = [x0, px0]\n",
    "    # Call the ODE solver\n",
    "    solPend = odeint(f, u0, t.cpu(), args=(lam,A,W,))\n",
    "    xP = solPend[:,0];        pxP = solPend[:,1];   \n",
    "    return xP, pxP\n",
    "\n",
    "def plot_predictions(RC, results, integrator_model, y0s, ax = None,  \n",
    "                     int_color = \"maroon\", RC_color = \"aquamarine\", RC_linestyle =':'):\n",
    "    \"\"\"plots a RC prediction and integrator model prediction for comparison\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    integrator model: function\n",
    "        the model to be passed to odeint which is a gold standard integrator numerical method\n",
    "        for solving ODE's written in Fortran. You may find the documentation here:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    X = RC.X.cpu().detach()\n",
    "    #int_sols = []\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "    \n",
    "    for i, y in enumerate(results[\"ys\"]):\n",
    "        y = y.cpu().detach()\n",
    "        if not i:\n",
    "            labels = [\"RC solver\",\"RC solver\", \"integrator\", \"integrator\"]\n",
    "        else:\n",
    "            labels = [None, None, None, None]\n",
    "        try:\n",
    "            labels\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #calculate the integrator prediction:\n",
    "        y_truth, p_truth  = NLosc_solution(RC.X.squeeze().data,y0s[i],1,lam=1, A=0, W= 0) \n",
    "        \n",
    "        #p = y[:,1].cpu()# + v0\n",
    "        #yy = y[:,0].cpu()# + y0\n",
    "        \n",
    "        #plot the integrator prediction\n",
    "        ax.plot(y_truth, p_truth,  color = int_color , linewidth = lineW+7, \n",
    "                label = labels[2])\n",
    "        \n",
    "        ax.plot(y[:,0], y[:,1], label = labels[0], \n",
    "                linewidth =lineW, color = RC_color, linestyle = RC_linestyle)#\"dodgerblue\")\n",
    "#         ax.plot(X, p, color = \"red\", alpha = 1.0, linewidth =3, \n",
    "#                 label = labels[3])\n",
    "        \n",
    "    ax.set_xlabel(r'$x(t)$')\n",
    "    ax.set_ylabel(r'$y(t)$')\n",
    "    ax.legend();\n",
    "    #return int_sols\n",
    "\n",
    "def force(X, A = 0):\n",
    "    return torch.zeros_like(X)\n",
    "\n",
    "def plot_rmsr(RC, results, force, ax = None):\n",
    "    \"\"\"plots the residuals of a RC prediction directly from the loss function\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    force: function\n",
    "        the force function describing the force term in the population equation\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10, 4))\n",
    "    X = RC.X.cpu().detach()\n",
    "    ys, ydots = results[\"ys\"], results[\"ydots\"]\n",
    "    \n",
    "    residuals = []\n",
    "    force_t = force(X)\n",
    "    for i, y in enumerate(ys):\n",
    "        ydot = ydots[i]\n",
    "        y = y.cpu().detach()\n",
    "        ydot = ydot.cpu().detach()\n",
    "        \n",
    "        ode_coefs = convert_ode_coefs(t = X, ode_coefs = RC.ode_coefs)\n",
    "        \n",
    "        resids = custom_loss(X, y, ydot, None, \n",
    "                             force_t = force_t, \n",
    "                             ode_coefs = RC.ode_coefs,\n",
    "                             mean = False,\n",
    "                             ham = False,\n",
    "                             init_conds = RC.init_conds)\n",
    "        if not i:\n",
    "            resids_tensor = resids\n",
    "            # label = r'{Individual Trajectory RMSR}'\n",
    "            label = 'Individual Trajectory Residuals'\n",
    "        else:\n",
    "            resids_tensor = torch.cat((resids_tensor, resids), axis = 1)\n",
    "            label = None\n",
    "        resids_specific_rmsr = torch.sqrt(resids/1) \n",
    "            \n",
    "        ax.plot(X, resids_specific_rmsr, color = \"orangered\", alpha = 0.4, label = label, linewidth = lineW-1)\n",
    "        residuals.append(resids)\n",
    "    \n",
    "    mean_resid = torch.mean(resids_tensor, axis =1)\n",
    "    rmsr = torch.sqrt(mean_resid)\n",
    "    ax.plot(X, rmsr, \n",
    "               color = \"blue\", \n",
    "               alpha = 0.9, \n",
    "               label = 'RMSR',\n",
    "               linewidth = lineW-0.5)\n",
    "\n",
    "    ax.legend(prop={\"size\":16});\n",
    "    \n",
    "    ax.set_xlabel(r'$t$')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel('RMSR')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def driven_force(X, A = 1):\n",
    "    return A * torch.sin(X)\n",
    "\n",
    "def no_force(X, A = 0):\n",
    "    return A\n",
    "\n",
    "#define a reparameterization function, empirically we find that g= 1-e^(-t) works well)\n",
    "def reparam(t, order = 1):\n",
    "    \n",
    "    exp_t = torch.exp(-t)\n",
    "    derivatives_of_g = []\n",
    "    \n",
    "    g = 1 - exp_t\n",
    "    g_dot = 1 - g\n",
    "    return g, g_dot\n",
    "    \n",
    "    #first derivative\n",
    "    \n",
    "    \n",
    "    #example code for higher derivatives:\n",
    "    #####################################\n",
    "    \n",
    "    #derivatives_of_g.append(g_dot)\n",
    "    #derivatives_of_g.append(g)\n",
    "#     for i in range(order):\n",
    "#         if i %2 == 0:\n",
    "#             #print(\"even\")\n",
    "#             derivatives_of_g.append(g_dot)\n",
    "#         else:\n",
    "#             #print(\"odd\")\n",
    "#             derivatives_of_g.append(-g_dot)\n",
    "#    return derivatives_of_g\n",
    "\n",
    "\n",
    "def custom_loss(X, y, ydot, out_weights, force_t = force, \n",
    "                reg = False, ode_coefs = None, mean = True,\n",
    "               enet_strength = None, enet_alpha = None, init_conds = None, lam = 1, ham = True):\n",
    "    \"\"\" The loss function of the ODE (in this case the population equation loss)\n",
    "    X: torch.tensor\n",
    "        The input (in the case of ODEs this is time t)\n",
    "    y: torch.tensor\n",
    "        The response variable\n",
    "    ydot: torch.tensor\n",
    "        The time derivative of the response variable\n",
    "    enet_strength: float\n",
    "        the magnitude of the elastic net regularization parameter. In this case there is no e-net regularization\n",
    "    enet_alpha: float\n",
    "        the proportion of the loss that is L2 regularization (ridge). 1-alpha is the L1 proportion (lasso).\n",
    "    ode_coefs: list\n",
    "        this list represents the ODE coefficients. They can be numbers or t**n where n is some real number.\n",
    "    force: function\n",
    "        this function needs to take the input time tensor and return a new tensor f(t)\n",
    "    reg: bool\n",
    "        if applicable (not in the case below) this will toggle the elastic net regularization on and off\n",
    "    reparam: function\n",
    "        a reparameterization function which needs to take in the time tensor and return g and gdot, which \n",
    "        is the reparameterized time function that satisfies the initial conditions.\n",
    "    init_conds: list\n",
    "        the initial conditions of the ODE.\n",
    "    mean: bool\n",
    "        if true return the cost (0 dimensional float tensor) else return the residuals (1 dimensional tensor)\n",
    "    ham : bool\n",
    "        if true use hamiltonian regularization.\n",
    "    lam : float\n",
    "        coefficient affecting the strength of the nonlinearity term.\n",
    "        \n",
    "    Returns:\n",
    "        the residuals or the cost depending on the mean argument (see above)\n",
    "    \"\"\"\n",
    "    \n",
    "    y, p = y[:,0].view(-1,1), y[:,1].view(-1,1)\n",
    "    ydot, pdot = ydot[:,0].view(-1,1), ydot[:,1].view(-1,1)\n",
    "    \n",
    "    #with paramization\n",
    "    L =  (ydot - p)**2 + (pdot + y + lam * y**3   - force(X))**2\n",
    "    \n",
    "    if mean:\n",
    "        L = torch.mean(L)\n",
    "    \n",
    "    if reg:\n",
    "        #assert False\n",
    "        weight_size_sq = torch.mean(torch.square(out_weights))\n",
    "        weight_size_L1 = torch.mean(torch.abs(out_weights))\n",
    "        L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)\n",
    "        L = L + 0.1 * L_reg \n",
    "    if ham:\n",
    "        y0, p0 = init_conds\n",
    "        ham = hamiltonian(y, p)\n",
    "        ham0 = hamiltonian(y0, p0)\n",
    "        L_H = (( ham - ham0).pow(2)).mean()\n",
    "        assert L_H >0\n",
    "\n",
    "        L = L +  0.1 * L_H\n",
    "    \n",
    "    #print(\"L1\", hi, \"L_elastic\", L_reg, \"L_H\", L_H)\n",
    "    return L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JJYn2M4HLknj",
   "metadata": {
    "id": "JJYn2M4HLknj"
   },
   "outputs": [],
   "source": [
    "nl_oscillator_hp_set = opt_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rpq-00HDLNIL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpq-00HDLNIL",
    "outputId": "536c7dee-a47f-44b1-c807-0a07dfcd969b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y0s = np.arange(0.7, 1.8, 0.2)\n",
    "v0 = 1\n",
    "\n",
    "RC = EchoStateNetwork(**nl_oscillator_hp_set, \n",
    "                       random_state = 209, \n",
    "                       feedback = False, \n",
    "                       id_ = 10,\n",
    "                       activation_f = torch.sin,\n",
    "                       act_f_prime = torch.cos,\n",
    "                       dtype = torch.float32, n_outputs = 2)\n",
    "\n",
    "train_args = {\"burn_in\" : int(BURN_IN), \n",
    "              \"ODE_order\" : 1,\n",
    "              \"force\" : force,\n",
    "              \"reparam_f\" : reparam,\n",
    "              \"init_conditions\" : [y0s, float(v0)],\n",
    "              \"ode_coefs\" :       [1, 1],\n",
    "              \"X\" :   xtrain.view(-1, 1),\n",
    "              \"eq_system\" : True,\n",
    "              #\"out_weights\" : out_weights\n",
    "              }\n",
    "#fit\n",
    "results = RC.fit(**train_args, \n",
    "                 SOLVE = True,\n",
    "                 train_score = True,\n",
    "                 backprop_f = optimize_last_layer, \n",
    "                 epochs = 10000,\n",
    "                 ODE_criterion = custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7sgGClYbMf_Q",
   "metadata": {
    "id": "7sgGClYbMf_Q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_ha1x8ALcDj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "P_ha1x8ALcDj",
    "outputId": "21730022-54ef-4617-daff-b4070eb28cfd"
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "fig = plt.figure(figsize = (7,9)); gs1 = gridspec.GridSpec(3, 3);\n",
    "ax = plt.subplot(gs1[:-1, :])\n",
    "\n",
    "plot_predictions(RC, results, NLosc_solution, y0s, ax = ax, int_color = \"black\", RC_color = \"cyan\")\n",
    "\n",
    "ax = plt.subplot(gs1[-1, :])\n",
    "# plot_data = plot_rmsr(pop_RC, \n",
    "#                           results, \n",
    "#                           force = no_force, \n",
    "#                           ax = ax)\n",
    "\n",
    "plot_data = plot_rmsr(RC,\n",
    "                      results, \n",
    "                      force = no_force, \n",
    "                      ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yM3WRr9RLydk",
   "metadata": {
    "id": "yM3WRr9RLydk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "systems_BO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "arm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
