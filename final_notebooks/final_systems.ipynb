{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "muslim-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from RcTorch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import time\n",
    "\n",
    "#this method will ensure that the notebook can use multiprocessing (train multiple \n",
    "#RC's in parallel) on jupyterhub or any other linux based system.\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\")\n",
    "except:\n",
    "    pass\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "%matplotlib inline\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "injured-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineW = 3\n",
    "lineBoxW=2\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',#'bold',\n",
    "        'size'   : 24}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-roman",
   "metadata": {},
   "source": [
    "## Rc and systems of ODEs: an overview\n",
    "\n",
    "In this notebook we demonstrate that the RC can solve systems of differential equations. Any higher order ODE can be decomposed into a system of first order ODEs, hence solving systems of ODEs means that RC can solver higher order ODEs. This is a standard procedure followed by standard integrators . To apply the RC to systems, the RC architecture needs to be modified to return multiple outputs $N_j$, where $j$ indicates a different output. Specifically, the number of the outputs needs to be the same as the number of the equations in a system. Each output has a different set of weights $W_{out}^{(j)}$ while all the $N_j$ share the same hidden states.\n",
    "\n",
    "We exploit the RC solver in solving the equations of motion for a nonlinear Hamiltonian system, the nonlinear oscillator.  The energy is conserved in this system and thus, we adopt hamiltonian energy regularization that drastically accelerates the training and improves the fidelity of the predicted solutions. \n",
    "\n",
    "### Hamiltonian systems\n",
    "Hamiltonian systems  obey the energy conservation law. In other words, these systems are characterized by a hamiltonian function that represents the total energy of the system which remains constant in time. The hamiltonian of a nonlinear oscillator with unity mass and frequency is given by:\n",
    "\\begin{align}\n",
    "    \\label{eq:NL_ham}\n",
    "    \\mathcal{H} = \\frac{p^2}{2} + \\frac{x^2}{2} + \\frac{x^4}{4},\n",
    "\\end{align}\n",
    "and the associated equations of motion reads:\n",
    "\\begin{align}\n",
    "    \\label{eq:NL_x} \n",
    "    \\dot x &= p \\\\\n",
    "    \\label{eq:NL_p}\n",
    "    \\dot p &= -x - x^3\n",
    "\\end{align}\n",
    "where $p$ is the momentum and $x$ represents the position of the system. The loss function consists of three parts: $L_\\text{ODE}$ for the ODEs of x and p; a hamiltonian penalty $L_{\\mathcal{H}}$ that penalizes violations in the energy conservation and is defined by Eq\n",
    "\n",
    "Subsequently, the total $L$  reads:\n",
    "\\begin{align}\n",
    "    L &= L_\\text{ODE}+ L_{\\mathcal{H}} + L_\\text{reg} \\nonumber \\\\\n",
    "     \\label{eq:NL_loss}\n",
    "     &= \\sum_{n=0}^{K}\n",
    "      \\Big[ \\left(\\dot x_n-p_n\\right)^2 + \\left(\\dot p_n + x_n + x_n^3  \\right)^2 +\\left(E - \\mathcal{H}(x_n, p_n)\\right)^2 \\Big]   + \\lambda \\sum_{j=x,p} W_{out}^{(j)T} W_{out}^{(j)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "remarkable-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(RC, results, integrator_model, ax = None):\n",
    "    \"\"\"plots a RC prediction and integrator model prediction for comparison\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    integrator model: function\n",
    "        the model to be passed to odeint which is a gold standard integrator numerical method\n",
    "        for solving ODE's written in Fortran. You may find the documentation here:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    X = RC.X.cpu()\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "    for i, y in enumerate(results[\"ys\"]):\n",
    "        y = y.cpu()\n",
    "        if not i:\n",
    "            labels = [\"RC\", \"Integrator Solution\"]\n",
    "        else:\n",
    "            labels = [None, None]\n",
    "        ax.plot(X, y, color = \"dodgerblue\", label = labels[0], linewidth = lineW + 1, alpha = 0.9)\n",
    "\n",
    "        #calculate the integrator prediction:\n",
    "        int_sol = odeint(integrator_model, y0s[i], np.array(X.cpu().squeeze()))\n",
    "        int_sol = torch.tensor(int_sol)\n",
    "        \n",
    "        #plot the integrator prediction\n",
    "        ax.plot(X, int_sol, '--', color = \"red\", alpha = 0.9, label = labels[1],  linewidth = lineW)\n",
    "    \n",
    "    plt.ylabel(r'$y(t)$');\n",
    "    ax.legend();\n",
    "    ax.tick_params(labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def covert_ode_coefs(t, ode_coefs):\n",
    "    \"\"\" converts coefficients from the string 't**n' or 't^n' where n is any float\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.tensor\n",
    "        input time tensor\n",
    "    ode_coefs: list\n",
    "        list of associated floats. List items can either be (int/floats) or ('t**n'/'t^n')\n",
    "    \"\"\"\n",
    "    type_t = type(t)\n",
    "    for i, coef in enumerate(ode_coefs):\n",
    "        if type(coef) == str:\n",
    "            if coef[0] == \"t\" and (coef[1] == \"*\" or (coef[1] == \"*\" and coef[2] == \"*\")):\n",
    "                pow_ = float(re.sub(\"[^0-9.-]+\", \"\", coef))\n",
    "                ode_coefs[i]  = t ** pow_\n",
    "                print(\"alterning ode_coefs\")\n",
    "        elif type(coef) in [float, int, type_t]:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, \"ode_coefs must be a list floats or strings of the form 't^pow', where pow is a real number.\"\n",
    "    return ode_coefs\n",
    "    \n",
    "\n",
    "def plot_rmsr(RC, results, force, ax = None):\n",
    "    \"\"\"plots the residuals of a RC prediction directly from the loss function\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    force: function\n",
    "        the force function describing the force term in the population equation\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10, 4))\n",
    "    X = RC.X.cpu()\n",
    "    ys, ydots = results[\"ys\"], results[\"ydots\"]\n",
    "    \n",
    "    residuals = []\n",
    "    force_t = force(X)\n",
    "    for i, y in enumerate(ys):\n",
    "        ydot = ydots[i]\n",
    "        y = y.cpu()\n",
    "        ydot = ydot.cpu()\n",
    "        \n",
    "        ode_coefs = covert_ode_coefs(t = X, ode_coefs = RC.ode_coefs)\n",
    "        \n",
    "        resids = custom_loss(X, y, ydot, None, \n",
    "                             force_t = force_t, \n",
    "                             ode_coefs = RC.ode_coefs,\n",
    "                             mean = False)\n",
    "        if not i:\n",
    "            resids_tensor = resids\n",
    "            label = r'{Individual Trajectory RMSR}'\n",
    "        else:\n",
    "            resids_tensor = torch.cat((resids_tensor, resids), axis = 1)\n",
    "            label = None\n",
    "        resids_specific_rmsr = torch.sqrt(resids/1) \n",
    "            \n",
    "        ax.plot(X, resids_specific_rmsr, color = \"orangered\", alpha = 0.4, label = label, linewidth = lineW-1)\n",
    "        residuals.append(resids)\n",
    "    \n",
    "    mean_resid = torch.mean(resids_tensor, axis =1)\n",
    "    rmsr = torch.sqrt(mean_resid)\n",
    "    ax.plot(X, rmsr, \n",
    "               color = \"blue\", \n",
    "               alpha = 0.9, \n",
    "               label = r'{RMSR}',\n",
    "               linewidth = lineW-0.5)\n",
    "\n",
    "    ax.legend(prop={\"size\":16});\n",
    "    \n",
    "    ax.set_xlabel(r'$t$')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(r'{RMSR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gorgeous-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(u, t ,lam=0,A=0,W=1):\n",
    "            x,  px = u      # unpack current values of u\n",
    "            derivs = [px, -x - lam*x**3 +A*np.sin(W*t)]     # you write the derivative here\n",
    "            return derivs\n",
    "        \n",
    "# Scipy Solver   \n",
    "def NLosc_solution(t, x0,  px0, lam=0, A=0,W=1):\n",
    "    u0 = [x0, px0]\n",
    "    # Call the ODE solver\n",
    "    solPend = odeint(f, u0, t.cpu(), args=(lam,A,W,))\n",
    "    xP = solPend[:,0];        pxP = solPend[:,1];   \n",
    "    return xP, pxP\n",
    "\n",
    "def plot_results(RC, results, integrator_model, y0s, ax = None):\n",
    "    \"\"\"plots a RC prediction and integrator model prediction for comparison\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    integrator model: function\n",
    "        the model to be passed to odeint which is a gold standard integrator numerical method\n",
    "        for solving ODE's written in Fortran. You may find the documentation here:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    X = RC.X.cpu().detach()\n",
    "    #int_sols = []\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "    \n",
    "    for i, y in enumerate(results[\"ys\"]):\n",
    "        y = y.cpu().detach()\n",
    "        if not i:\n",
    "            labels = [\"RC\",\"integrator\"]\n",
    "        else:\n",
    "            labels = [None, None, None, None]\n",
    "        try:\n",
    "            labels\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ax.plot(y[:,0], y[:,1], label = labels[0], \n",
    "                linewidth =7, alpha = 0.9, color = \"dodgerblue\")\n",
    "        \n",
    "        \n",
    "        #calculate the integrator prediction:\n",
    "        y_truth, p_truth  = NLosc_solution(RC.X.squeeze().data,y0s[i],1,lam=1, A=0, W= 0) \n",
    "        \n",
    "        #p = y[:,1].cpu()# + v0\n",
    "        #yy = y[:,0].cpu()# + y0\n",
    "        \n",
    "        #plot the integrator prediction\n",
    "        ax.plot(y_truth, p_truth, color = \"red\", linewidth =3, \n",
    "                alpha = 1.0, label = labels[1])\n",
    "#         ax.plot(X, p, color = \"red\", alpha = 1.0, linewidth =3, \n",
    "#                 label = labels[3])\n",
    "        \n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"p\")\n",
    "    ax.set_ylim(-3,3)\n",
    "    ax.set_xlim(-3,3)\n",
    "    ax.legend();\n",
    "    #return int_sols\n",
    "\n",
    "def force(X, A = 0):\n",
    "    return torch.zeros_like(X)\n",
    "\n",
    "def plot_rmsr(RC, results, force, log = False, ax = None):\n",
    "    \"\"\"plots the residuals of a RC prediction directly from the loss function\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    force: function\n",
    "        the force function describing the force term in the population equation\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10, 4))\n",
    "    X = RC.X.cpu().detach()\n",
    "    ys, ydots = results[\"ys\"], results[\"ydots\"]\n",
    "    \n",
    "    residuals = []\n",
    "    for i, y in enumerate(ys):\n",
    "        y = y.cpu().detach()\n",
    "        ydot = ydots[i].cpu().detach()\n",
    "        resids = custom_loss(X, y, ydot, None, \n",
    "                             force = force, \n",
    "                             ode_coefs = RC.ode_coefs,\n",
    "                             mean = False,\n",
    "                            init_conds = RC.init_conds,\n",
    "                            ham = False)\n",
    "        if not i:\n",
    "            resids_tensor = resids\n",
    "            label = r'{Individual Trajectory RMSR}'\n",
    "        else:\n",
    "            resids_tensor = torch.cat((resids_tensor, resids), axis = 1)\n",
    "            label = None\n",
    "        resids_specific_rmsr = torch.sqrt(resids/1) \n",
    "            \n",
    "        ax.plot(X, resids_specific_rmsr, color = \"orangered\", alpha = 0.4, label = label, linewidth = lineW-1)\n",
    "        residuals.append(resids)\n",
    "    \n",
    "    mean_resid = torch.mean(resids_tensor, axis =1)\n",
    "    rmsr = torch.sqrt(mean_resid)\n",
    "    ax.plot(X, rmsr, \n",
    "               color = \"blue\", \n",
    "               alpha = 0.9, \n",
    "               label = r'{RMSR}',\n",
    "               linewidth = lineW-0.5)\n",
    "\n",
    "    ax.legend(prop={\"size\":16});\n",
    "    \n",
    "    ax.set_xlabel(r'$t$')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(r'{RMSR}')\n",
    "\n",
    "def driven_force(X, A = 1):\n",
    "    return A * torch.sin(X)\n",
    "\n",
    "def no_force(X, A = 0):\n",
    "    return A\n",
    "\n",
    "#define a reparameterization function, empirically we find that g= 1-e^(-t) works well)\n",
    "def reparam(t, order = 1):\n",
    "    \n",
    "    exp_t = torch.exp(-t)\n",
    "    derivatives_of_g = []\n",
    "    \n",
    "    g = 1 - exp_t\n",
    "    g_dot = 1 - g\n",
    "    return g, g_dot\n",
    "    \n",
    "    #first derivative\n",
    "    \n",
    "    \n",
    "    #example code for higher derivatives:\n",
    "    #####################################\n",
    "    \n",
    "    #derivatives_of_g.append(g_dot)\n",
    "    #derivatives_of_g.append(g)\n",
    "#     for i in range(order):\n",
    "#         if i %2 == 0:\n",
    "#             #print(\"even\")\n",
    "#             derivatives_of_g.append(g_dot)\n",
    "#         else:\n",
    "#             #print(\"odd\")\n",
    "#             derivatives_of_g.append(-g_dot)\n",
    "#    return derivatives_of_g\n",
    "\n",
    "\n",
    "def custom_loss(X, y, ydot, out_weights, force = force, \n",
    "                reg = False, ode_coefs = None, mean = True,\n",
    "               enet_strength = None, enet_alpha = None, init_conds = None, lam = 1, ham = True):\n",
    "    y, p = y[:,0].view(-1,1), y[:,1].view(-1,1)\n",
    "    ydot, pdot = ydot[:,0].view(-1,1), ydot[:,1].view(-1,1)\n",
    "    \n",
    "    #with paramization\n",
    "    L =  (ydot - p)**2 + (pdot + y + lam * y**3   - force(X))**2\n",
    "    \n",
    "    if mean:\n",
    "        L = torch.mean(L)\n",
    "    \n",
    "    if reg:\n",
    "        #assert False\n",
    "        weight_size_sq = torch.mean(torch.square(out_weights))\n",
    "        weight_size_L1 = torch.mean(torch.abs(out_weights))\n",
    "        L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)\n",
    "        L = L + 0.1 * L_reg \n",
    "    if ham:\n",
    "        y0, p0 = init_conds\n",
    "        ham = hamiltonian(y, p)\n",
    "        ham0 = hamiltonian(y0, p0)\n",
    "        L_H = (( ham - ham0).pow(2)).mean()\n",
    "        assert L_H >0\n",
    "\n",
    "        L = L +  0.1 * L_H\n",
    "    \n",
    "    #print(\"L1\", hi, \"L_elastic\", L_reg, \"L_H\", L_H)\n",
    "    return L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "provincial-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_last_layer(esn, \n",
    "                        SAVE_AFTER_EPOCHS = 1,\n",
    "                        epochs = 45000,\n",
    "                        custom_loss = custom_loss,\n",
    "                        loss_threshold = 10**-10,#10 ** -8,\n",
    "                        f = force,\n",
    "                        lr = 0.05, \n",
    "                        reg = None,\n",
    "                        plott = True,\n",
    "                        plot_every_n_epochs = 2000):#gamma 0.1, spikethreshold 0.07 works\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        #define new_x\n",
    "        new_X = esn.extended_states.detach()\n",
    "        spikethreshold = esn.spikethreshold\n",
    "\n",
    "        #force detach states_dot\n",
    "        esn.states_dot = esn.states_dot.detach().requires_grad_(False)\n",
    "\n",
    "        #define criterion\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        #assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        #assert not new_X.requires_grad\n",
    "\n",
    "        #define previous_loss (could be used to do a convergence stop)\n",
    "        previous_loss = 0\n",
    "\n",
    "        #define best score so that we can save the best weights\n",
    "        best_score = 0\n",
    "\n",
    "        #define the optimizer\n",
    "        optimizer = optim.Adam(esn.parameters(), lr = lr)\n",
    "\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "        if esn.gamma_cyclic:\n",
    "            cyclic_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 10**-6, 0.01,\n",
    "                                              gamma = esn.gamma_cyclic,#0.9999,\n",
    "                                              mode = \"exp_range\", cycle_momentum = False)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=esn.gamma)\n",
    "        lrs = []\n",
    "\n",
    "        #define the loss history\n",
    "        loss_history = []\n",
    "\n",
    "        if plott:\n",
    "            #use pl for live plotting\n",
    "            fig, ax = pl.subplots(1,3, figsize = (16,4))\n",
    "\n",
    "        t = esn.X#.view(*N.shape).detach()\n",
    "        g, g_dot = esn.G\n",
    "        y0  = esn.init_conds[0]\n",
    "\n",
    "        floss_last = 0\n",
    "\n",
    "\n",
    "        try:\n",
    "            assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        except:\n",
    "            esn.LinOut.weight.requires_grad_(True)\n",
    "            esn.LinOut.bias.requires_grad_(True)\n",
    "\n",
    "        #begin optimization loop\n",
    "        for e in range(epochs):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            N = esn.forward( esn.extended_states )\n",
    "            N_dot = esn.calc_Ndot(esn.states_dot)\n",
    "\n",
    "            y = g *N \n",
    "\n",
    "            ydot = g_dot * N + g * N_dot\n",
    "\n",
    "            y[:,0] = y[:,0] + esn.init_conds[0]\n",
    "            y[:,1] = y[:,1] + esn.init_conds[1]\n",
    "\n",
    "            assert N.shape == N_dot.shape, f'{N.shape} != {N_dot.shape}'\n",
    "\n",
    "            loss = custom_loss(esn.X, y, ydot, esn.LinOut.weight, reg = reg, ode_coefs = esn.ode_coefs,\n",
    "                              init_conds = esn.init_conds, enet_alpha= esn.enet_alpha, enet_strength = esn.enet_strength)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if esn.gamma_cyclic and e > 0 and e <5000:\n",
    "                cyclic_scheduler.step()\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "\n",
    "            floss = float(loss)\n",
    "            \n",
    "            if e > 0:\n",
    "                loss_delta = float(np.log(floss_last) - np.log(floss)) \n",
    "                if loss_delta > esn.spikethreshold:# or loss_delta < -3:\n",
    "                    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "                    scheduler.step()\n",
    "            \n",
    "            if not e and not best_score:\n",
    "                best_bias, best_weight, best_fit = esn.LinOut.bias.detach(), esn.LinOut.weight.detach(), y.clone()\n",
    "\n",
    "            if e > SAVE_AFTER_EPOCHS:\n",
    "                if not best_score:\n",
    "                    best_score = min(loss_history)\n",
    "                best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "                best_score = float(loss)\n",
    "                best_fit = y.clone()\n",
    "                best_pred = y.clone()\n",
    "                best_ydot = ydot.clone()\n",
    "            \n",
    "            loss_history.append(floss)\n",
    "            floss_last = floss\n",
    "            if plott and e:\n",
    "\n",
    "                if e % plot_every_n_epochs == 0:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        print('lr', param_group['lr'])\n",
    "                    ax[0].clear()\n",
    "                    logloss_str = 'Log(L) ' + '%.2E' % Decimal((loss).item())\n",
    "                    delta_loss  = ' delta Log(L) ' + '%.2E' % Decimal((loss-previous_loss).item())\n",
    "\n",
    "                    print(logloss_str + \", \" + delta_loss)\n",
    "                    ax[0].plot(y.detach().cpu(), label = \"exact\")\n",
    "                    ax[0].set_title(f\"Epoch {e}\" + \", \" + logloss_str)\n",
    "                    ax[0].set_xlabel(\"t\")\n",
    "\n",
    "                    ax[1].set_title(delta_loss)\n",
    "                    ax[1].plot(N_dot.detach().cpu())\n",
    "                    #ax[0].plot(y_dot.detach(), label = \"dy_dx\")\n",
    "                    ax[2].clear()\n",
    "                    #weight_size = str(weight_size_sq.detach().item())\n",
    "                    #ax[2].set_title(\"loss history \\n and \"+ weight_size)\n",
    "\n",
    "                    ax[2].loglog(loss_history)\n",
    "                    ax[2].set_xlabel(\"t\")\n",
    "\n",
    "                    [ax[i].legend() for i in range(3)]\n",
    "                    previous_loss = loss.item()\n",
    "\n",
    "                    #clear the plot outputt and then re-plot\n",
    "                    display.clear_output(wait=True) \n",
    "                    display.display(pl.gcf())\n",
    "\n",
    "\n",
    "        return {\"weights\": best_weight, \n",
    "                \"bias\" : best_bias, \n",
    "                \"loss\" : {\"loss_history\" : loss_history},\n",
    "                \"ydot\" : best_ydot, \n",
    "                \"y\" : best_pred,\n",
    "                \"best_score\" : best_score}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "active-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force(X, A = 0):\n",
    "    return torch.zeros_like(X)\n",
    "lam =1\n",
    "def hamiltonian(x, p, lam = lam):\n",
    "    return (1/2)*(x**2 + p**2) + lam*x**4/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "official-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "BURN_IN = 1000\n",
    "x0,xf, nsteps = 0, 5, 1000\n",
    "xtrain = torch.linspace(x0, xf, steps = nsteps, requires_grad=False)\n",
    "\n",
    "#the length of xtrain won't matter above. Only dt , x0, and xf matter for ODEs.\n",
    "#the reason for this is that the input time vector is reconstructed internally in rctorch\n",
    "#in order to satisfy the specified dt.\n",
    "xtrain = torch.linspace(x0, xf, steps = nsteps, requires_grad=False).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accurate-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_oscillator_hp_set = {'dt': 0.001,\n",
    "                        'regularization': 48.97788193684461,\n",
    "                        'n_nodes': 500,\n",
    "                        'connectivity': 0.017714821964432213,\n",
    "                        'spectral_radius': 2.3660330772399902,\n",
    "                        'leaking_rate': 0.0024312976747751236,\n",
    "                        'bias': 0.37677669525146484,\n",
    "                        'enet_alpha': 0.2082211971282959,\n",
    "                        'enet_strength': 0.118459548397668,\n",
    "                        'spikethreshold': 0.43705281615257263,\n",
    "                        'gamma': 0.09469877928495407,\n",
    "                        'gamma_cyclic': 0.999860422666841}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raising-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010000612819567323"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = 10*np.pi#10*np.pi\n",
    "x0, xf= 0, base\n",
    "nsteps = int(abs(xf - x0)/(nl_oscillator_hp_set[\"dt\"]))\n",
    "xtrain = torch.linspace(x0, xf, nsteps, requires_grad=False).view(-1,1)\n",
    "float(xtrain[1]- xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y0s = np.arange(0.7, 1.8, 0.2)\n",
    "v0 = 1\n",
    "\n",
    "RC = EchoStateNetwork(**nl_oscillator_hp_set, \n",
    "                       random_state = 209, \n",
    "                       id_ = 10,\n",
    "                       activation_f = torch.sin,\n",
    "                       act_f_prime = torch.cos,\n",
    "                       dtype = torch.float32, n_outputs = 2)\n",
    "\n",
    "train_args = {\"burn_in\" : int(BURN_IN), \n",
    "              \"ODE_order\" : 1,\n",
    "              \"force\" : force,\n",
    "              \"reparam_f\" : reparam,\n",
    "              \"init_conditions\" : [y0s, float(v0)],\n",
    "              \"ode_coefs\" :       [1, 1],\n",
    "              \"X\" :   xtrain.view(-1, 1),\n",
    "              \"eq_system\" : True,\n",
    "              #\"out_weights\" : out_weights\n",
    "              }\n",
    "#fit\n",
    "results = RC.fit(**train_args, \n",
    "                 SOLVE = True,\n",
    "                 train_score = True,\n",
    "                 backprop_f = optimize_last_layer, \n",
    "                 epochs = 10000,\n",
    "                 ODE_criterion = custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (14,4))\n",
    "plot_results(RC, results, NLosc_solution, y0s, ax = ax[0])\n",
    "plot_data = plot_rmsr(RC,\n",
    "                      results, \n",
    "                      force = no_force, \n",
    "                      log = True, \n",
    "                      ax = ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f'Total notebook runtime: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-priority",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
