{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "## Advanced-Sections: Homework 3 - Echo-State Reservoir Computing (AKA HW6-209)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2020**<br/>\n",
    "**Instructors**: Mark Glickman, Pavlos Protopapas, & Chris Tanner \n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)\n",
    "import os\n",
    "import pathlib\n",
    "working_dir = pathlib.Path().absolute()\n",
    "# Uncomment the line below to help debug if the path to included images don't show\n",
    "#print(working_dir)\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- This homework can be submitted in pairs.\n",
    "\n",
    "- If you submit individually but you have worked with someone, please include the name of your **one** partner below.\n",
    "- Please restart the kernel and run the entire notebook again before you submit. (Exception - you may skip the cells where you train neural networks, running the cells which load previously saved weights instead. However, **don't delete/overwrite the output that model.fit produced during training!**)\n",
    "\n",
    "<br><BR>\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reservoir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f026114178f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# The pyESN.py file must be in the same directory with this notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#from pyESN import ESN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreservoir\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reservoir'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This is a class for reservoir computing\n",
    "# The pyESN.py file must be in the same directory with this notebook\n",
    "#from pyESN import ESN\n",
    "from rctorch import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "We discussed in class the formulation of Reservoir Computing (RC), an echo-state recurrent neural network. One of the examples that we discussed was the Mackey-Glass nonlinear dynamical system. This example can be found in the seminal paper: http://www.rctn.org/vs265/jaeger04-ESN.pdf. This is the paper that introduced RC, so we highly encourage you to read it.\n",
    "\n",
    "In this homework, you are asked to work on the Rossler dynamical system. It is a very popular chaotic system that has been used to describe the evolution of chemical reactions. For more information check the Wikipedia page:  https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor\n",
    "For the implementation of the RC you have to use the class `pyESN` which is available at  https://github.com/cknd/pyESN. In this github repository you can also find the Mackey-Glass example. We encourage you to explore this library. \n",
    "\n",
    "In the homework, you have to employ the RC network to predict the time evolution of a chaotic time series.  We provide you three time series `x(t), y(t), z(t)` (files: `x.dat`, `y.dat`, `z.dat`), which are the solutions of the chaotic Rossler system. \n",
    "\n",
    "In the first question you are asked to make a short-range forecast. It is a prediction where the network is learning from the past (training set) and trying to make a future prediction based on the past. In this case the prediction is not a response to the previous signal, therefore, the input should be an array of ones. \n",
    "\n",
    "In the second  question you are asked to make a long-range forecast. You have to confirm that making a prediction by learning only the past yields very bad performance. This is expected because we deal with a very difficult (chaotic) time-series. On the other hand, we saw in the class that by  using the concept of the `observers`  we can perform extremely long-range forecast. To include the observers you need to use three different inputs: a vector of ones, and the two other known time series `y(t)` and `z(t)` (the observers). In this case the prediction is a response to the past behavior and also to the present observers' signals. This kind of predictions are called **inference**. For the inference, we need to know the values of the `observers` also for the future values, since they are used as inputs in the RC. This is why we have an inference instead of a pure forecasting.\n",
    "\n",
    "\n",
    "As we discussed in the class RC is very sensitive to the hyper-parameters. In all the questions you are asked to find the optimal set of hyper-parameters that gives the best predictions. For convenience, we are asking you to optimize just two of the hyper-parameters, the `spectrum radius`  and `sparsity term`.  The rest of the hyper-parameters are given.\n",
    "\n",
    "The goals of this homework are for you to:\n",
    "1. learn the mechanics of RC\n",
    "2. confirm that RC training is fast\n",
    "3. learn how to use RC for forecasting\n",
    "4. acknowledge that RC is sensitive to hyper-parameters (no free lunch)\n",
    "5. learn how to optimize hyper-parameters of RC\n",
    "6. evaluate RC forecasting\n",
    "7. learn observers-based RC for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview for the pyESN library for the RC implementation\n",
    "\n",
    "\n",
    "#### You call the RC as:\n",
    "esn = ESN(n_inputs =  #, <br>\n",
    " $\\quad$ $\\quad$     n_outputs = #, <br>\n",
    " $\\quad$ $\\quad$     n_reservoir = #,<br>\n",
    " $\\quad$ $\\quad$     sparsity= #,<br>\n",
    " $\\quad$ $\\quad$     random_state= #, <br>\n",
    " $\\quad$ $\\quad$     spectral_radius = #,<br>\n",
    " $\\quad$ $\\quad$     noise= #)\n",
    "<br> where # denotes the value that you choose.\n",
    "\n",
    "##### Brief explanation of the parameters:\n",
    "`n_inputs`: number of input dimensions <br>\n",
    "`n_outputs`: number of output dimensions <br>\n",
    "`n_reservoir`: number of reservoir neurons <br> \n",
    "`random_state`: seed for the random generator<br>\n",
    "`sparsity`: proportion of recurrent weights set to zero <br>\n",
    "`spectral_radius`: spectral radius of the recurrent weight matrix <br>\n",
    "`noise`: noise added to each hidden neuron (regularization) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout homework you should fix the following hyper-parameters.\n",
    "\n",
    "`n_outputs = 1`, <br>\n",
    "`n_reservoir = 1000`, <br>\n",
    "`noise = 0.0001`, <br>\n",
    "`random_state=42` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions**\n",
    "\n",
    "We are providing three helper functions.  You can use them if you want or you can make your own implementation, it's up to you. If you define any other helper functions, they should be placed *after* this next cell.  While heler functions are useful in keeping code organized organized, you are not required to use them for this homework. \n",
    "\n",
    "The given functions calculate the `MSE`, the `residuals`, and prepare the data. The `prepareData` splits the output  data into training and testing set, and create a training and testing array of ones. Note that the `prepareData` **does not** prepare the *observers*, you will need to do it manually in the question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS GO HERE\n",
    "\n",
    "def myMSE(prediction,target):\n",
    "    return np.sqrt(np.mean((prediction.flatten() - target.flatten() )**2))\n",
    "\n",
    "def residuals(prediction,target):\n",
    "    return (target.flatten() - prediction.flatten())\n",
    "\n",
    "    \n",
    "def prepareData(target, train_perc=0.9, plotshow=False):\n",
    "    datalen =  len(target)        \n",
    "    trainlen = int(train_perc*datalen)\n",
    "    testlen  = datalen-trainlen\n",
    "\n",
    "# Train/Test sets\n",
    "    trainTarget = target[:trainlen]\n",
    "    testTarget  = target[trainlen:trainlen+testlen]    \n",
    "    inputTrain = np.ones(trainlen)\n",
    "    inputTest  = np.ones(testlen)\n",
    "        \n",
    "    if plotshow:\n",
    "        plt.figure(figsize=(14,3))\n",
    "        plt.plot(range(0,trainlen), trainTarget,'g',label='Train')\n",
    "        plt.plot(range(trainlen,trainlen+testlen), testTarget,'-r',label='Test')\n",
    "        plt.legend(loc=(0.1,1.1),fontsize=18,ncol=2)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    return trainTarget, testTarget, inputTrain, inputTest\n",
    "\n",
    "def prepare_data(data, train_split):\n",
    "    split_idx = int(len(data) * train_split)\n",
    "    \n",
    "    return_dict = {}\n",
    "    \n",
    "    return_dict['train']= data[:split_idx]\n",
    "    return_dict['target'] = data[split_idx:]\n",
    "    return return_dict\n",
    "\n",
    "def plot_train_prediction(RC_, training_set):\n",
    "    output_unnorm = RC.LinOut.weight.T@vals[0].T + RC.LinOut.bias\n",
    "    hi = RC_._output_stds* (output_unnorm)+ RC._output_means\n",
    "    hi = hi.view(-1,).numpy()\n",
    "\n",
    "    plt.figure( figsize = (16,5))\n",
    "    plt.title( \"RC â€” training set prediction\")\n",
    "    plt.plot( output_unnorm.view(-1,), color = \"yellow\")\n",
    "    plt.plot( training_set, linewidth =4, color = \"cyan\", label = \"ground truth\")\n",
    "    plt.plot( training_set, linewidth =2, color = \"blue\", alpha = 0.9)\n",
    "    plt.plot( hi, '--', color = \"red\", label = \"train prediction\")\n",
    "    plt.xlabel(\"timestep\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    plt.legend();\n",
    "\n",
    "def plot_states(RC_):\n",
    "    plt.figure(figsize = (16,4))\n",
    "    for i in range(4):\n",
    "        plt.plot(RC_.state.T[-(i+1), :], alpha = 0.6);\n",
    "    plt.title(\"RC Example States\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlabel(\"timestep\")\n",
    "\n",
    "def plot_test_prediction(test_set, pred_):\n",
    "    #plt.plot(test_outputs.T[-1,:100])\n",
    "    plt.figure(figsize = (16,4))\n",
    "    plt.plot(test_set, label = \"ground truth\");\n",
    "    plt.plot(pred_, label = \"prediction\")\n",
    "    plt.title(\"unoptimized hyper-parameter test set prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you define any other helper functions, they should be put in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS GO HERE\n",
    "### your code here\n",
    "def sinsq(x):\n",
    "    return torch.square(torch.sin(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and plot your data (three time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "\n",
    "dataX = np.loadtxt('./data/x.dat')\n",
    "dataY = np.loadtxt('./data/y.dat')\n",
    "dataZ = np.loadtxt('./data/z.dat')\n",
    "\n",
    "def torchify(x):\n",
    "    return torch.tensor(x).type(torch.float32).view(-1,1)\n",
    "\n",
    "dataX, dataY, dataZ = [torchify(x) for x in (dataX, dataY, dataZ)]\n",
    "\n",
    "dataX.shape\n",
    "fig,ax = plt.subplots(3,1, figsize = (16,8))\n",
    "ax[0].plot(dataX, label = \"X\")\n",
    "ax[1].plot(dataY, label = \"Y\", color = \"Red\")\n",
    "ax[2].plot(dataZ, label = \"Z\", color = \"Green\")\n",
    "for i in range(3):\n",
    "    ax[i].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1:   Short-range forecast  [50pts total] </b></div> \n",
    "\n",
    "In this question you are asked to perform a short range prediction. In particular, you have to use the first `95%` of the sequential points of the time-series `x(t)` and predict the final `5%`, this is considered the validation or testing set; in this homework the validation and the testing sets are the same. \n",
    "\n",
    "First, try to manually find a set of the hyper-parameters `spectral_radius` and  `sparsity` that yields a prediction with relatively low validation/testing MSE (smaller than 0.25). Plot the training and the prediction along with the ground truth data. Also, show the residual between the ground truth and your prediction. \n",
    "\n",
    "\n",
    "Next, make a more systematic hyper-parameter optimization by using a grid search for the hyper-parameters `spectral_radius` and  `sparsity` . The goal is to find the optimal set that gives the lowest MSE on the prediction. Make a 2D color plot to show the MSE for the different values of `spectral_radius` and `sparity`.\n",
    "\n",
    "Finally, you have to make predictions with the optimal hyper-parameter set. Plot the training and the predictions along with the ground truth data. Again, show the residual between the ground truth and your prediction. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the target time-series and name it `target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X95 = prepare_data(dataX, 0.95)\n",
    "Y95 = prepare_data(dataY, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obvious tasks:\n",
    "    1) colored warning about how if not specified, x will default to ones and feedback will be enabled for training.\n",
    "    2) Tomorrow Get Zhizhuo redone predictions.\n",
    "    3) impliment bias properly (random uniform and normal bias)\n",
    "    4) fix error function\n",
    "    \n",
    "Dimensionality problem with my shit. Currently elastic net working, ridge regression failing.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the RC PyESNnoise=0.001, \n",
    "plot_single_RC = False\n",
    "if plot_single_RC:\n",
    "    RC = EchoStateNetwork(n_nodes=1000, connectivity=0.2,\n",
    "                          leaking_rate=1.0, spectral_radius=1.4, regularization=10**-1, \n",
    "                          feedback=True, epochs = 20, bias = 1, l2_prop = 1.0,\n",
    "                          random_state = 999, noise = 0.1, activation_f = nn.Tanh())\n",
    "\n",
    "    vals = RC.train(y = X95['train'], learning_rate = 5e-3)\n",
    "\n",
    "    err_, pred = RC.test(y=X95['train'])\n",
    "    pl.plot(pred)\n",
    "    pl.plot(X95['train'])\n",
    "    plot_train_prediction(RC, training_set = X95['train'])\n",
    "    plot_states(RC)\n",
    "    err, pred95 = RC.test( y = X95['target'], steps_ahead = None, scoring_method = \"nmse\") \n",
    "    plot_test_prediction(X95['target'], pred95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data: The target time-series should be split into training and testing sets. Plot the time-series using different colors to indicate the training and testing sets. You might want to use the given helper function `prepareData()` or you can do it by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "#%%time\n",
    "%matplotlib inline\n",
    "#### eps isn't what it used to be.\n",
    "\n",
    "bounds_dict = {\"spectral_radius\" : (1,2), \n",
    "               \"connectivity\" : (-4,0.5), \n",
    "               \"regularization\": (-5,3),\n",
    "               \"leaking_rate\" : (0, 1),\n",
    "               #\"feedback_scaling\" : (0,1),\n",
    "               \"l2_prop\" : 1.0,\n",
    "               \"bias\" : (-0.5, 0.5),\n",
    "               #\"noise\" : (-5,-1),#\"uniform\",#\n",
    "               \"PyESNnoise\":0.0,\n",
    "               \"n_nodes\" : 1000, \n",
    "               \"feedback\": 1}\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 7000, esn_feedback = True,\n",
    "                            steps_ahead = None, scoring_method = \"nmse\", interactive = True, random_seed = 123,\n",
    "                            initial_samples = 50, approximate_reservoir = False, length_min = 2**-9,\n",
    "                            batch_size = 8, backprop = False, esn_burn_in = 0, validate_fraction = 0.2,\n",
    "                            activation_function = nn.Tanh(), cv_samples = 2\n",
    "                            ) #activation_function = sinsq,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "results_round1 = esn_cv.optimize(y = X95['train'])\n",
    "\n",
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))\n",
    "results_round1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a quick prediction until find a testing MSE < 0.25: Try around `spectral_radius = [1.2, 2.6]` and   `sparsity = [0.16,  0.24]`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters optimization\n",
    "Make a search grid for the hyper-parameters `spectra-radius` and `sparsity`. Visualize the result by ploting the testing MSE in a 2D color plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device == torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "RC = EchoStateNetwork(**results_round1, backprop = False, epochs = 20)\n",
    "         #activation_f = sinsq, ) #bias = -10,\n",
    "\n",
    "RC.train( y = X95['train'].to(device), learning_rate = 5e-3)\n",
    "\n",
    "err, pred, _ = RC.test(y = X95['target'].to(device), \n",
    "                     scoring_method = \"mse\", steps_ahead = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "pred = pred.to('cpu')\n",
    "pl.plot(X95['target'], label = \"test set\")\n",
    "pl.plot(pred, label = \"pred\")\n",
    "print(\"MSE\", torch.mean((X95['target']- pred)**2))\n",
    "plt.title(\"RC after optimization\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Prediction (my new idea is to have multiple rounds. What we can do is to center a new search around a range of values for each hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.004)/np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "\"\"\"manual fine tuning:\n",
    "\n",
    "regularization value was 0.7 roughly. log10(0.7)\n",
    "So we can search between -1 and 2.\n",
    "\n",
    "Spectral radius was 1.2 so we can search from 0.7 to 1.7.\n",
    "connectivity was 0.04 so log10(0.04) = (-1.39). So we can search from -2.39 to -0.39\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "##############\n",
    "bounds_dict = results_round1.copy()\n",
    "bounds_dict[\"spectral_radius\"] =  (0.7, 1.7)\n",
    "bounds_dict[\"connectivity\"] = (-2.39, -0.39)\n",
    "#bounds_dict[\"l2_prop\"] = (0,1)\n",
    "bounds_dict[\"regularization\"] = (-1,2)\n",
    "\n",
    "bounds_dict = {**bounds_dict,\n",
    "               \"feedback\": 1,\n",
    "               \"n_nodes\" : 1000}\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 7000, esn_feedback = True,\n",
    "                            steps_ahead = None, scoring_method = \"nmse\", interactive = True, random_seed = 123,\n",
    "                            initial_samples = 16, approximate_reservoir = False, length_min = 2**-7,\n",
    "                            batch_size = 8, backprop = False, esn_burn_in = 0, validate_fraction = 0.2\n",
    "                            ) #activation_function = sinsq,\n",
    "results_round2 = esn_cv.optimize(y = X95['train'])\n",
    "\n",
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))\n",
    "results_round2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_round2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_sensitivity(optimized_parameters, per_param = 2):\n",
    "    \"\"\"\n",
    "    This function will check which hyper-parameters the RC is most sensitive to.\n",
    "    \n",
    "    \"\"\"\n",
    "    fixed_variables = ['feedback', 'n_nodes', 'PyESNnoise', 'l2_prop']\n",
    "    fixed_dict = {}\n",
    " \n",
    "    opt_params_mus = optimized_parameters.copy()\n",
    "    log_params = [\"regularization\", \"connectivity\"]\n",
    "    bounded_01_params = ['leaking_rate']\n",
    "    \n",
    "    \n",
    "    for param in list(opt_params_mus.keys()):\n",
    "        if param in fixed_variables:\n",
    "            fixed_dict[param] = opt_params_mus[param]\n",
    "            del opt_params_mus[param]\n",
    "            \n",
    "    #loop:\n",
    "    #parameter by the number of trained RC's\n",
    "    parameters_tested = list(opt_params_mus.keys())\n",
    "    \n",
    "    n = per_param\n",
    "    m = len(parameters_tested)\n",
    "    results = torch.zeros(n,m)\n",
    "    \n",
    "    displacements = torch.linspace(0,1,per_param) *2 - 1\n",
    "    \n",
    "    \n",
    "    for i, param in enumerate(parameters_tested):\n",
    "        #print(\"parameter_{}:__{}\".format(i, param))\n",
    "        optimized_parameters_copy = opt_params_mus.copy()\n",
    "        param_mu_spec = opt_params_mus[param]\n",
    "        if param in log_params:\n",
    "            log_displacements = np.linspace(np.log10(param_mu_spec) - 1, np.log10(param_mu_spec) + 1, per_param )\n",
    "            print(log_displacements)\n",
    "        for j in range(per_param):\n",
    "            if param in log_params:\n",
    "                print(\"LOG PARAM\")\n",
    "                optimized_parameters_copy[param] = 10**log_displacements[j]#np.log10( 10**(opt_params_mus[param]) + displacements[j])\n",
    "            else:\n",
    "                offset = displacements[j]\n",
    "                if param in bounded_01_params:\n",
    "                    offset /= 10\n",
    "                optimized_parameters_copy[param] = float(opt_params_mus[param] + offset)\n",
    "             \n",
    "            #print(optimized_parameters_copy)\n",
    "            RC = EchoStateNetwork(**optimized_parameters_copy, **fixed_dict, backprop = False, epochs = 20)\n",
    "            \n",
    "            try:\n",
    "                RC.train( y = X95['train'])\n",
    "                err, pred = RC.test(y = X95['target'], \n",
    "                                 scoring_method = \"nmse\", steps_ahead = None)\n",
    "                print(torch.mean((pred - X95['target'])**2))\n",
    "            \n",
    "                results[j,i] = float(err)\n",
    "            except:\n",
    "                results[j,i] = 1000\n",
    "            \n",
    "            \n",
    "    results = pd.DataFrame(results.numpy())\n",
    "    results.columns = list(parameters_tested)\n",
    "    return results\n",
    "    \n",
    "rez = parameter_sensitivity(results_round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez.std(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(esn_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vals)\n",
    "\n",
    "second_round_parameters = {}\n",
    "for key in results_round1.keys():\n",
    "    if key in results_round2.keys():\n",
    "        second_round_parameters[key] = results_round2[key]\n",
    "    else:\n",
    "        second_round_parameters[key] = results_round1[key]\n",
    "print(second_round_parameters)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "RC = EchoStateNetwork(**second_round_parameters, backprop = False, epochs = 20)\n",
    "\n",
    "RC.train( y = X95['train'].to(device))\n",
    "\n",
    "err, pred, _ = RC.test(y = X95['target'].to(device), \n",
    "                     scoring_method = \"nmse\", steps_ahead = None)\n",
    "pred = pred.cpu().numpy()\n",
    "pl.plot(X95['target'], label = \"test set\")\n",
    "pl.plot(pred, label = \"pred\")\n",
    "print(\"MSE\", torch.mean((X95['target']- pred)**2))\n",
    "plt.title(\"RC after optimization\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2:   Long-range forecast  [50pts total] </b></div> \n",
    "\n",
    "Here you are asked to make a  long-range prediction. Use the first `50%` of your data to train the RC network and then predict the final `50%`. This is a very long prediction and, consequently, it is extremely hard. \n",
    "\n",
    "First, show that by using the RC as before, it is imposible to make a good prediction (with MSE smaller than 0.4). Make a grid search to check the lowest possible testing MSE.\n",
    "\n",
    "Next, use the concept of the `observers` and perform an inference prediction. Follow the steps of the Question 1. Make a grid search in the hyperparameters `spectral-radius` and `sparsity`. Visualize the MSE in prediction by using a 2D plot.  Then perform a inference prediction with the optimal set. Plot the training and prediction along with the ground truth. Again, show the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data: The target time-series should be splitted  into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X55 = prepare_data(dataX, 0.55)\n",
    "Y55 = prepare_data(dataY, 0.55)\n",
    "Z55 = prepare_data(dataY, 0.55)\n",
    "\n",
    "obs_train = torch.cat([Y55['train'], Z55[\"train\"]], axis = 1)\n",
    "obs_target = torch.cat([Y55['target'], Z55[\"target\"]], axis = 1)\n",
    "obs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC.LinIn.weight.shape\n",
    "obs_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "\n",
    "### your code here\n",
    "#n_nodes=1000, connectivity=0.01, input_scaling=0.5, feedback_scaling=0.5, leaking_rate=0.3, spectral_radius=1.25, regularization=1e-8, feedback=True)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "RC = EchoStateNetwork(n_nodes=1000, connectivity=0.16, input_scaling=1, feedback_scaling=1, \n",
    "         leaking_rate=0.1, spectral_radius=1.3, regularization=1, feedback=True, backprop = False, epochs = 20,\n",
    "         activation_f = sinsq, bias = 1)\n",
    "\n",
    "RC.train(y = X55[\"train\"], X = obs_train)\n",
    "err, pred, _ = RC.test(y = X55['target'], x = obs_target, scoring_method = \"nmse\", steps_ahead = None)\n",
    "\n",
    "pl.plot(X55['target'])\n",
    "pl.plot(pred) \n",
    "print(\"MSE\", torch.sum(X55['target'] - pred).item()**2/len(pred))\n",
    "plt.title(\"RC before optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "%matplotlib inline\n",
    "#### eps isn't what it used to be.\n",
    "\n",
    "bounds_dict = {\"spectral_radius\" : (0,2), \n",
    "               \"connectivity\" : (-5,0), \n",
    "               \"regularization\": (-5,0.5),\n",
    "               \"leaking_rate\" : (0, 1),\n",
    "               #\"feedback_scaling\" : (0,1),\n",
    "               #\"input_scaling\" : (0,1),\n",
    "               \"bias\" : (-3,3),#\"uniform\",#\n",
    "               \"n_nodes\" : 1000, \n",
    "               \"feedback\": 1}\n",
    "\n",
    "\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 7000, esn_feedback = True,\n",
    "                            steps_ahead = None, scoring_method = \"nmse\", interactive = True, random_seed = 123,\n",
    "                            initial_samples = 100, approximate_reservoir = False, length_min = 2**-7,\n",
    "                            batch_size = 8, cv_samples = 2, backprop = False, esn_burn_in = 0, validate_fraction = 0.2)\n",
    "                           #activation_function = sinsq)\n",
    "vals = esn_cv.optimize(X55['train'])\n",
    "\n",
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))\n",
    "vals;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "RC = EchoStateNetwork(**vals, backprop = False, epochs = 20,\n",
    "         activation_f = sinsq) #bias = -10,\n",
    "\n",
    "RC.train( X55['train'], learning_rate = 5e-3)\n",
    "\n",
    "err, pred, _ = RC.test(X55['target'], scoring_method = \"nmse\", steps_ahead = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "pl.plot(X55['target'], alpha = 0.5)\n",
    "pl.plot(pred, alpha = 0.5)\n",
    "print(\"MSE\", torch.sum(X55['target'] - pred).item()**2/len(pred))\n",
    "plt.title(\"RC before optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference: Observers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your `observers`. The given `prepareData()` function does not prepare the observers, so you need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "### your code here\n",
    "%matplotlib inline\n",
    "#### eps isn't what it used to be.\n",
    "\n",
    "bounds_dict = {\"spectral_radius\" : (0,2), \n",
    "               \"connectivity\" : (-4,0), \n",
    "               \"regularization\": (-5,3),\n",
    "               \"leaking_rate\" : (0, 1),\n",
    "               #\"feedback_scaling\" : (0,1),\n",
    "               \"l2_prop\" : 1,\n",
    "               \"bias\" : (-3,3),#\"uniform\",#\n",
    "               \"PyESNnoise\":0.0,\n",
    "               \"n_nodes\" : 1000}\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 7000, esn_feedback = False,\n",
    "                            steps_ahead = None, scoring_method = \"nmse\", interactive = True, random_seed = 123,\n",
    "                            initial_samples = 50, approximate_reservoir = False, length_min = 2**-9,\n",
    "                            batch_size = 6, cv_samples = 3, backprop = False, esn_burn_in = 0, validate_fraction = 0.4)\n",
    "                           #activation_function = sinsq)\n",
    "obs_round1_results = esn_cv.optimize(x = obs_train, y = X55['train'])\n",
    "\n",
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))\n",
    "obs_round1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a quick prediction to see the improvement (without optimizing the hyper-parameters yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_round1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_ = obs_round1_results.copy()\n",
    "#vals_[\"connectivity\"] = 10**(vals_[\"connectivity\"])\n",
    "#vals_[\"regularization\"] = 10**(vals_[\"regularization\"])\n",
    "log_connectivity = np.log(vals_[\"connectivity\"])/np.log(10)\n",
    "log_spect = np.log(vals_[\"spectral_radius\"])/np.log(10)\n",
    "log_reg = np.log(vals_[\"regularization\"])/np.log(10)\n",
    "vals_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "torch.manual_seed(123)\n",
    "RC = EchoStateNetwork(**vals_, backprop = False, epochs = 20\n",
    "         ) #bias = -10,activation_f = sinsq\n",
    "\n",
    "RC.train( X = obs_train, y = X55['train'], learning_rate = 5e-3)\n",
    "\n",
    "err, pred, _ = RC.test(x = obs_target , y = X55['target'], scoring_method = \"mse\", steps_ahead = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "### your code here\n",
    "pl.plot(X55['target'], alpha = 0.5, label = \"test\")\n",
    "pl.plot(pred, alpha = 0.5, label = \"prediction\")\n",
    "print(\"MSE\", torch.sum(X55['target'] - (pred)).item()**2/len(pred))\n",
    "plt.title(\"RC before optimization\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "# bias distributions!\n",
    "# have the user input error function, have some default (normalized mean square error)\n",
    "\n",
    "\n",
    "### your code here\n",
    "%matplotlib inline\n",
    "#### eps isn't what it used to be.\n",
    "\n",
    "variables_for_second_round = [\"connectivity\", \"spectral_radius\", \"regularization\"]\n",
    "\n",
    "vals_ = obs_round1_results.copy()\n",
    "for key in variables_for_second_round:\n",
    "    del vals_[key]\n",
    "del vals_[\"bias\"]\n",
    "bounds_dict = {\"spectral_radius\" : (log_spect - 0.1, log_spect + 0.1), \n",
    "               \"connectivity\" : (log_connectivity - 1,log_connectivity + 1), \n",
    "               \"regularization\" : (log_reg -1, log_reg +1),\n",
    "               \"bias\" : (-3, 3),\n",
    "               **vals_}\n",
    "esn_cv = EchoStateNetworkCV(bounds = bounds_dict, subsequence_length = 7000, esn_feedback = False,\n",
    "                            steps_ahead = None, scoring_method = \"nmse\", interactive = True, random_seed = 123,\n",
    "                            initial_samples = 5, approximate_reservoir = False, length_min = 2**-7,\n",
    "                            batch_size = 4, cv_samples = 3, backprop = False, esn_burn_in = 0, validate_fraction = 0.4)\n",
    "                           #activation_function = sinsq)\n",
    "vals = esn_cv.optimize(x = obs_train, y = X55['train'])\n",
    "\n",
    "pl.plot(esn_cv.iteration_durations)\n",
    "print(np.mean(esn_cv.iteration_durations))\n",
    "vals;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the optimal prediction (inference) here. As in Q1, show the fitting and prediction data along with the ground truth. And one, last time, plot the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "### your code here\n",
    "torch.manual_seed(123)\n",
    "RC = EchoStateNetwork(**vals, backprop = False, epochs = 20, feedback = False\n",
    "         ) #bias = -10,activation_f = sinsq\n",
    "\n",
    "RC.train( X = obs_train, y = X55['train'], learning_rate = 5e-3)\n",
    "\n",
    "err, pred, _ = RC.test(x = obs_target, y = X55['target'], scoring_method = \"mse\", steps_ahead = None)\n",
    "\n",
    "\n",
    "### your code here\n",
    "### your code here\n",
    "pl.plot(X55['target'], alpha = 0.5, label = \"test\")\n",
    "pl.plot(pred, alpha = 0.5, label = \"prediction\")\n",
    "print(\"MSE\", torch.sum(X55['target'] - (pred)).item()**2/len(pred))\n",
    "plt.title(\"RC before optimization\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "\n",
    "- https://github.com/cknd/pyESN\n",
    "- https://github.com/FilippoMB/Reservoir-Computing-framework-for-multivariate-time-series-classification\n",
    "- https://towardsdatascience.com/gentle-introduction-to-echo-state-networks-af99e5373c68\n",
    "\n",
    "\n",
    "1. H. Jaeger and H. Haas. Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication, Science **304**  (2004)\n",
    "2. Z. Lu, J. Pathak, B. Hunt, M. Girvan, R. Brockett, and E. Ott. Reservoir observers: Model-free inference of unmeasured variables in chaotic systems, Chaos **27** (2017)\n",
    "3. G. N. Neofotistos, M. Mattheakis, G. Barmparis, J. Hitzanidi, G. P. Tsironis, and E. Kaxiras. Machine learning with observers predicts complex spatiotemporal behavior. Front. Phys. - Quantum Computing **7** (2019)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# x = torch.ones(*train_purePred.shape)/len(train_purePred),\n",
    "#x = torch.ones(*test_purePred.shape)/len(train_purePred), "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
