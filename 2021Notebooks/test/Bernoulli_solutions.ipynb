{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exact-worship",
   "metadata": {},
   "source": [
    "# RcTorch 2021 NuerIPS submission Notebook 2\n",
    "## Backpropagation solutions: 1st order  nonlinear Bernoulli Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from rctorchprivate import *\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "#this method will ensure that the notebook can use multiprocessing (train multiple \n",
    "#RC's in parallel) on jupyterhub or any other linux based system.\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\")\n",
    "except:\n",
    "    pass\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "%matplotlib inline\n",
    "start_time = time.time()\n",
    "import RcTorchPrivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rctorch==0.7163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineW = 3\n",
    "lineBoxW=2\n",
    "\n",
    "font = {'size'   : 24}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-thursday",
   "metadata": {},
   "source": [
    "### This notebook contains the solutions to the Bernoulli type nonlinear equations of the form \n",
    "\\begin{align}\n",
    "    \\label{eq:bernoulli}\n",
    "    a_1(t) \\dot y + a_0(t) y + q(t) y^2 = f(t).\n",
    "\\end{align}\n",
    "\n",
    "While we cannot solve this equation exactly, we do have a linearized approximation that we can use to speed up the approximation method. We will refer to this method as a hybrid linearization-backprop method.\n",
    "\n",
    "With RC we only optimize the last layer (the output layer). In the case of  a linear differential equation, we can solve this exactly. In the nonlinear case we can drop the nonlinear term and thereby get a linear approximation, but this won't get us all the way to the bottom of the loss function. Thus, we need to use gradient descent which is fast because we only optimize one layer. The hybrid method initializes the weights to the linearization, which is shown below to find better solutions that the backprop only method (random weight initialization).\n",
    "A comparison of hybrid vs backprop is shown below (credit to Shivam Raval). The idea is that the hybrid method is indeed faster for finding solutions of the same quality. That is, we observe faster descent and lower minima when initializing to the linearized approximation. Somehow we jump to a convex part of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driven_force(X, A = 1):\n",
    "    \"\"\" a force function, specifically f(t) = sin(t)\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: torch.tensor\n",
    "        the input time tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the force, a torch.tensor of equal dimension to the input time tensor.\n",
    "    \"\"\"\n",
    "    return A * torch.sin(X)\n",
    "\n",
    "def no_force(X, A = 0):\n",
    "    \"\"\" a force function (returns 0)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: torch.tensor\n",
    "        the input time tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the force, in this case 0.\n",
    "    \"\"\"\n",
    "    return torch.zeros_like(X)\n",
    "\n",
    "q = 0.5\n",
    "def custom_loss(X , y, ydot, out_weights, force_t = no_force, \n",
    "                reg = True, ode_coefs = None, q = q, \n",
    "                init_conds = None, enet_strength = None, enet_alpha = None,\n",
    "                mean = True):\n",
    "    \"\"\" The loss function of the ODE (in this case the bernoulli equation loss)\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: torch.tensor\n",
    "        The input (in the case of ODEs this is time t)\n",
    "    y: torch.tensor\n",
    "        The response variable\n",
    "    ydot: torch.tensor\n",
    "        The time derivative of the response variable\n",
    "    enet_strength: float\n",
    "        the magnitude of the elastic net regularization parameter. In this case there is no e-net regularization\n",
    "    enet_alpha: float\n",
    "        the proportion of the loss that is L2 regularization (ridge). 1-alpha is the L1 proportion (lasso).\n",
    "    ode_coefs: list\n",
    "        this list represents the ODE coefficients. They can be numbers or t**n where n is some real number.\n",
    "    force: function\n",
    "        this function needs to take the input time tensor and return a new tensor f(t)\n",
    "    reg: bool\n",
    "        if applicable (not in the case below) this will toggle the elastic net regularization on and off\n",
    "    reparam: function\n",
    "        a reparameterization function which needs to take in the time tensor and return g and gdot, which \n",
    "        is the reparameterized time function that satisfies the initial conditions.\n",
    "    init_conds: list\n",
    "        the initial conditions of the ODE.\n",
    "    mean: bool\n",
    "        if true return the cost (0 dimensional float tensor) else return the residuals (1 dimensional tensor)\n",
    "    q: float\n",
    "        a bernoulli specific hyper-parameter\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    the residuals or the cost depending on the mean argument (see above)\n",
    "    \"\"\"\n",
    "    #with paramization\n",
    "    L =  ydot + ode_coefs[0]* y - force_t + q*y**2\n",
    "    \n",
    "    L = torch.square(L)\n",
    "    if mean:\n",
    "         L = torch.mean(L)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(RC, results, integrator_model, y0s = None,  ax = None):\n",
    "    \"\"\"plots a RC prediction and integrator model prediction for comparison\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    integrator model: function\n",
    "        the model to be passed to odeint which is a gold standard integrator numerical method\n",
    "        for solving ODE's written in Fortran. You may find the documentation here:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    X = process(RC.X)\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "    for i, y in enumerate(results[\"ys\"]):\n",
    "        y = process(y)\n",
    "        if not i:\n",
    "            labels = [\"RC\", \"Integrator Solution\"]\n",
    "        else:\n",
    "            labels = [None, None]\n",
    "        ax.plot(X, y, label = labels[0], linewidth = lineW + 2, alpha = 0.9)\n",
    "\n",
    "        #calculate the integrator prediction:\n",
    "        int_sol = odeint(integrator_model, y0s[i], np.array(X.cpu().squeeze()))\n",
    "        int_sol = torch.tensor(int_sol)\n",
    "        \n",
    "        #plot the integrator prediction\n",
    "        ax.plot(X, int_sol, '--', color = \"black\", alpha = 0.95, label = labels[1],  linewidth = lineW - 1)\n",
    "    \n",
    "    plt.ylabel(r'$y(t)$');\n",
    "    ax.legend();\n",
    "    ax.tick_params(labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_results(RC, results, integrator_model, y0s = None, ax = None):\n",
    "    \"\"\"plots a RC prediction and integrator model prediction for comparison\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    integrator model: function\n",
    "        the model to be passed to odeint which is a gold standard integrator numerical method\n",
    "        for solving ODE's written in Fortran. You may find the documentation here:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    X = RC.X.detach()\n",
    "    \n",
    "    #int_sols = []\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "    for i, y in enumerate(results[\"ys\"]):\n",
    "        if not i:\n",
    "            labels = [\"RC\", \"integrator solution\"]\n",
    "        else:\n",
    "            labels = [None, None]\n",
    "        y = y.detach()\n",
    "        ax.plot(X, y, color = \"blue\", label = labels[0])\n",
    "\n",
    "        #calculate the integrator prediction:\n",
    "        int_sol = odeint(integrator_model, y0s[i], np.array(X.cpu().squeeze()))\n",
    "        int_sol = torch.tensor(int_sol)\n",
    "        #int_sols.append(int_sol)\n",
    "        \n",
    "        #plot the integrator prediction\n",
    "        ax.plot(X, int_sol, '--', color = \"red\", alpha = 0.9, label = labels[1])\n",
    "        \n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend();\n",
    "    #return int_sols\n",
    "        \n",
    "def process(tensor_):\n",
    "    \"\"\" takes a tensor and prepares it for plotting.\n",
    "    \"\"\"\n",
    "    return tensor_.cpu().detach()\n",
    "        \n",
    "def plot_rmsr(RC, results, force, ax = None):\n",
    "    \"\"\"plots the residuals of a RC prediction directly from the loss function\n",
    "    Parameters\n",
    "    ----------\n",
    "    RC: RcTorchPrivate.esn\n",
    "        the RcTorch echostate network to evaluate. This model should already have been fit.\n",
    "    results: dictionary\n",
    "        the dictionary of results returned by the RC after fitting\n",
    "    force: function\n",
    "        the force function describing the force term in the population equation\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        If provided, the function will plot on this subplot axes\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10, 4))\n",
    "    X = process(RC.X)\n",
    "    ys, ydots = results[\"ys\"], results[\"ydots\"]\n",
    "    \n",
    "    residuals = []\n",
    "    force_t = force(X)\n",
    "    for i, y in enumerate(ys):\n",
    "        ydot = ydots[i]\n",
    "        y = process(y)\n",
    "        ydot = process(ydot)\n",
    "        \n",
    "        ode_coefs = covert_ode_coefs(t = X, ode_coefs = RC.ode_coefs)\n",
    "        \n",
    "        resids = custom_loss(X, y, ydot, None, \n",
    "                             force_t = force_t, \n",
    "                             ode_coefs = RC.ode_coefs,\n",
    "                             mean = False)\n",
    "        if not i:\n",
    "            resids_tensor = resids\n",
    "        else:\n",
    "            resids_tensor = torch.cat((resids_tensor, resids), axis = 1)\n",
    "        resids_specific_rmsr = torch.sqrt(resids/1) \n",
    "            \n",
    "        ax.plot(X, resids_specific_rmsr, alpha = 0.7, linewidth = lineW -1)\n",
    "        residuals.append(resids)\n",
    "    \n",
    "    mean_resid = torch.mean(resids_tensor, axis =1)\n",
    "    rmsr = torch.sqrt(mean_resid)\n",
    "#     ax.plot(X, rmsr, \n",
    "#                color = \"blue\", \n",
    "#                alpha = 0.9, \n",
    "#                label = r'{RMSR}')\n",
    "\n",
    "    #ax.legend(prop={\"size\":16});\n",
    "    \n",
    "    ax.set_xlabel(r'$t$')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(r'{RMSR}')\n",
    "\n",
    "def covert_ode_coefs(t, ode_coefs):\n",
    "    \"\"\" converts coefficients from the string 't**n' or 't^n' where n is any float\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.tensor\n",
    "        input time tensor\n",
    "    ode_coefs: list\n",
    "        list of associated floats. List items can either be (int/floats) or ('t**n'/'t^n')\n",
    "    \"\"\"\n",
    "    type_t = type(t)\n",
    "    for i, coef in enumerate(ode_coefs):\n",
    "        if type(coef) == str:\n",
    "            if coef[0] == \"t\" and (coef[1] == \"*\" or (coef[1] == \"*\" and coef[2] == \"*\")):\n",
    "                pow_ = float(re.sub(\"[^0-9.-]+\", \"\", coef))\n",
    "                ode_coefs[i]  = t ** pow_\n",
    "                print(\"alterning ode_coefs\")\n",
    "        elif type(coef) in [float, int, type_t]:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, \"ode_coefs must be a list floats or strings of the form 't^pow', where pow is a real number.\"\n",
    "    return ode_coefs\n",
    "    \n",
    "\n",
    "#define a reparameterization function, empirically we find that g= 1-e^(-t) works well)\n",
    "def reparam(t, order = 1):\n",
    "    \"\"\" A reparameterization function, specifically g= 1-e^(-t)\n",
    "    \n",
    "    Arguments:\n",
    "    t: torch.tensor\n",
    "        the input time tensor\n",
    "    order:\n",
    "        the ODE order\n",
    "    \n",
    "    Returns:\n",
    "    g: torch.tensor\n",
    "        the reparameterization of t which satisfies the initial conditions\n",
    "    g_dot: torch.tensor\n",
    "        the time derivative of g\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_t = torch.exp(-t)\n",
    "    derivatives_of_g = []\n",
    "    \n",
    "    g = 1 - exp_t\n",
    "    g_dot = 1 - g\n",
    "    return g, g_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_last_layer(esn, \n",
    "                        SAVE_AFTER_EPOCHS = 1,\n",
    "                        epochs = None,#30000,\n",
    "                        custom_loss = custom_loss,\n",
    "                        force = no_force,\n",
    "                        learning_rate = 0.01,\n",
    "                        plott = False,\n",
    "                        spikethreshold = 0.25,\n",
    "                        plot_every_n_epochs = 1500):\n",
    "    \"\"\" The backprop function for the output layer of the RC\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    esn: EchoStateNetwork object\n",
    "        a pretrained echostate network\n",
    "    SAVE_AFTER_EPOCHS: int\n",
    "        after this amount of epochs the function will start saving the best score and predictions\n",
    "    epochs: int\n",
    "        epochs of backpropagation to complete\n",
    "    custom_loss: function\n",
    "        the loss function\n",
    "    force: function\n",
    "        the force function\n",
    "    learning_rate: float\n",
    "        the starting learning rate (0.1 or 0.01 recommended)\n",
    "    plott: bool\n",
    "        if true, the backpropogation will plot interactively\n",
    "    spikethreshold: float\n",
    "        if the loss increases by more than this amount, the learning rate will slow down.\n",
    "    plot_every_n_epochs: int\n",
    "        how often to plot the loss \n",
    "    Returns\n",
    "    -------\n",
    "    a dict of the best_score, best weight (and bias), best prediction (and associated derivative), loss history, \n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        #define new_x\n",
    "        new_X = esn.extended_states.detach()\n",
    "\n",
    "        #force detach states_dot\n",
    "        esn.states_dot = esn.states_dot.detach().requires_grad_(False)\n",
    "\n",
    "        #define criterion\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        try:\n",
    "            assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "        except:\n",
    "            esn.LinOut.weight.requires_grad_(True)\n",
    "            esn.LinOut.bias.requires_grad_(True)\n",
    "        #define previous_loss (could be used to do a convergence stop)\n",
    "        previous_loss = 0\n",
    "\n",
    "        #define best score so that we can save the best weights\n",
    "        best_score = None\n",
    "\n",
    "        #define the optimizer\n",
    "        optimizer = optim.Adam(esn.parameters(), lr = learning_rate)\n",
    "\n",
    "        #define the loss history\n",
    "        loss_history = []\n",
    "\n",
    "        if plott:\n",
    "            #use pl for live plotting\n",
    "            fig, ax = pl.subplots(1,3, figsize = (16,4))\n",
    "\n",
    "        t = esn.X#.view(*N.shape).detach()\n",
    "        g, g_dot = esn.G\n",
    "        y0  = esn.init_conds[0]\n",
    "\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "        lrs = []\n",
    "\n",
    "        floss_last = 0\n",
    "        force_t = force(t)\n",
    "        #begin optimization loop\n",
    "        for e in range(epochs):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            N = esn.forward( esn.extended_states )\n",
    "            N_dot = esn.calc_Ndot(esn.states_dot)\n",
    "\n",
    "            y = y0  + g *N \n",
    "\n",
    "            ydot = g_dot * N + g * N_dot\n",
    "\n",
    "            assert N.shape == N_dot.shape, f'{N.shape} != {N_dot.shape}'\n",
    "\n",
    "            assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad\n",
    "\n",
    "            total_ws = esn.LinOut.weight.shape[0] + 1\n",
    "            weight_size_sq = torch.mean(torch.square(esn.LinOut.weight))\n",
    "\n",
    "            loss = custom_loss(esn.X, y, ydot, esn.LinOut.weight, force_t = force_t, reg = False, ode_coefs = esn.ode_coefs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            floss = float(loss)\n",
    "            \n",
    "\n",
    "            if not e and not best_score:\n",
    "                best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "\n",
    "            if e > SAVE_AFTER_EPOCHS:\n",
    "                if not best_score:\n",
    "                    best_score = min(loss_history)\n",
    "                if floss <= min(loss_history):\n",
    "                    best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()\n",
    "                    best_score = float(loss)\n",
    "                    best_pred = y.clone()\n",
    "                    best_ydot = ydot.clone()\n",
    "            \n",
    "            if e > 1:\n",
    "                #if the function spikes (starts to move away from the convex area)\n",
    "                #lower the lr.\n",
    "                if float(np.log(floss_last) - np.log(floss)) > spikethreshold:\n",
    "                    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "                    scheduler.step()\n",
    "                    #for param_group in optimizer.param_groups:\n",
    "                    #    print('lr', param_group['lr'])\n",
    "            floss_last = floss\n",
    "            loss_history.append(floss)\n",
    "            if plott:\n",
    "\n",
    "                if e % plot_every_n_epochs == 0:\n",
    "                    ax[0].clear()\n",
    "                    logloss_str = 'Log(L) ' + '%.2E' % Decimal((loss).item())\n",
    "                    delta_loss  = ' delta Log(L) ' + '%.2E' % Decimal((loss-previous_loss).item())\n",
    "\n",
    "                    print(logloss_str + \", \" + delta_loss)\n",
    "                    ax[0].plot(y.detach().cpu(), label = \"exact\")\n",
    "                    ax[0].set_title(f\"Epoch {e}\" + \", \" + logloss_str)\n",
    "                    ax[0].set_xlabel(\"epochs\")\n",
    "\n",
    "                    ax[1].set_title(delta_loss)\n",
    "                    ax[1].plot(y_dot.detach().cpu())\n",
    "\n",
    "                    ax[2].clear()\n",
    "                    weight_size = str(weight_size_sq.detach().item())\n",
    "                    ax[2].set_title(\"loss history \\n and \"+ weight_size)\n",
    "\n",
    "                    ax[2].loglog(loss_history)\n",
    "                    \n",
    "                    ax[0].legend()\n",
    "                    previous_loss = loss.item()\n",
    "\n",
    "                    #clear the plot outputt and then re-plot\n",
    "                    display.clear_output(wait=True) \n",
    "                    display.display(pl.gcf())\n",
    "        backprop_args = {\"loss_history\" : loss_history, \n",
    "                             \"lr\" : learning_rate,\n",
    "                             \"epochs\" : epochs\n",
    "                            }\n",
    "        return {\"weights\": best_weight, \n",
    "                \"bias\" : best_bias, \n",
    "                \"loss\" : backprop_args,\n",
    "                \"ydot\" : best_ydot, \n",
    "                \"y\" : best_pred,\n",
    "                \"best_score\" : best_score}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "BURN_IN = 1000\n",
    "x0,xf, nsteps = 0, 5, 1000\n",
    "xtrain = torch.linspace(x0, xf, steps = nsteps, requires_grad=False)\n",
    "\n",
    "#the length of xtrain won't matter above. Only dt , x0, and xf matter for ODEs.\n",
    "#the reason for this is that the input time vector is reconstructed internally in rctorch\n",
    "#in order to satisfy the specified dt.\n",
    "xtrain = torch.linspace(x0, xf, steps = nsteps, requires_grad=False).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hybrid_hps_q05 = {'dt': 0.007943282347242814,\n",
    "#  'n_nodes': 500,\n",
    "#  'connectivity': 0.0003179179463749722,\n",
    "#  'spectral_radius': 7.975825786590576,\n",
    "#  'regularization': 0.3332787303378571,\n",
    "#  'leaking_rate': 0.07119506597518921,\n",
    "#  'bias': -0.9424528479576111}\n",
    "\n",
    "\n",
    "hybrid_hps_q05 = opt_hps1 = {'bias': 0.9490906000137329,\n",
    " 'connectivity': 0.00024854583199299384,\n",
    " 'dt': 0.01,\n",
    " 'leaking_rate': 0.009424317628145218,\n",
    " 'n_nodes': 500,\n",
    " 'regularization': 0.02499297583727001,\n",
    " 'spectral_radius': 3.27508282661438}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "dRay=0.75\n",
    "y0s = np.arange(-1., 1 + dRay, dRay)\n",
    "\n",
    "train_args = {\"burn_in\" : int(BURN_IN), \n",
    "              \"ODE_order\" : 1,\n",
    "              \"force\" : no_force,\n",
    "              \"reparam_f\" : reparam,\n",
    "              \"init_conditions\" : [y0s],\n",
    "              \"ode_coefs\" : [1, 1],\n",
    "              \"X\" : xtrain.view(-1,1),\n",
    "              \"q\" : q,\n",
    "              \"nl\" : True,\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-tuesday",
   "metadata": {},
   "source": [
    "### Linear approximation of Bernoulli equation (no backprop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "RC = EchoStateNetwork(**hybrid_hps_q05,\n",
    "                      random_state = 209, \n",
    "                      dtype = torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "results = RC.fit(backprop_f = None,#optimize_last_layer,\n",
    "                 train_score = True, \n",
    "                 ODE_criterion = custom_loss,\n",
    "                 SOLVE = True,\n",
    "                 **train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bernoulli_model(y,t, q = q):\n",
    "    k = 1\n",
    "    dydt = -k * y -q*y**2\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results:\n",
    "fig = plt.figure(figsize = (9,7)); gs1 = gridspec.GridSpec(3, 3);\n",
    "ax = plt.subplot(gs1[:-1, :])\n",
    "\n",
    "plot_predictions(RC, results, Bernoulli_model, y0s, ax = ax)\n",
    "\n",
    "ax = plt.subplot(gs1[-1, :])\n",
    "plot_data = plot_rmsr(RC, \n",
    "                      results, \n",
    "                      force = no_force, \n",
    "                      ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-vietnamese",
   "metadata": {},
   "source": [
    "### Backprop Only Method (output weights randomly initialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "RC = EchoStateNetwork(**hybrid_hps_q05,\n",
    "                      random_state = 209, \n",
    "                      dtype = torch.float32)\n",
    "\n",
    "backprop_results = RC.fit( backprop_f = optimize_last_layer,\n",
    "                           train_score = True, \n",
    "                           ODE_criterion = custom_loss,\n",
    "                           SOLVE = False,\n",
    "                           **train_args,\n",
    "                           epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(losses):\n",
    "    for loss_hist in losses:\n",
    "        plt.loglog(loss_hist, linewidth = lineW)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"RMSR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results:\n",
    "fig = plt.figure(figsize = (14,8)); gs1 = gridspec.GridSpec(3, 3);\n",
    "gs1 = gridspec.GridSpec(3, 6)\n",
    "horiz_boundary = -3\n",
    "vert_boundary = -1\n",
    "g1, g2, g3 = gs1[:vert_boundary, :horiz_boundary], gs1[vert_boundary, :horiz_boundary], gs1[:, horiz_boundary:]\n",
    "ax=plt.subplot(g1)\n",
    "plot_predictions(RC, backprop_results, Bernoulli_model, y0s, ax = ax)\n",
    "\n",
    "ax=plt.subplot(g2)\n",
    "plot_data = plot_rmsr(RC, \n",
    "                      backprop_results, \n",
    "                      force = no_force, \n",
    "                      ax = ax)\n",
    "plt.subplot(g3)\n",
    "loss_plot(backprop_results[\"losses\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-thread",
   "metadata": {},
   "source": [
    "### Hybrid Method: Linearization and backpropagation combined.\n",
    "\n",
    "Here we initialize the weights for gradient descent to the linearized approximation, which improves the quality of the solutions we get from backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hybrid_results = RC.fit(backprop_f = optimize_last_layer,\n",
    "                        train_score = True, \n",
    "                        ODE_criterion = custom_loss,\n",
    "                        SOLVE = True,\n",
    "                        **train_args,\n",
    "                        epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results:\n",
    "fig = plt.figure(figsize = (14,8)); gs1 = gridspec.GridSpec(3, 3);\n",
    "gs1 = gridspec.GridSpec(3, 6)\n",
    "horiz_boundary = -3\n",
    "vert_boundary = -1\n",
    "\n",
    "g1, g2, g3 = gs1[:vert_boundary, :horiz_boundary], gs1[vert_boundary, :horiz_boundary], gs1[:, horiz_boundary:]\n",
    "ax=plt.subplot(g1)\n",
    "\n",
    "plot_predictions(RC, hybrid_results, Bernoulli_model, y0s, ax = ax)\n",
    "\n",
    "ax=plt.subplot(g2)\n",
    "plot_data = plot_rmsr(RC, \n",
    "                      hybrid_results, \n",
    "                      force = no_force, \n",
    "                      ax = ax)\n",
    "\n",
    "plt.subplot(g3)\n",
    "loss_plot(hybrid_results[\"losses\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-institute",
   "metadata": {},
   "source": [
    "#### Below backprop only means that we didn't use the linearization approximation for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_color = \"dodgerblue\"\n",
    "backprop_color = \"orangered\"\n",
    "individual_traj_alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-designation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 6))\n",
    "plt.title(\"Loss history comparison\")\n",
    "\n",
    "clr = ['blue', 'green', 'orange','red']\n",
    "\n",
    "for i, hybrid_loss_i in enumerate(hybrid_results[\"losses\"]):\n",
    "    hybrid_loss_i = torch.tensor(hybrid_loss_i)\n",
    "    plt.loglog(hybrid_loss_i, '--', color = clr[i],\n",
    "               linewidth = lineW, alpha = individual_traj_alpha )\n",
    "for i, backprop_loss_i in enumerate(backprop_results[\"losses\"]):\n",
    "    plt.loglog(backprop_loss_i, color = clr[i],\n",
    "               linewidth = lineW, alpha = individual_traj_alpha )\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hs = [min(loss) for loss in hybrid_results[\"losses\"]]\n",
    "final_bs = [min(loss) for loss in backprop_results[\"losses\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between backprop_loss and hybrid_loss\")\n",
    "[final_bs[i] - final_hs[i] for i in range(len(final_bs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f'Total notebook runtime: {end_time - start_time:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
