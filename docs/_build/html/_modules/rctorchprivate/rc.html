
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>rctorchprivate.rc &#8212; RcTorch 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for rctorchprivate.rc</h1><div class="highlight"><pre>
<span></span><span class="c1">#Imports</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1">#botorch</span>
<span class="kn">from</span> <span class="nn">botorch.acquisition</span> <span class="kn">import</span> <span class="n">qExpectedImprovement</span>
<span class="kn">from</span> <span class="nn">botorch.fit</span> <span class="kn">import</span> <span class="n">fit_gpytorch_model</span>
<span class="kn">from</span> <span class="nn">botorch.generation</span> <span class="kn">import</span> <span class="n">MaxPosteriorSampling</span>
<span class="kn">from</span> <span class="nn">botorch.models</span> <span class="kn">import</span> <span class="n">FixedNoiseGP</span><span class="p">,</span> <span class="n">SingleTaskGP</span>
<span class="kn">from</span> <span class="nn">botorch.optim</span> <span class="kn">import</span> <span class="n">optimize_acqf</span>
<span class="c1">#from botorch.test_functions import Ackley</span>
<span class="kn">from</span> <span class="nn">botorch.utils.transforms</span> <span class="kn">import</span> <span class="n">unnormalize</span>
<span class="kn">from</span> <span class="nn">decimal</span> <span class="kn">import</span> <span class="n">Decimal</span>

<span class="c1">#gpytorch</span>
<span class="kn">import</span> <span class="nn">gpytorch</span>
<span class="kn">from</span> <span class="nn">gpytorch.constraints</span> <span class="kn">import</span> <span class="n">Interval</span>
<span class="kn">from</span> <span class="nn">gpytorch.likelihoods</span> <span class="kn">import</span> <span class="n">GaussianLikelihood</span>
<span class="kn">from</span> <span class="nn">gpytorch.mlls</span> <span class="kn">import</span> <span class="n">ExactMarginalLogLikelihood</span>
<span class="kn">from</span> <span class="nn">gpytorch.priors</span> <span class="kn">import</span> <span class="n">HorseshoePrior</span>

<span class="c1">#torch (we import functions from modules for small speed ups in performance)</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.nn.init</span> <span class="k">as</span> <span class="nn">init</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span> <span class="k">as</span> <span class="n">Function</span>
<span class="kn">from</span> <span class="nn">torch.quasirandom</span> <span class="kn">import</span> <span class="n">SobolEngine</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">device</span> <span class="k">as</span> <span class="n">torch_device</span>
<span class="kn">from</span> <span class="nn">torch.cuda</span> <span class="kn">import</span> <span class="n">is_available</span> <span class="k">as</span> <span class="n">cuda_is_available</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">MSELoss</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">,</span> <span class="n">NLLLoss</span><span class="p">,</span> <span class="n">Parameter</span>

<span class="c1">#other packages</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="kn">from</span> <span class="nn">.custom_loss</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">.defs</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">matplotlib.gridspec</span> <span class="k">as</span> <span class="nn">gridspec</span>

<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="k">def</span> <span class="nf">_inverse_hyperbolic_tangent</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse hyperbolic tangent function</span>

<span class="sd">    0.5 * log((1+z)/(1-z))</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z : pytorch.tensor</span>
<span class="sd">        Desc</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pytorch.tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># z_max = z.abs().max()  + 0.0001</span>
    <span class="c1"># z = z/z_max</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span><span class="o">+</span><span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="n">n_gpus</span><span class="p">,</span> <span class="n">max_calls</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">execute_objective</span><span class="p">(</span><span class="n">parallel_arguments</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">X_turbo_spec</span><span class="p">,</span> <span class="n">trust_region_id</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallelized execution of the objective function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    parallel_arguments : ...</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    parameters : str</span>
<span class="sd">        Description of arg2</span>
<span class="sd">    X_turbo_spec : pytorch.tensor</span>
<span class="sd">        The set of hyper-parameters to test the objective function with</span>
<span class="sd">    trust_region_id : int</span>
<span class="sd">        id for trust region</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    None</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">optimize</span><span class="p">(</span><span class="o">**</span><span class="n">parameters</span><span class="p">)</span>

<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="n">n_gpus</span><span class="p">,</span> <span class="n">max_calls</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">execute_backprop</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>  
                     <span class="n">y0</span><span class="p">,</span> 
                     <span class="n">lr</span> <span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> 
                     <span class="n">plott</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
                     <span class="n">reg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                     <span class="n">plot_every_n_epochs</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> 
                     <span class="n">SAVE_AFTER_EPOCHS</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallelized backpropagation</span>

<span class="sd">    Extended description of function.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    args : dtype</span>
<span class="sd">        Desc</span>
<span class="sd">    y0 : dtype</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    lr : float</span>
<span class="sd">        learning rate</span>
<span class="sd">    plott : bool</span>
<span class="sd">        if True ...</span>
<span class="sd">    reg : dtype</span>
<span class="sd">        Regularization for the loss function</span>
<span class="sd">    plot_every_n_epochs : int</span>
<span class="sd">        plotting interval</span>
<span class="sd">    SAVE_AFTER_EPOCHS : int</span>
<span class="sd">        begin saving the best weights after this many epochs</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        {&quot;weights&quot;: best_weight, &quot;bias&quot; : best_bias, &quot;y&quot; : best_fit, &quot;ydot&quot; : best_ydot, </span>
<span class="sd">          &quot;loss&quot; : {&quot;loss_history&quot; : loss_history}, &quot;best_score&quot; : torch.tensor(best_score)}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># esn, </span>
    <span class="c1"># </span>
    <span class="c1"># epochs = 45000,</span>
    <span class="c1"># custom_loss = custom_loss,</span>
    <span class="c1"># EPOCHS_TO_TERMINATION = None,</span>
    <span class="c1"># f = force,</span>
    <span class="c1"># force_t = None,</span>
    <span class="c1"># lr = 0.05, </span>
    <span class="c1"># </span>
    <span class="c1"># plott = False,</span>
    <span class="c1"># </span>


    <span class="c1">#RC = args[&quot;rc&quot;]</span>
    <span class="n">custom_loss</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;custom_loss&quot;</span><span class="p">]</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;epochs&quot;</span><span class="p">]</span>
    <span class="n">new_X</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;New_X&quot;</span><span class="p">]</span>
    <span class="n">states_dot</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;states_dot&quot;</span><span class="p">]</span>
    <span class="n">LinOut</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;out_W&quot;</span><span class="p">]</span>
    <span class="n">force_t</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;force_t&quot;</span><span class="p">]</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;criterion&quot;</span><span class="p">]</span>
    <span class="n">spikethreshold</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;spikethreshold&quot;</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;t&quot;</span><span class="p">]</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;G&quot;</span><span class="p">]</span>
    <span class="n">g</span><span class="p">,</span> <span class="n">g_dot</span> <span class="o">=</span> <span class="n">G</span>
    <span class="n">gamma_cyclic</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;gamma_cyclic&quot;</span><span class="p">]</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">]</span>
    <span class="n">init_conds</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;init_conds&quot;</span><span class="p">]</span>
    <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;ode_coefs&quot;</span><span class="p">]</span>
    <span class="n">enet_strength</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;enet_strength&quot;</span><span class="p">]</span>
    <span class="n">enet_alpha</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;enet_alpha&quot;</span><span class="p">]</span>
    <span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y0</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="p">],</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gamma_cyclic</span><span class="p">:</span>
        <span class="n">cyclic_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CyclicLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="mi">10</span><span class="o">**-</span><span class="mi">6</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span>
                                            <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma_cyclic</span><span class="p">,</span><span class="c1">#0.9999,</span>
                                            <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;exp_range&quot;</span><span class="p">,</span> <span class="n">cycle_momentum</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">plott</span><span class="p">:</span>
      <span class="c1">#use pl for live plotting</span>
      <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">previous_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">floss_last</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_score</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">pow_</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>


    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
        
        <span class="c1">#begin optimization loop</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">N</span> <span class="o">=</span> <span class="n">LinOut</span><span class="p">(</span> <span class="n">new_X</span><span class="p">)</span>
            
            <span class="n">N_dot</span> <span class="o">=</span> <span class="n">states_dot</span> <span class="o">@</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span> <span class="c1">#esn.calc_Ndot(states_dot)</span>
            
            <span class="n">y</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span><span class="n">N</span> 

            <span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">g</span> <span class="o">*</span> <span class="n">N_dot</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">init_conds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="c1"># y[:,0] = y[:,0] + init_conds[0]</span>
            <span class="c1"># y[:,1] = y[:,1] + init_conds[1]</span>

            <span class="c1">#assert N.shape == N_dot.shape, f&#39;{N.shape} != {N_dot.shape}&#39;</span>

            <span class="c1">#assert esn.LinOut.weight.requires_grad and esn.LinOut.bias.requires_grad</span>

            <span class="c1">#total_ws = esn.LinOut.weight.shape[0] + 1</span>
            <span class="c1">#weight_size_sq = torch.mean(torch.square(esn.LinOut.weight))</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">custom_loss</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ydot</span><span class="p">,</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span><span class="p">,</span> <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">,</span>
                    <span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_conds</span><span class="p">,</span> <span class="n">enet_alpha</span><span class="o">=</span> <span class="n">enet_alpha</span><span class="p">,</span> <span class="n">enet_strength</span> <span class="o">=</span> <span class="n">enet_strength</span><span class="p">,</span> <span class="n">force_t</span> <span class="o">=</span> <span class="n">force_t</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">gamma_cyclic</span> <span class="ow">and</span> <span class="n">e</span> <span class="o">&gt;</span> <span class="mi">100</span> <span class="ow">and</span> <span class="n">e</span> <span class="o">&lt;</span><span class="mi">5000</span><span class="p">:</span>
                <span class="n">cyclic_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>


            <span class="n">floss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">floss</span><span class="p">)</span>


            <span class="k">if</span> <span class="n">e</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">loss_delta</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">floss_last</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">floss</span><span class="p">))</span> 
                <span class="k">if</span> <span class="n">loss_delta</span> <span class="o">&gt;</span> <span class="n">spikethreshold</span><span class="p">:</span><span class="c1"># or loss_delta &lt; -3:</span>
                    <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


            <span class="c1"># if not e and not best_score:</span>
            <span class="c1">#     </span>

            <span class="k">if</span> <span class="n">e</span> <span class="o">&gt;</span> <span class="n">SAVE_AFTER_EPOCHS</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">best_score</span><span class="p">:</span>
                    <span class="n">best_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">floss</span> <span class="o">&lt;</span> <span class="n">best_score</span><span class="p">:</span>  
                    <span class="n">best_bias</span><span class="p">,</span> <span class="n">best_weight</span> <span class="o">=</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="n">best_score</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                    <span class="n">best_fit</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="n">best_ydot</span> <span class="o">=</span> <span class="n">ydot</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">best_bias</span><span class="p">,</span> <span class="n">best_weight</span><span class="p">,</span> <span class="n">best_fit</span> <span class="o">=</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

            <span class="n">floss_last</span> <span class="o">=</span> <span class="n">floss</span>

            <span class="c1"># else:</span>
            <span class="c1">#     if floss &lt; best_score:</span>
            <span class="c1">#         best_bias, best_weight = esn.LinOut.bias.detach(), esn.LinOut.weight.detach()</span>
            <span class="c1">#         best_score = float(loss)</span>
            <span class="c1">#         best_fit = y.clone()</span>
            <span class="c1">#         best_ydot = ydot.clone()</span>
            
            <span class="c1"># if e &gt;= EPOCHS_TO_TERMINATION and EPOCHS_TO_TERMINATION:</span>
            <span class="c1">#     return {&quot;weights&quot;: best_weight, &quot;bias&quot; : best_bias, &quot;y&quot; : best_fit, </span>
            <span class="c1">#           &quot;loss&quot; : {&quot;loss_history&quot; : loss_history},  &quot;best_score&quot; : torch.tensor(best_score),</span>
            <span class="c1">#           &quot;RC&quot; : esn}</span>
            
            <span class="c1"># if plott and e:</span>

            <span class="c1">#     if e % plot_every_n_epochs == 0:</span>
            <span class="c1">#         for param_group in optimizer.param_groups:</span>
            <span class="c1">#             print(&#39;lr&#39;, param_group[&#39;lr&#39;])</span>
            <span class="c1">#         ax[0].clear()</span>
            <span class="c1">#         logloss_str = &#39;Log(L) &#39; + &#39;%.2E&#39; % Decimal((loss).item())</span>
            <span class="c1">#         delta_loss  = &#39; delta Log(L) &#39; + &#39;%.2E&#39; % Decimal((loss-previous_loss).item())</span>

            <span class="c1">#         print(logloss_str + &quot;, &quot; + delta_loss)</span>
            <span class="c1">#         ax[0].plot(y.detach().cpu())</span>
            <span class="c1">#         ax[0].set_title(f&quot;Epoch {e}&quot; + &quot;, &quot; + logloss_str)</span>
            <span class="c1">#         ax[0].set_xlabel(&quot;t&quot;)</span>

            <span class="c1">#         ax[1].set_title(delta_loss)</span>
            <span class="c1">#         ax[1].plot(ydot.detach().cpu(), label = &quot;ydot&quot;)</span>
            <span class="c1">#         #ax[0].plot(y_dot.detach(), label = &quot;dy_dx&quot;)</span>
            <span class="c1">#         ax[2].clear()</span>
            <span class="c1">#         #weight_size = str(weight_size_sq.detach().item())</span>
            <span class="c1">#         #ax[2].set_title(&quot;loss history \n and &quot;+ weight_size)</span>

            <span class="c1">#         ax[2].loglog(loss_history)</span>
            <span class="c1">#         ax[2].set_xlabel(&quot;t&quot;)</span>

            <span class="c1">#         #[ax[i].legend() for i in range(3)]</span>
            <span class="c1">#         previous_loss = loss.item()</span>

            <span class="c1">#         #clear the plot outputt and then re-plot</span>
            <span class="c1">#         display.clear_output(wait=True) </span>
            <span class="c1">#         display.display(pl.gcf())</span>


    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="n">best_weight</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span> <span class="p">:</span> <span class="n">best_bias</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span> <span class="p">:</span> <span class="n">best_fit</span><span class="p">,</span> <span class="s2">&quot;ydot&quot;</span> <span class="p">:</span> <span class="n">best_ydot</span><span class="p">,</span> 
          <span class="s2">&quot;loss&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loss_history&quot;</span> <span class="p">:</span> <span class="n">loss_history</span><span class="p">},</span> <span class="s2">&quot;best_score&quot;</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_score</span><span class="p">)}</span>
          <span class="c1">#&quot;RC&quot; : esn}</span>






<span class="c1">#pytorch elastic net regularization:</span>
<span class="c1">#https://github.com/jayanthkoushik/torch-gel</span>

<span class="c1">#TODO: unit test setting interactive to False.</span>

<span class="c1">#TODO: repair esn documentation (go strait to reinier&#39;s, copy and make adjustments)</span>

<span class="c1">#TODO: rename some pyesn variables.</span>

<span class="k">def</span> <span class="nf">_sech2</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sech2 is the derivative of tanh.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z : pytorch.tensor</span>
<span class="sd">        tensor to perform the sech2 operation on</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pytorch.tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">z</span><span class="p">)))</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">_sigmoid_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Derivative of the sigmoid function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z : pytorch.tensor</span>
<span class="sd">        tensor to perform the operation on</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pytorch.tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">s</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_dfx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span> <span class="n">retain_graph</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">create_graph</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">grad_outputs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    f : str</span>
<span class="sd">        Description of arg2</span>
<span class="sd">    retain_graph : bool</span>
<span class="sd">        Desc</span>
<span class="sd">    grad_outputs : dtype</span>
<span class="sd">        Desc</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># try:</span>
    <span class="c1">#     assert not grad_outputs</span>
    <span class="c1">#     return grad([f],[x], grad_outputs=torch.ones_like(f), </span>
    <span class="c1">#                 create_graph = create_graph, retain_graph = retain_graph)[0]</span>
    <span class="c1"># except:</span>
    <span class="k">return</span> <span class="n">grad</span><span class="p">([</span><span class="n">f</span><span class="p">],[</span><span class="n">x</span><span class="p">],</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">create_graph</span> <span class="o">=</span> <span class="n">create_graph</span><span class="p">,</span> 
                             <span class="n">retain_graph</span> <span class="o">=</span> <span class="n">retain_graph</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_check_x</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tensor_args</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">supervised</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Make sure X is a valid input. </span>
<span class="sd">    X is typically an observer, an input time series for a parameter aware RC</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : torch.tensor</span>
<span class="sd">        Observer time series</span>
<span class="sd">    y : torch.tensor</span>
<span class="sd">        target time series</span>
<span class="sd">    tensor_args : dict</span>
<span class="sd">        arguments to be fed to X, for example device and dtype</span>
<span class="sd">    supervised : bool</span>
<span class="sd">        supervised training or not (unsupervised data-less ODE solution)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X: torch.tensor</span>
<span class="sd">        valid X input (2d, on device etc)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">X</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">supervised</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">tensor_args</span><span class="p">)</span> <span class="c1">#*y.shape,</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">tensor_args</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>  <span class="o">**</span><span class="n">tensor_args</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span>

<span class="k">def</span> <span class="nf">_check_y</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tensor_args</span> <span class="o">=</span> <span class="p">{}):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
         <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">tensor_args</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">y</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">tensor_args</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor_args</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">_printn</span><span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">_name_</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> </span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_nrmse</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Normalized root mean squared error loss function.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    yhat : torch.tensor</span>
<span class="sd">        the network prediction</span>
<span class="sd">    y : torch.tensor</span>
<span class="sd">        the ground truth data, that we would like the RC to fit</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.tensor</span>
<span class="sd">        the error tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">yhat</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_sinsq</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_printc</span><span class="p">(</span><span class="n">string_</span><span class="p">,</span> <span class="n">color_</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">colorz</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s2">&quot;header&quot;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[95m&#39;</span><span class="p">,</span>
          <span class="s2">&quot;blue&quot;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[94m&#39;</span><span class="p">,</span>
          <span class="s1">&#39;cyan&#39;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[96m&#39;</span><span class="p">,</span>
          <span class="s1">&#39;green&#39;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[92m&#39;</span><span class="p">,</span>
          <span class="s1">&#39;warning&#39;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[93m&#39;</span><span class="p">,</span>
          <span class="s1">&#39;fail&#39;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[91m&#39;</span><span class="p">,</span>
          <span class="s1">&#39;endc&#39;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[0m&#39;</span><span class="p">,</span>
           <span class="s1">&#39;bold&#39;</span> <span class="p">:</span><span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[1m&#39;</span><span class="p">,</span>
           <span class="s2">&quot;underline&quot;</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[4m&#39;</span>
        <span class="p">}</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">colorz</span><span class="p">[</span><span class="n">color_</span><span class="p">]</span> <span class="o">+</span> <span class="n">string_</span> <span class="o">+</span> <span class="n">colorz</span><span class="p">[</span><span class="s2">&quot;endc&quot;</span><span class="p">]</span> <span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">end</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_convert_ode_coefs</span><span class="p">(</span><span class="n">ode_coefs_</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#print(&#39;type_X&#39;, type_X)</span>
    <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs_</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">ode_coefs_</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ode_coefs_</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;t&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;^&quot;</span> <span class="ow">or</span> <span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;*&quot;</span> <span class="ow">and</span> <span class="n">coef</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;*&quot;</span><span class="p">)):</span>
                    <span class="n">pow_</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;[^0-9.-]+&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">coef</span><span class="p">))</span>
                    <span class="n">ode_coefs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">=</span> <span class="n">X</span> <span class="o">**</span> <span class="n">pow_</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)]:</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;ode_coefs must be a list of floats or strings of the form &#39;t^pow&#39;, where pow is a real number.&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;ode_coefs must be a list of floats or strings of the form &#39;t^pow&#39;, where pow is a real number.&quot;</span>
    <span class="k">return</span> <span class="n">ode_coefs</span>

<span class="c1"># def execute_backprop(RC):</span>

<span class="c1">#     gd_weights = []</span>
<span class="c1">#     gd_biases = []</span>
<span class="c1">#     ys = []</span>
<span class="c1">#     ydots =[]</span>
<span class="c1">#     scores = []</span>
<span class="c1">#     Ls = []</span>
<span class="c1">#     init_conds_clone = init_conditions.copy()</span>
<span class="c1">#     if not SOLVE:</span>
<span class="c1">#         orig_weights = self.LinOut.weight.clone()</span>
<span class="c1">#         orig_bias = self.LinOut.bias.clone()</span>
        
<span class="c1">#     for i, y0 in enumerate(init_conds_clone[0]):</span>
<span class="c1">#         #print(&quot;w&quot;, i)</span>
<span class="c1">#         if SOLVE:</span>
<span class="c1">#             self.LinOut.weight = Parameter(self.weights_list[i].view(self.n_outputs, -1)).requires_grad_(True)</span>
<span class="c1">#             self.LinOut.bias = Parameter(self.biases_list[i].view(1, self.n_outputs)).requires_grad_(True)</span>
<span class="c1">#         else:</span>
<span class="c1">#             self.LinOut.weight = Parameter(orig_weights.view(self.n_outputs, -1))</span>
<span class="c1">#             self.LinOut.bias = Parameter(orig_bias.view(1, self.n_outputs))</span>
<span class="c1">#         self.init_conds[0] = float(y0)</span>
<span class="c1">#         #print(self.init_conds[0])</span>
<span class="c1">#         with torch.enable_grad():</span>
<span class="c1">#             weight_dict = backprop_f(self, force_t = self.force_t, custom_loss = ODE_criterion, epochs = epochs)</span>

<span class="c1">#         score=weight_dict[&quot;best_score&quot;]</span>
<span class="c1">#         y = weight_dict[&quot;y&quot;]</span>
<span class="c1">#         ydot = weight_dict[&quot;ydot&quot;]</span>
<span class="c1">#         loss, gd_weight, gd_bias = weight_dict[&quot;loss&quot;][&quot;loss_history&quot;], weight_dict[&quot;weights&quot;],  weight_dict[&quot;bias&quot;]</span>
<span class="c1">#         scores.append(score)</span>
<span class="c1">#         ys.append(y)</span>
<span class="c1">#         ydots.append(ydot)</span>
<span class="c1">#         gd_weights.append(gd_weight)</span>
<span class="c1">#         gd_biases.append(gd_bias)</span>
<span class="c1">#         Ls.append(loss)</span>


<span class="n">tanh_activation</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_sech2_</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">z</span><span class="p">)))</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">_identity</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">_neg_sin</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span>  <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_neg_double_sin</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span> <span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_double_cos</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>

<span class="n">tanh_at_2</span> <span class="o">=</span> <span class="mf">0.9640275800</span>
<span class="n">tanh_at_2_half</span> <span class="o">=</span> <span class="mf">0.48201379003</span>

<span class="k">def</span> <span class="nf">_my_relu_i</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">lim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;=</span> <span class="n">lim</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="c1">#tanh_at_2</span>
    <span class="k">elif</span> <span class="n">z</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">lim</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="c1">#tanh_at_2 </span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tanh_at_2_half</span><span class="o">*</span><span class="n">z</span>

<span class="k">def</span> <span class="nf">_rnn_relu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">z</span><span class="o">.</span><span class="n">apply_</span><span class="p">(</span><span class="n">_my_relu_i</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_my_relu_i_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">lim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="n">lim</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">z</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">lim</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_sech2_</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tanh_at_2_half</span>

<span class="k">def</span> <span class="nf">_rnn_relu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">z</span><span class="o">.</span><span class="n">apply_</span><span class="p">(</span><span class="n">_my_relu_i_prime</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_log_sin</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_log_sin_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    sin(log(z))</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z : torch.tensor</span>
<span class="sd">        input tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.tensor</span>
<span class="sd">        activated torch.tensor</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_sin2</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    sin squared</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z : torch.tensor</span>
<span class="sd">        input tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.tensor</span>
<span class="sd">        activated torch.tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">z</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">_sin2_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    derivative of sin2</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z : torch.tensor</span>
<span class="sd">        input tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.tensor</span>
<span class="sd">        activated torch.tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">_sincos</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_sincos_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">z</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_convert_activation_f</span><span class="p">(</span><span class="n">string</span><span class="p">,</span> <span class="n">derivative</span>  <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">both</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summary line.</span>

<span class="sd">    Extended description of function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arg1 : int</span>
<span class="sd">        Description of arg1</span>
<span class="sd">    arg2 : str</span>
<span class="sd">        Description of arg2</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Description of return value</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">_sigmoid_derivative</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">_sech2</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;sin&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;cos&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">,</span> <span class="n">_neg_sin</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;double_cos&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">_double_cos</span><span class="p">,</span> <span class="n">_neg_double_sin</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">_rnn_relu</span><span class="p">,</span> <span class="n">_rnn_relu_prime</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;log_sin&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">_log_sin</span><span class="p">,</span> <span class="n">_log_sin_prime</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;sin2&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">_sin2</span><span class="p">,</span> <span class="n">_sin2_derivative</span>
    <span class="k">elif</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;sincos&quot;</span><span class="p">:</span>
        <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span> <span class="o">=</span>   <span class="n">_sincos</span><span class="p">,</span> <span class="n">_sincos_derivative</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;activation function &#39;</span><span class="si">{</span><span class="n">activation_function</span><span class="si">}</span><span class="s2">&#39; not yet implimented&quot;</span>
    <span class="k">if</span> <span class="n">both</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">act_f</span><span class="p">,</span> <span class="n">act_f_prime</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">derivative</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">act_f</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">act_f_prime</span>


<div class="viewcode-block" id="RcNetwork"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork">[docs]</a><span class="k">class</span> <span class="nc">RcNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class with all functionality to train Reservoir Computers (RCs).</span>
<span class="sd">    Builds and trains RC networks with the specified parameters.</span>
<span class="sd">    In training (fitting), testing and predicting, X is a matrix consisting of column-wise time series features.</span>
<span class="sd">    Y is a zero-dimensional target vector or a matrix consisting of a matrix of column-wise time series vectors.</span>
<span class="sd">    </span>
<span class="sd">    The evolution of the RC is governed by the following formula:</span>
<span class="sd">    :math:`\bf{h}_k = \left(1-\alpha \right)\bf{h}_{k-1} + \alpha \phi \left( \bf{W}_\text{res} \cdot \bf{h}_{k-1}+\bf{W}_\text{in}\cdot \bf{u} + \bf {b}  \right)`</span>
<span class="sd">    where :math:`\bf{h}_k` is the k\ :sup:`th` hidden state, :math:`\alpha` is the leaking rate,</span>
<span class="sd">    :math:`\phi` is the activation function, :math:`\bf{W}_\text{res}` is the matrix of reservoir weights (the adjacency matrix which determines the structure of the reservoir),</span>
<span class="sd">    :math:`\bf{W}_\text{in}` is the set of input weights, :math:`\bf{u}` is the input and :math:`\bf{b}` is the bias.</span>

<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    .. warning ::</span>
<span class="sd">        The :class:`RcBayes` class trains many individual :class:`RcNetwork` instances.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The most important methods are the :meth:`fit`, :meth:`predict`, and :meth:`test`.</span>
<span class="sd">    ..</span>
<span class="sd">        an alernative way to use readthedocs autodoc is to put the method doctrings in the same docstring as</span>
<span class="sd">        as the main class. We decided to just put a docstring in each public method.</span>
<span class="sd">        Methods</span>
<span class="sd">        -------</span>
<span class="sd">        fit(y, x=None, burn_in=100)</span>
<span class="sd">            Train an Echo State Network</span>
<span class="sd">            test(y, x=None, y_start=None, scoring_method=&#39;mse&#39;, alpha=1.)</span>
<span class="sd">            Tests and scores against known output</span>
<span class="sd">            predict(n_steps, x=None, y_start=None)</span>
<span class="sd">            Predicts n values in advance</span>
<span class="sd">    </span>
<span class="sd">    To see the hidden states check out the :attr:`state` or run the :meth:`plot_states` method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    n_nodes : int</span>
<span class="sd">        Number of nodes that together make up the reservoir</span>
<span class="sd">    input_scaling : float</span>
<span class="sd">        The scaling of input values into the network</span>
<span class="sd">    feedback_scaling : float</span>
<span class="sd">        The scaling of feedback values back into the reservoir</span>
<span class="sd">    spectral_radius : float</span>
<span class="sd">        Sets the magnitude of the largest eigenvalue of the transition matrix (weight matrix)</span>
<span class="sd">    leaking_rate : float</span>
<span class="sd">        Specifies how much of the state update &#39;leaks&#39; into the new state</span>
<span class="sd">    connectivity : float</span>
<span class="sd">        The probability that two nodes will be connected</span>
<span class="sd">    regularization : float</span>
<span class="sd">        The L2-regularization parameter used in Ridge regression for model inference</span>
<span class="sd">    feedback : bool</span>
<span class="sd">        Sets feedback of the last value back into the network on or off</span>
<span class="sd">    random_seed : int</span>
<span class="sd">        Seed used to initialize RandomState in reservoir generation and weight initialization</span>
<span class="sd">    backprop: bool</span>
<span class="sd">        if true the network initiates backpropogation.</span>
<span class="sd">    classification: bool</span>
<span class="sd">        if true the network assumes a categorical response, initiates backprop. Not yet working.</span>
<span class="sd">    criterion: torch.nn.Loss function</span>
<span class="sd">        loss function for backprogation training</span>
<span class="sd">    epochs: int</span>
<span class="sd">        the number of epochs to train the network for.</span>
<span class="sd">    l2_prop: float (between 0 and 1)</span>
<span class="sd">        this is the proportion of the l2 norm. if 1, ridge regression. if 0, lasso. in between it&#39;s elastic net regularization.</span>
<span class="sd">        **Please note that a significant slowdown will occur with values other than 0**</span>
<span class="sd">    enet_alpha: float (between 0 and 1)</span>
<span class="sd">        to be used in the context of solving ODEs (unsupervised). Represents the proportion of the </span>
<span class="sd">        elastic net loss that is L2. Specifically the :func:`rctorchprivate.defs.elastic_loss` criterion uses the hyper-parameter in the following way:</span>
<span class="sd">        ``L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)``</span>
<span class="sd">    enet_strength : </span>
<span class="sd">        to be used in the context of solving ODEs (unsupervised). Represents the strength of the</span>
<span class="sd">        elastic net regularization. Specifically the :func:`rctorchprivate.defs.elastic_loss` criterion uses the hyper-parameter in the following way:</span>
<span class="sd">        ``L_reg = enet_strength*(enet_alpha * weight_size_sq + (1- enet_alpha) * weight_size_L1)``</span>
<span class="sd">    n_inputs : int</span>
<span class="sd">        the number of observers (input time-series) that are being input to the model.</span>
<span class="sd">    n_outputs : int</span>
<span class="sd">        the number of time series that the model will try to fit.</span>
<span class="sd">    input_weight_dist: str</span>
<span class="sd">        The probability distribution from which the input weights are drawn. </span>
<span class="sd">        Valid values include `&quot;uniform&quot;`, and `&quot;discrete&quot;`</span>
<span class="sd">    input_weight_dist: str</span>
<span class="sd">        The probability distribution from which the reservoir weights are drawn.  </span>
<span class="sd">        Valid values include `&quot;uniform&quot;`, `&quot;discrete&quot;`, and `&quot;normal&quot;`.</span>
<span class="sd">    output_activation : str</span>
<span class="sd">        the output activation function. Valid values include `&quot;identity&quot;` (the default), `&quot;tanh&quot;`, and `&quot;sin&quot;`.</span>
<span class="sd">        Currently only `&quot;identity&quot;` and `&quot;tanh&quot;` are recommended. Specifically if it is known that the output is bounded,</span>
<span class="sd">        generally in a similar range to the training set, then `&quot;tanh&quot;` is recommended otherwise use the `&quot;identity&quot;`.</span>
<span class="sd">    gamma_cyclic : float</span>
<span class="sd">        gamma hyper-parameter to be passed to torch.optim.lr_scheduler.CyclicLR, only used in certain </span>
<span class="sd">        unsupervised loss functions. Check out the defs.py file where this HP is currently in use in </span>
<span class="sd">        the :func:`rctorchprivate.defs.optimize_last_layer` function.</span>
<span class="sd">    input_connectivity : float</span>
<span class="sd">        connectivity = (1- sparcity) of the input weights (number of non null weights)</span>
<span class="sd">    feedback_connectivity : float</span>
<span class="sd">        connectivity = (1 - sparcity) of the feedback weights (number of non null weights)</span>
<span class="sd">    noise: float</span>
<span class="sd">        random normal noise will be added with the following shape </span>
<span class="sd">        torch.normal(0, 1, size = (self.n_nodes, t)) * self.noise</span>
<span class="sd">        it will be added as :math:`\epsilon` in the equation:</span>
<span class="sd">        :math:`\bf{h}_k = \left(1-\alpha \right)\bf{h}_{k-1} + \alpha f \left( \bf{W}_\text{res} \cdot \bf{h}_{k-1}+\bf{W}_\text{in}\cdot \bf{u} + \bf {b} + \epsilon \right)`</span>


<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#BACKPROP ARGUMENTS (not needed for the homework)</span>
    <span class="c1">#predict_stepwise(y, x=None, steps_ahead=1, y_start=None)</span>
    <span class="c1">#Predicts a specified number of steps into the future for every time point in y-values array (NOT IMPLIMENTED)</span>
    <span class="c1">#Arguments to be implimented later:</span>
    <span class="c1">#    obs_idx = None, resp_idx = None, input_weight_type = None, model_type = &quot;uniform&quot;, PyESNnoise=0.001, </span>
    <span class="c1">#    regularization lr: reg_lr = 10**-4, </span>
    <span class="c1">#    change bias back to &quot;uniform&quot;&quot;&quot;</span>


    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">n_nodes</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> 
                 <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                 <span class="n">connectivity</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">leaking_rate</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span> 
                 <span class="n">spectral_radius</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>                  <span class="c1">#&lt;-- activation, feedback</span>
                 <span class="n">input_scaling</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> 
                 <span class="n">feedback_scaling</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> 
                 <span class="n">activation_function</span> <span class="o">=</span> <span class="s2">&quot;tanh&quot;</span><span class="p">,</span> 
                 <span class="n">output_activation</span> <span class="o">=</span> <span class="s2">&quot;identity&quot;</span><span class="p">,</span> 
                 <span class="n">input_weight_dist</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span> 
                 <span class="n">reservoir_weight_dist</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span> 
                 <span class="n">solve_sample_prop</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">feedback_weight_dist</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span> 
                 <span class="n">feedback</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
                 <span class="n">l2_prop</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
                 <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">,</span> 
                 <span class="n">approximate_reservoir</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="c1">#&lt;-- this line is backprop arguments #beta = None</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">#The commented Arguments below were removed from class definition for the sake of cleanly docs.</span>
        
        <span class="c1">#  id_ = None, </span>
        <span class="c1"># reservoir = None, #&lt;-- process args</span>
        <span class="c1"># classification = False, </span>
        <span class="c1"># n_inputs = None, </span>
        <span class="c1"># n_outputs = None,</span>
        <span class="c1">#  </span>
        <span class="c1"># dt = None,</span>
        <span class="c1"># feedback_connectivity = None,</span>
        <span class="c1"># input_connectivity = None, </span>
        <span class="c1"># gamma = None, </span>
        <span class="c1"># spikethreshold = None,</span>
        <span class="c1"># enet_strength = None, </span>
        <span class="c1"># mu = None, </span>
        <span class="c1"># sigma = None,  </span>
        <span class="c1"># noise = None, #&lt;-- important hyper-parameters</span>
        <span class="c1"># regularization = None, </span>
        <span class="c1"># enet_alpha = None, </span>
        <span class="c1"># gamma_cyclic = None, </span>
        <span class="c1">#device = None,</span>
        <span class="c1">#dtype = None,  </span>

        <span class="n">acceptable_args</span> <span class="o">=</span> <span class="p">[</span>
                 <span class="s1">&#39;acceptable_args&#39;</span><span class="p">,</span> <span class="s1">&#39;approximate_reservoir&#39;</span><span class="p">,</span> <span class="s1">&#39;activation_function&#39;</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> 
                 <span class="s1">&#39;classification&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;connectivity&#39;</span><span class="p">,</span> <span class="s1">&#39;device&#39;</span><span class="p">,</span>  <span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;enet_alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;enet_strength&#39;</span><span class="p">,</span> <span class="s1">&#39;feedback&#39;</span><span class="p">,</span> <span class="s1">&#39;feedback_connectivity&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;feedback_scaling&#39;</span><span class="p">,</span> <span class="s1">&#39;feedback_weight_dist&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma_cyclic&#39;</span><span class="p">,</span> <span class="s1">&#39;id_&#39;</span><span class="p">,</span> <span class="s1">&#39;input_connectivity&#39;</span><span class="p">,</span> 
                 <span class="s1">&#39;input_scaling&#39;</span><span class="p">,</span> <span class="s1">&#39;input_weight_dist&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;leaking_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;l2_prop&#39;</span><span class="p">,</span> <span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="s1">&#39;n_inputs&#39;</span><span class="p">,</span> <span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;n_nodes&#39;</span><span class="p">,</span> <span class="s1">&#39;noise&#39;</span><span class="p">,</span> <span class="s1">&#39;output_activation&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;random_state&#39;</span><span class="p">,</span> <span class="s1">&#39;regularization&#39;</span><span class="p">,</span> <span class="s1">&#39;reservoir&#39;</span><span class="p">,</span> <span class="s1">&#39;reservoir_weight_dist&#39;</span><span class="p">,</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="s1">&#39;spectral_radius&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;solve_sample_prop&#39;</span><span class="p">,</span> <span class="s1">&#39;spikethreshold&#39;</span><span class="p">,</span>  
                 <span class="s1">&#39;kwargs&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;__class__&#39;</span><span class="p">]</span> <span class="c1">#act_f_prime = sech2,  </span>

        

        <span class="c1">#assign attributes to self</span>
        <span class="n">all_args</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">all_args</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            
            <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s1">&#39;self&#39;</span><span class="p">:</span>
                
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">acceptable_args</span><span class="p">:</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">split_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">split_key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;log&#39;</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">split_key</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">acceptable_args</span><span class="p">:</span>

                            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_key</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">10</span><span class="o">**</span><span class="n">val</span><span class="p">)</span>

                            <span class="k">continue</span>
                        <span class="k">else</span> <span class="p">:</span>
                            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;invalid argument, </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">&#39;</span>
                        
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;invalid argument, </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">&#39;</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;invalid argument, </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">&#39;</span>

        <span class="c1">#assign leftover args in the acceptable_args_list as None</span>
        <span class="n">entered_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_args</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">acceptable_args</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">entered_keys</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


                

        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s1">&#39;self&#39;</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

        <span class="c1">#if not self.device:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch_device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">cuda_is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="c1">#else:</span>
        <span class="c1">#    self.device = device</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dev</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;requires_grad&quot;</span> <span class="p">:</span> <span class="kc">False</span><span class="p">}</span>

        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">activation_function</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_f_prime</span> <span class="o">=</span> <span class="n">_convert_activation_f</span><span class="p">(</span><span class="n">activation_function</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">activation_function</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_act_fs</span> <span class="o">=</span> <span class="p">[</span><span class="n">_convert_activation_f</span><span class="p">(</span><span class="n">act_f</span><span class="p">,</span> <span class="n">derivative</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">both</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">act_f</span> <span class="ow">in</span> <span class="n">activation_function</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_act_f_primes</span> <span class="o">=</span> <span class="p">[</span><span class="n">_convert_activation_f</span><span class="p">(</span><span class="n">act_f</span><span class="p">,</span> <span class="n">derivative</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">both</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">act_f</span> <span class="ow">in</span> <span class="n">activation_function</span><span class="p">]</span>
            <span class="c1">#mask = torch.tensor(np.random.choice(list(range(len(self._act_fs))), size = n_nodes)) #(torch.tensor(np.ones(n_nodes)) * torch.rand(250) &lt; 0.5)*1</span>

            <span class="n">n_fs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_fs</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_act_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_fs</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">))</span> <span class="c1">#(torch.tensor(np.ones(n_nodes), **self.dev) * torch.rand(n_nodes) &lt; 0.5)*1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_act_f</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_f_prime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_act_f_prime</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">activation_function</span><span class="p">)</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_act_f</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_act_fs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_act_f_primes</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">act_f</span><span class="p">,</span>  <span class="n">prop</span> <span class="ow">in</span> <span class="n">activation_function</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_act_fs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_convert_activation_f</span><span class="p">(</span><span class="n">act_f</span><span class="p">,</span> <span class="n">derivative</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">both</span> <span class="o">=</span> <span class="kc">False</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_act_f_primes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_convert_activation_f</span><span class="p">(</span><span class="n">act_f</span><span class="p">,</span> <span class="n">derivative</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">both</span> <span class="o">=</span> <span class="kc">False</span><span class="p">))</span>
                <span class="n">probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prop</span><span class="p">)</span>

            <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
            <span class="n">n_fs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_fs</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_act_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_fs</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;inproper activation function input&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">output_activation</span> <span class="o">==</span> <span class="s2">&quot;identity&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f_inv</span>  <span class="o">=</span> <span class="n">_identity</span><span class="p">,</span> <span class="n">_identity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="n">output_activation</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">_inverse_hyperbolic_tangent</span>
        <span class="k">elif</span> <span class="n">output_activation</span> <span class="o">==</span> <span class="s2">&quot;sin&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">asin</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;output_activation </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">output_f</span><span class="si">}</span><span class="s2"> not yet implimented&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)</span>
        <span class="c1">#activation function</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;to use mu and sigma hps use reservoir_weight_dist = &#39;normal&#39;&quot;</span>
        

        <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span> <span class="o">=</span> <span class="p">[</span><span class="n">leaking_rate</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">leaking_rate</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">)</span>

        <span class="c1">#https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19</span>
        <span class="c1">#self.classification = classification</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span> <span class="o">&lt;=</span><span class="mi">1</span>

        <span class="c1"># if self.activation_function != tanh_activation and self.act_f_prime == sech2:</span>
        <span class="c1">#     assert False, f&#39;your activation f is not tanh but act_f_prime is sech2&#39;</span>

        <span class="c1">#cuda (gpu)</span>
        
        <span class="c1"># random state and default tensor arguments</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_grad_</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;requires_grad&quot;</span> <span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;generator&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">no_grad_</span><span class="p">}</span>

        <span class="c1"># hyper-parameters:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        
        

        <span class="c1">#Feedback</span>
        

        <span class="c1">#For speed up: approximate implimentation and preloaded reservoir matrices.</span>
        

        <span class="c1">#elastic net attributes: (default is 1, which is ridge regression for speed)</span>
        
        
        <span class="c1">#Reservoir layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

        
        <span class="c1">#if self.classification:</span>
        <span class="c1">#    self.reg = Linear(self.n_nodes, 2)</span>
        <span class="c1">#    #self.criterion = criterion #torch.nn.CrossEntropyLoss()</span>
        <span class="c1">#else:</span>
        <span class="c1">#    #self.criterion = MSELoss()</span>
        
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gen_reservoir</span><span class="p">()</span>

        <span class="c1">#, &quot;requires_grad&quot;: self.track_in_grad}</span>
        
        <span class="c1">#scaler = &quot;standardize&quot;</span>
        <span class="c1">#if scaler == &quot;standardize&quot;:</span>
        <span class="c1">#    self.scale   = self.stardardize</span>
        <span class="c1">#    self.descale = self.destandardize</span>

        <span class="sd">&quot;&quot;&quot;TODO: additional hyper-parameters</span>
<span class="sd">        noise from pyesn  unlike my implimentation it happens outside the activation function. </span>
<span class="sd">        TBD if this actually can improve the RC.</span>
<span class="sd">        self.PyESNnoise = 0.001</span>
<span class="sd">        self.external_noise = torch.rand(self.n_nodes, device = self.device)</span>
<span class="sd">        colorz = {</span>
<span class="sd">          &quot;header&quot; : &#39;\033[95m&#39;,</span>
<span class="sd">          &quot;blue&quot; : &#39;\033[94m&#39;,</span>
<span class="sd">          &#39;cyan&#39; : &#39;\033[96m&#39;,</span>
<span class="sd">          &#39;green&#39; : &#39;\033[92m&#39;,</span>
<span class="sd">          &#39;warning&#39; : &#39;\033[93m&#39;,</span>
<span class="sd">          &#39;fail&#39; : &#39;\033[91m&#39;,</span>
<span class="sd">          &#39;endc&#39; : &#39;\033[0m&#39;,</span>
<span class="sd">           &#39;bold&#39; :&#39;\033[1m&#39;,</span>
<span class="sd">           &quot;underline&quot; : &#39;\033[4m&#39;</span>
<span class="sd">        }&quot;&quot;&quot;</span>
        <span class="c1">#print(&quot;finished building RC&quot;)</span>



    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">n_nodes</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)</span>
        <span class="n">connect</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span><span class="p">)</span>
        <span class="n">spect</span>   <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spectral_radius</span><span class="p">)</span>

        <span class="n">strr</span> <span class="o">=</span> <span class="s2">&quot;{&quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;n_nodes : </span><span class="si">{</span><span class="n">n_nodes</span><span class="si">}</span><span class="s2">, connectivity : </span><span class="si">{</span><span class="n">connect</span><span class="si">}</span><span class="s2">, spectral_radius : </span><span class="si">{</span><span class="n">spect</span><span class="si">}</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;}&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;EchoStateNetwork: &quot;</span> <span class="o">+</span> <span class="n">strr</span>

    <span class="k">def</span> <span class="nf">_multiple_act_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">activation_function</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_fs</span><span class="p">):</span>

            <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_mask</span> <span class="o">==</span> <span class="n">i</span>
            <span class="n">new_X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">activation_function</span><span class="p">(</span><span class="n">new_X</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">new_X</span>

    <span class="k">def</span> <span class="nf">_multiple_act_f_prime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">activation_function</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_f_primes</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_masks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">new_X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">activation_function</span><span class="p">(</span><span class="n">new_X</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">new_X</span>


    <span class="c1"># def plot_reservoir(self):</span>
    <span class="c1">#     &quot;&quot;&quot;Plot the network weights&quot;&quot;&quot;</span>
    <span class="c1">#     sns.histplot(self.weights.cpu().numpy().view(-1,))</span>

    <span class="c1"># def forward(self, t, input_, current_state, output_pattern):</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     Arguments:</span>
    <span class="c1">#         t: the current timestep</span>
    <span class="c1">#         input_: the input vector for timestep t</span>
    <span class="c1">#         current_state: the current hidden state at timestep t</span>
    <span class="c1">#         output_pattern: the output pattern at timestep t.</span>
    <span class="c1">#     Returns:</span>
    <span class="c1">#         next_state: a torch.tensor which is the next hidden state</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     # generator = self.random_state, device = self.device)</span>

    <span class="c1">#     preactivation = self.LinIn(input_) + self.bias_ + self.LinRes(current_state)</span>

    <span class="c1">#     if self.feedback:</span>
    <span class="c1">#         preactivation += self.LinFeedback(output_pattern)</span>
        
    <span class="c1">#     #alternative: uniform noise</span>
    <span class="c1">#     #self.noise = torch.rand(self.n_nodes, **self.tensor_args).view(-1,1) if noise else None</span>

    <span class="c1">#     update = self.activation_function(preactivation) # + self.PyESNnoise * (self.external_noise - 0.5)</span>
    <span class="c1">#     if self.noise != None:</span>
    <span class="c1">#         #noise_vec = torch.normal(mean = torch.zeros(self.n_nodes, device = self.device), </span>
    <span class="c1">#         #                              std = torch.ones(self.n_nodes, device = self.device),</span>
    <span class="c1">#         #                              generator = self.random_state)* self.noise</span>
    <span class="c1">#         noise_vec = torch.rand(self.n_nodes, **self.tensor_args) * self.noise</span>
    <span class="c1">#         update += noise_vec </span>
    <span class="c1">#     next_state = self.leaking_rate[0] * update + self.leaking_rate[1] * current_state</span>
    <span class="c1">#     return next_state</span>

    <span class="c1"># def preactivation_beta(self, t, input_vector, recurrent_vec, bias, betas):</span>
    <span class="c1">#     return input_vector + recurrent_vec +  self.bias * self.beta[t-1,:]</span>

    <span class="k">def</span> <span class="nf">_preactivation_vanilla</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">,</span> <span class="n">recurrent_vec</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span> <span class="c1">#, betas):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">input_vector</span> <span class="o">+</span> <span class="n">recurrent_vec</span> <span class="o">+</span>  <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

    <span class="k">def</span> <span class="nf">_preactivation_noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">,</span> <span class="n">recurrent_vec</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">input_vector</span> <span class="o">+</span> <span class="n">recurrent_vec</span> <span class="o">+</span>  <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_z</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span>

    <span class="c1"># def activate(self, dt):</span>
    <span class="c1">#     alpha = self.alpha ** dt, self.alpha ** (1 - dt)</span>


    <span class="k">def</span> <span class="nf">_train_state_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">retain_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Arguments:</span>
<span class="sd">            t: the current timestep</span>
<span class="sd">            input_: the input vector for timestep t</span>
<span class="sd">            current_state: the current state at timestep t</span>
<span class="sd">            output_pattern: the output pattern at timestep t.</span>
<span class="sd">        Returns:</span>
<span class="sd">            next_state: a torch.tensor which is the next hidden state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">breakpoint</span><span class="p">()</span>

        <span class="n">input_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
        <span class="n">recurrent_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">preactivation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivation</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">,</span> <span class="n">recurrent_vec</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="c1">#, self.beta)</span>

        <span class="c1">#feedback_vec = self.LinFeedback(y)</span>

        <span class="c1">#preactivation = preactivation + feedback_vec</span>

        <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">preactivation</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">update</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span>
        <span class="c1"># if output:</span>
        <span class="c1">#     return next_state, self.LinOut(torch.cat([X, next_state], axis = 0).view(self.n_outputs,-1))</span>
        <span class="c1">#next_extended_state = torch.hstack((X, next_state)).view(1,-1)</span>

        <span class="c1">#breakpoint()</span>
        <span class="c1">#output = self.LinOut(next_extended_state)</span>
        <span class="c1">#assert False, f&#39;{X.shape} {next_state.shape} {self.LinOut.weight.shape}, {output.shape} &#39;</span>
        
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="kc">None</span><span class="c1">#output #.view(self.n_outputs,-1))</span>

    <span class="k">def</span> <span class="nf">_train_state_feedback_unsupervised</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">retain_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Arguments:</span>
<span class="sd">            t: the current timestep</span>
<span class="sd">            input_: the input vector for timestep t</span>
<span class="sd">            current_state: the current state at timestep t</span>
<span class="sd">            output_pattern: the output pattern at timestep t.</span>
<span class="sd">        Returns:</span>
<span class="sd">            next_state: a torch.tensor which is the next hidden state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#try:</span>
        <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
        <span class="c1"># except:</span>
        <span class="c1">#     breakpoint()</span>

        <span class="n">input_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
        <span class="n">recurrent_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">preactivation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivation</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">,</span> <span class="n">recurrent_vec</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="c1">#, self.beta)</span>

        <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">preactivation</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">update</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span>
        
        <span class="n">next_extended_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">next_state</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">next_extended_state</span><span class="p">)</span>
        <span class="c1">#assert False, f&#39;{X.shape} {next_state.shape} {self.LinOut.weight.shape}, {output.shape} &#39;</span>
        
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_train_state_vanilla</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">retain_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Arguments:</span>
<span class="sd">            t: the current timestep</span>
<span class="sd">            input_: the input vector for timestep t</span>
<span class="sd">            current_state: the current state at timestep t</span>
<span class="sd">            output_pattern: the output pattern at timestep t.</span>

<span class="sd">        The function split makes sense for a speedup (remove the if statement)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#assert False</span>
        <span class="n">input_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">recurrent_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="n">preactivation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivation</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">,</span> <span class="n">recurrent_vec</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="c1">#, self.beta)</span>

        <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">preactivation</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">update</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span>
        <span class="c1"># if output:</span>
        <span class="c1">#     return next_state, self.LinOut(torch.cat([X, next_state], axis = 0).view(self.n_outputs,-1))</span>
        <span class="c1"># else:</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_train_state_vanilla_rs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">retain_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Arguments:</span>
<span class="sd">            t: the current timestep</span>
<span class="sd">            input_: the input vector for timestep t</span>
<span class="sd">            current_state: the current state at timestep t</span>
<span class="sd">            output_pattern: the output pattern at timestep t.</span>

<span class="sd">        The function split makes sense for a speedup (remove the if statement)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#assert False</span>
        <span class="n">input_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">recurrent_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="n">preactivation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivation</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">,</span> <span class="n">recurrent_vec</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="c1">#, self.beta)</span>

        <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">preactivation</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">update</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span>
        <span class="c1"># if output:</span>
        <span class="c1">#     return next_state, self.LinOut(torch.cat([X, next_state], axis = 0).view(self.n_outputs,-1))</span>
        <span class="c1"># else:</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_output_i</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">extended_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">next_state</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">extended_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">extended_states</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">extended_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_calc_Ndot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states_dot</span><span class="p">,</span> <span class="n">cutoff</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        cutoff: whether or not to cutoff</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#if self.burn_in and cutoff:</span>
        <span class="c1">#    states_dot = torch.cat((states_dot[0,:].view(1,-1), states_dot[(self.burn_in + 1):,:]), axis = 0)</span>
        <span class="c1">#else:</span>
        <span class="c1">#    states_dot = states_dot</span>
        <span class="n">dN_dx</span> <span class="o">=</span> <span class="n">states_dot</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span>
        <span class="k">return</span> <span class="n">dN_dx</span>

    <span class="k">def</span> <span class="nf">_gen_discrete_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">connectivity</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sparcity</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">connectivity</span><span class="p">),</span> <span class="n">connectivity</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">np_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma</span><span class="p">],</span>  <span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="n">sparcity</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">size</span> <span class="o">=</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np_weights</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weights</span>

    <span class="c1"># in_weights = torch.rand(n, m, generator = self.random_state, device = self.device, requires_grad = False)</span>
    <span class="c1">#                 in_weights =  (in_weights * 2) - 1</span>
    <span class="c1">#                 if self.input_connectivity is not None:</span>
    <span class="c1">#                     accept = torch.rand(n, m, **self.tensor_args) &lt; self.input_connectivity</span>
    <span class="c1">#                     in_weights *= accept</span>

    <span class="k">def</span> <span class="nf">_gen_uniform_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">connectivity</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">m</span>  <span class="o">=</span> <span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">generator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span>  <span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">connectivity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">accept</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_args</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">connectivity</span>
            <span class="n">weights</span> <span class="o">*=</span> <span class="n">accept</span>
        <span class="k">return</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">_gen_reservoir</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_idx</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">targ_idx</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">load_failed</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates random reservoir from parameters set at initialization.&quot;&quot;&quot;</span>
        <span class="c1"># Initialize new random state</span>

        <span class="c1">#random_state = np.random.RandomState(self.random_state)</span>

        <span class="n">max_tries</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Will usually finish on the first iteration</span>
        <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span>

        <span class="c1">#if the size of the reservoir has changed, reload it.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">n_nodes_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">:</span>
                <span class="n">load_failed</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">already_warned</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">book_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_tries</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">_printc</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="s1">&#39;fail&#39;</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>

            <span class="c1">#only initialize the reservoir and connectivity matrix if we have to for speed in esn_cv.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">approximate_reservoir</span> <span class="ow">or</span> <span class="n">load_failed</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">accept</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_args</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span>
               
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_args</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
                    <span class="n">shape_tuple</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)</span>
                    <span class="n">ones_tensor</span><span class="p">,</span> <span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_tuple</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape_tuple</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="n">ones_tensor</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">zeros_tensor</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;discrete&quot;</span><span class="p">:</span>
                    <span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_discrete_weights</span><span class="p">(</span><span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir_sigma</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span><span class="p">,</span> <span class="n">connectivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span><span class="p">)</span>
                    <span class="c1"># sigma = self.reservoir_sigma</span>
                    <span class="c1"># sparcity, c = (1 - self.connectivity), self.connectivity/2</span>
                    <span class="c1"># dim = (self.n_nodes, self.n_nodes)</span>
                    <span class="c1"># np_weights = np.random.choice([0, -sigma, sigma],  p = [sparcity, c, c], size = dim)</span>
                    <span class="c1"># self.weights = torch.tensor(np_weights, **self.dev)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{self.reservoir_weight_dist}</span><span class="s2"> reservoir_weight_distribution not yet implimented&quot;</span>


                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accept</span>
                <span class="c1">#self.weights = csc_matrix(self.weights)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#print(&quot;LOADING MATRIX&quot;, load_failed)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">approximate_reservoir</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">get_approx_preRes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">reservoir_pre_weights</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">accept</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

                        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">accept</span><span class="p">;</span> <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">reservoir_pre_weights</span><span class="p">;</span>

                    <span class="c1">#_printc(&quot;reservoir successfully loaded (&quot; + str(self.weights.shape) , &#39;green&#39;) </span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span><span class="p">:</span>
                        <span class="n">_printc</span><span class="p">(</span><span class="s2">&quot;approx reservoir &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; failed to load ...regenerating...&quot;</span><span class="p">,</span> <span class="s1">&#39;fail&#39;</span><span class="p">)</span>
                    <span class="c1">#skip to the next iteration of the loop</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">number_of_preloaded_sparse_sets</span><span class="p">:</span>
                        <span class="n">load_failed</span> <span class="o">=</span> <span class="mi">1</span>
                        <span class="n">_printc</span><span class="p">(</span><span class="s2">&quot;All preloaded reservoirs Nilpotent, generating random reservoirs, connectivity =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39;...regenerating&#39;</span><span class="p">,</span> <span class="s1">&#39;fail&#39;</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;TODO, case not yet handled.&quot;</span>

            <span class="n">max_eigenvalue</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="c1">#.type(torch.float32) .sort(descending = True).values.</span>
             
            <span class="c1">#max_eigenvalue = self.weights.eig(eigenvectors = False)[0].abs().max()</span>
            
            <span class="k">if</span> <span class="n">max_eigenvalue</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span> 
                <span class="k">if</span> <span class="ow">not</span> <span class="n">already_warned</span><span class="p">:</span>
                    <span class="n">_printc</span><span class="p">(</span><span class="s2">&quot;Loaded Reservoir is Nilpotent (max_eigenvalue =</span><span class="si">{}</span><span class="s2">), connectivity =</span><span class="si">{}</span><span class="s2">.. .regenerating&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_eigenvalue</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">connectivity</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span> <span class="s1">&#39;fail&#39;</span><span class="p">)</span>
                <span class="n">already_warned</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1">#if we have run out of pre-loaded reservoirs to draw from :</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">max_tries</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Nilpotent reservoirs are not allowed. Increase connectivity and/or number of nodes.&#39;</span><span class="p">)</span>

        <span class="c1"># Set spectral radius of weight matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">spectral_radius</span> <span class="o">/</span> <span class="n">max_eigenvalue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        
        <span class="k">if</span> <span class="n">load_failed</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">no_grad_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">state</span>

        <span class="c1"># Set output weights to none to indicate untrained ESN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_weights</span> <span class="o">=</span> <span class="kc">None</span>
             

    <span class="k">def</span> <span class="nf">_set_Win</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build the input weights.</span>
<span class="sd">        Currently only uniform and discrete weights implimented.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span>
            <span class="c1">#weight</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span> <span class="ow">or</span> <span class="s1">&#39;in_weights&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="p">):</span> 
                
                <span class="c1">#print(&quot;GENERATING IN WEIGHTS&quot;)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
                    <span class="n">in_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_uniform_weights</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">),</span> <span class="n">connectivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_connectivity</span><span class="p">)</span>
                    <span class="c1"># in_weights = torch.rand(n, m, generator = self.random_state, device = self.device, requires_grad = False)</span>
                    <span class="c1"># in_weights =  (in_weights * 2) - 1</span>
                    <span class="c1"># if self.input_connectivity is not None:</span>
                    <span class="c1">#     accept = torch.rand(n, m, **self.tensor_args) &lt; self.input_connectivity</span>
                    <span class="c1">#     in_weights *= accept</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;discrete&quot;</span><span class="p">:</span>

                    <span class="c1">#input_scaling is input sigma.</span>
                    <span class="n">in_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_discrete_weights</span><span class="p">(</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">connectivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_connectivity</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;input_weight_dist </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_weight_dist</span><span class="si">}</span><span class="s1"> not implimented, try uniform or discrete.&#39;</span>
                
            <span class="k">else</span><span class="p">:</span>
                
                <span class="n">in_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">in_weights</span> <span class="c1">#+ self.noise * self.reservoir.noise_z One possibility is to add noise here, another is after activation.</span>
                
                <span class="c1">##### Later for speed re-add the feedback weights here.</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span><span class="p">:</span>
                    
                    <span class="n">feedback_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_scaling</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">reservoir</span><span class="o">.</span><span class="n">feedback_weights</span>
                
                    <span class="c1">#in_weights = torch.hstack((in_weights, feedback_weights)).view(self.n_nodes, -1)</span>

            <span class="n">in_weights</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_scaling</span>

            <span class="c1">#if there is white noise add it in (this will be much more useful later with the exponential model)</span>
            <span class="c1">#populate this bias matrix based on the noise</span>

            <span class="c1">#bias</span>
            <span class="c1">#uniform bias can be seen as means of normal random variables.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
                <span class="c1">#random uniform distributed bias</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)]:</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">no_grad_</span><span class="p">)</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

                <span class="c1">#you could also add self.noise here.</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_</span> <span class="o">=</span> <span class="n">bias</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span><span class="p">:</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
                    <span class="n">feedback_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_uniform_weights</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">o</span><span class="p">),</span> <span class="n">connectivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_connectivity</span><span class="p">)</span> <span class="c1">#torch.rand(self.n_nodes, self.n_outputs, device = self.device, requires_grad = False, generator = self.random_state) * 2 - 1</span>
                    <span class="n">feedback_weights</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_scaling</span>
                    <span class="n">feedback_weights</span> <span class="o">=</span> <span class="n">feedback_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_weight_dist</span> <span class="o">==</span> <span class="s2">&quot;discrete&quot;</span><span class="p">:</span>

                    <span class="c1">#input_scaling is input sigma.</span>
                    <span class="n">feedback_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_discrete_weights</span><span class="p">(</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">o</span><span class="p">),</span> <span class="n">connectivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_connectivity</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;feedback weight dist </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">feedback_weight_dist</span><span class="si">}</span><span class="s1"> not implimented. Try uniform or discrete&#39;</span>

                
                <span class="n">feedback_weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">feedback_weights</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_state_feedback_unsupervised</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_state_feedback</span> 

            <span class="k">else</span><span class="p">:</span>
                <span class="n">feedback_weights</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="c1">#self.train_state = self.train_state_feedback_unsupervised</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_state_vanilla_rs</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1">#self.train_state = self.train_state_feedback </span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_state_vanilla</span>

        <span class="k">if</span> <span class="n">feedback_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">in_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">in_weights</span><span class="p">,</span> <span class="n">feedback_weights</span><span class="p">))</span>
   
        <span class="n">in_weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">in_weights</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1">#in_weights._name_ = &quot;in_weights&quot;</span>

        <span class="k">return</span> <span class="n">in_weights</span><span class="c1">#(in_weights, feedback_weights)</span>
    
    <span class="c1">#def _check_device_cpu(self):</span>
    <span class="c1">#    #TODO: make a function that checks if a function is on the cpu and moves it there if not</span>
    <span class="c1">#    pass</span>

<div class="viewcode-block" id="RcNetwork.display_in_weights"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.display_in_weights">[docs]</a>    <span class="k">def</span> <span class="nf">display_in_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot a heatmap of the input weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="RcNetwork.display_out_weights"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.display_out_weights">[docs]</a>    <span class="k">def</span> <span class="nf">display_out_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot a heatmap of the output weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="RcNetwork.display_res_weights"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.display_res_weights">[docs]</a>    <span class="k">def</span> <span class="nf">display_res_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot a heatmap of the reservoir weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="RcNetwork.plot_states"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.plot_states">[docs]</a>    <span class="k">def</span> <span class="nf">plot_states</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plots up to n hidden states.</span>

<span class="sd">        :: note::</span>
<span class="sd">            The :attr:`state` is being plotted.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n : int</span>
<span class="sd">            Number of hidden states to plot</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[:,</span><span class="n">i</span><span class="p">]))),</span> <span class="n">RC</span><span class="o">.</span><span class="n">state</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_freeze_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="n">names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;LinOut.weight&quot;</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="c1">#print(&#39;param:&#39;, name,  params.requires_grad)</span>

    <span class="k">def</span> <span class="nf">_train_states_supervised</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span> <span class="c1">#calc_grads : bool = True, </span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : tensor</span>
<span class="sd">            Description</span>
<span class="sd">        y : tensor</span>
<span class="sd">            Descriptio</span>
<span class="sd">        states : tensor</span>
<span class="sd">            Description</span>
<span class="sd">        outputs : bool</span>
<span class="sd">            Description</span>
<span class="sd">        calc_grads: bool</span>
<span class="sd">            Description</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        states : tensor</span>
<span class="sd">            Description</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#self.state_grads = []</span>
        <span class="c1">#self.state_list = []</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>


            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="c1">#print(&quot;super&quot;)</span>
                <span class="c1"># self.state[t, :] = self.forward(t, input_ = X[t, :].T,</span>
                <span class="c1">#                                        current_state = self.state[t-1,:], </span>
                <span class="c1">#                                        output_pattern = y[t-1]).squeeze()</span>
                <span class="n">input_t</span> <span class="o">=</span>  <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span>
                <span class="n">state_t</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">input_t</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,:])</span>

                <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">state_t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
            <span class="c1"># for t in range(0, X.shape[0]):</span>
            <span class="c1">#     input_t =  X[t, :].T</span>
            <span class="c1">#     state_t, _ = self.train_state(t, X = input_t,</span>
            <span class="c1">#                               state = states[t,:], </span>
            <span class="c1">#                               y = y[t,:],</span>
            <span class="c1">#                               output = False)</span>


            <span class="c1">#     states = torch.cat([states, state_t.view(-1, self.n_nodes)], axis = 0)</span>

        <span class="k">return</span> <span class="n">states</span>

    <span class="k">def</span> <span class="nf">_train_states_unsupervised</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal function: Train states unsupervised.</span>

<span class="sd">        The unsupervised part of RcTorch solves differential equations.</span>
<span class="sd">        These arguments were stripped calc_grads : bool = True, outputs : bool = True</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        y : str</span>
<span class="sd">            Description of arg2</span>
<span class="sd">        states : dtype</span>
<span class="sd">            Description</span>
<span class="sd">        calc_grads : bool</span>
<span class="sd">            Description</span>
<span class="sd">        outputs : bool</span>
<span class="sd">            Description</span>



<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tensor</span>
<span class="sd">            states</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#self.state_grads = []</span>
        <span class="c1">#self.state_list = []</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

            <span class="c1">#output_prev = y[0]</span>

            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">input_t</span> <span class="o">=</span>  <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span>
                <span class="c1">#assert False,f&#39;y {y.shape} input_t {input_t.shape}&#39;</span>
                <span class="n">state_t</span><span class="p">,</span> <span class="n">output_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">input_t</span><span class="p">,</span>
                                                     <span class="n">state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span> 
                                                     <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span>
                                                     <span class="n">output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>


                <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">state_t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
                <span class="c1">#output_prev = output_t</span>

        <span class="c1">#outputs = self.LinOut(states)</span>
        <span class="k">return</span> <span class="n">states</span>

    <span class="k">def</span> <span class="nf">_extend_X</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">random_sampling</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : dtype</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        random_sampling : bool?</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tensor</span>
<span class="sd">            X_extended</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">random_sampling</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span><span class="p">)</span>
            <span class="n">neg_X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">X_extended</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">neg_X</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="ow">and</span> <span class="n">random_sampling</span><span class="p">:</span>

            <span class="n">dt_mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dt</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span> <span class="o">*</span> <span class="n">dt_mu</span><span class="p">)</span>
            <span class="n">neg_X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">X_extended</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">neg_X</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;implementation incomplete&quot;</span>

            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span><span class="o">**</span><span class="n">dt_mu</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_new</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span> <span class="n">alpha</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_extended</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">return</span> <span class="n">X_extended</span>

    <span class="k">def</span> <span class="nf">_calc_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates bias</span>

<span class="sd">        Extended description of function.</span>
<span class="sd">        In particular when is this function relevant?</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        weights : pytorch.tensor</span>
<span class="sd">            weights</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tensor</span>
<span class="sd">            bias tensor, shape (...)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_means</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_means</span> <span class="o">@</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">_solve_supervised</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">return_states</span><span class="p">,</span> <span class="n">SCALE</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve, via ridge regularized linear regression, for the output weights.</span>
<span class="sd">        &quot;supervised&quot; means if the esn has data.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : torch.tensor</span>
<span class="sd">            target time series</span>
<span class="sd">        return_states : bool</span>
<span class="sd">            if True the function returns (states, y, burn_in)</span>
<span class="sd">        SCALE: bool</span>
<span class="sd">            whether or not to scale the data</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">         if True the function returns (states, y, burn_in)</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_sampling</span><span class="p">:</span>
            <span class="n">n_time_points</span>  <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">bool_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_time_points</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solve_sample_prop</span>

            
            <span class="c1"># ones_row = torch.ones( train_x.shape[0], 1, **self.dev)</span>
            <span class="c1"># train_x = torch.hstack((ones_row, train_x))</span>

            <span class="n">train_x</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">bool_mask</span><span class="p">]</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">bool_mask</span><span class="p">]</span>
            

        <span class="bp">self</span><span class="o">.</span><span class="n">_x_means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y_means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        

        
        <span class="n">train_x</span> <span class="o">=</span> <span class="n">train_x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_means</span>

        <span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="n">ridge_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">train_x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ridge_x_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">ridge_x</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">ridge_x_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">ridge_x</span><span class="p">)</span>


        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">train_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">train_y</span> <span class="o">=</span> <span class="n">train_y</span>

   
            <span class="n">ridge_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">ridge_x_inv</span> <span class="o">@</span> <span class="n">ridge_y</span>


            <span class="c1">#intercept line, pyds inspired</span>
            <span class="c1">#self.intercept_ = self._y_mean - self._x_means @ self.coef_</span>
        
            <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_means</span> <span class="o">@</span> <span class="n">weight</span>
            <span class="c1">#bias = self._y_means[i] - self._x_means @ weight</span>

            <span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

        
        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">biases</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span>

        <span class="c1"># self.N = self.LinOut(self.extended_states)</span>
                
        <span class="c1"># # Store last y value as starting value for predictions</span>
        <span class="c1"># self.lastoutput = y[-1, :]</span>

        <span class="c1"># if self.burn_in:</span>
        <span class="c1">#     self.N = self.N[self.burn_in:,:]</span>
        <span class="c1">#     self.N = self.N.view(-1, self.n_outputs)</span>
        <span class="c1">#     self.X = self.X[self.burn_in:]</span>
            
        <span class="c1"># # Return all data for computation or visualization purposes (Note: these are normalized)</span>
        <span class="c1"># if return_states:</span>
        <span class="c1">#     return extended_states, (y[1:,:] if self.feedback else y), self.burn_in</span>
        <span class="c1"># else:</span>
        <span class="c1">#     print(&quot;burn in&quot;, self.burn_in, &quot;ex&quot;, self.extended_states.shape, &quot;linout&quot;, self.LinOut.weight.shape)</span>
        <span class="c1">#     #self.yfit = self.LinOut(self.extended_states)</span>
        <span class="c1">#     if SCALE:   </span>
        <span class="c1">#         self.yfit = self._output_stds * self.N + self._output_means</span>
        <span class="c1">#     if not SCALE:</span>
        <span class="c1">#         assert False</span>
        <span class="c1">#     return self.yfit</span>
        <span class="k">if</span> <span class="n">return_states</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">:,:]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span> <span class="k">else</span> <span class="n">y</span><span class="p">),</span> <span class="n">burn_in</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">yfit_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="p">)</span> <span class="c1">#self.LinOut.weight.cpu()@self.extended_states.T.cpu() + self.LinOut.bias.cpu()</span>
            <span class="c1">#yfit = self._output_stds.cpu()* (yfit_norm)+ self._output_means.cpu()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_descale</span><span class="p">(</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">yfit_norm</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span>

    <span class="c1"># def fit_unsupervised(self):</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     if the esn is unsupervised, this fork.</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     pass</span>

    <span class="c1"># def fit_hamiltonian(self):</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     for the new project</span>
    <span class="c1">#     &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_center_H</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keep</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Centers the hidden states?</span>

<span class="sd">        INSTRUCTIONS:</span>
<span class="sd">        1. assign `_x_means` to self, along the axis such that </span>
<span class="sd">           the numbers of means matches the number of features (2)</span>
<span class="sd">        2. assign `_y_mean` to self (y.mean())</span>
<span class="sd">        3. subtract _x_means from X and assign it to X_centered</span>
<span class="sd">        4. subtract _y_mean from y and assign it to y_centered</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : tensor or array-like</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        outputs : tensor or array-like</span>
<span class="sd">            Description of arg2</span>
<span class="sd">        keep : bool</span>
<span class="sd">            if true the means and standard deviations will be saved</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">inputs</span>

            <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_x_means</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_x_stds</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

            <span class="n">X_centered</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_means</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_stds</span>
            <span class="k">return</span> <span class="n">X_centered</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">outputs</span>

            <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_means</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

            <span class="n">y_centered</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_means</span> <span class="c1">#(y - y_means)/y_stds</span>

            <span class="k">return</span> <span class="n">y_centered</span>

    <span class="k">def</span> <span class="nf">_assign_random_sampling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dt</span> <span class="o">=</span> <span class="n">x1</span><span class="o">-</span><span class="n">x2</span>
        

        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
        <span class="c1">#breakpoint()</span>

        
        <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span> <span class="o">=</span>  <span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_assign_const_leaking_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#self.leaking_rate_orig = self.leaking_rate.copy()</span>

        <span class="n">ones</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ones</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ones</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">ones</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">ones</span>
        


<div class="viewcode-block" id="RcNetwork.fit"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
            <span class="n">y</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">X</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">burn_in</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
            <span class="n">criterion</span> <span class="o">=</span>  <span class="n">MSELoss</span><span class="p">(),</span> 
            
            <span class="n">return_states</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
            <span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">out_weights</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">ODE_order</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">SOLVE</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">reparam_f</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">init_conditions</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">force</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">ode_coefs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">train_score</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
            <span class="n">ODE_criterion</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">q</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">eq_system</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">backprop_f</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">epochs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">nl</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">n_inputs</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">n_outputs</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">random_sampling</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
            <span class="n">sample_timepoints</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
            <span class="n">verbose</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">):</span> <span class="c1">#beta = None, </span>
        
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train the network by fitting the hidden states and then solving for the output weights.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        .. note::</span>
<span class="sd">            To see the hidden states check out the :attr:`extended_states` attribute!</span>

<span class="sd">        .. warning::</span>
<span class="sd">            If you enter ODE_order &gt;=1  then the RC will perform unsupervised differential equation solving.</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : pytorch.tensor or numpy.array</span>
<span class="sd">            Target</span>
<span class="sd">        X : pytorch.tensor or numpy.array</span>
<span class="sd">            Observers</span>
<span class="sd">        burn_in : int</span>
<span class="sd">            number of initial steps to throw away, similar to role in Markov Chain Monte Carlo simulations</span>
<span class="sd">        criterion : dtype</span>
<span class="sd">            loss function</span>
<span class="sd">        epochs : int</span>
<span class="sd">            the number of epochs to train</span>
<span class="sd">        return_states : bool</span>
<span class="sd">            if True return the hidden states </span>
<span class="sd">        nl : bool</span>
<span class="sd">            If True then the network tries to solve the Bernoulli differential eq family</span>
<span class="sd">        eq_system : bool</span>
<span class="sd">            If True then the network tries to solve a system of differential equations</span>
<span class="sd">        n_inputs : int</span>
<span class="sd">            the number of input timeseries, if None then the network will use teacher forcing with a pure prediction</span>
<span class="sd">        n_outputs : int</span>
<span class="sd">            the number of output timeseries</span>
<span class="sd">        backprop_f : function</span>
<span class="sd">            the backpropagation loss function</span>
<span class="sd">        ODE_order : int</span>
<span class="sd">            the order of the differential equation that the network will solve. </span>
<span class="sd">            The default value is None. If left as None then the network will not perform unsupervised training and</span>
<span class="sd">            will instead default to supervised training.</span>
<span class="sd">        q : float</span>
<span class="sd">            a hyper-parameter related to solving the Bernoulli family of differential equations.</span>
<span class="sd">        train_score : bool</span>
<span class="sd">            if True the network will return the train_score as well. This is for use in unsupervised </span>
<span class="sd">            equations.</span>
<span class="sd">        epochs: int</span>
<span class="sd">            number of epochs to train (for use with unsupervised non-linear diffeqs)</span>
<span class="sd">        ode_coefs : list</span>
<span class="sd">            list of ODE coefficients</span>
<span class="sd">        random_sampling: bool</span>
<span class="sd">            if True then the network will perform random sampling, instead of uniform sampling, of time points.</span>
<span class="sd">            This makes the network considerably more powerful when solving diffeqs.</span>
<span class="sd">        sample_timepoints : int</span>
<span class="sd">            if the :attr:`random_sampling` argument is True then the network will sample this many timepoints.</span>


<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        TODO doctstring</span>

<span class="sd">        NLLLoss(),</span>
<span class="sd">        </span>
<span class="sd">        </span>
<span class="sd">        Arguments: TODO</span>
<span class="sd">            y: response matrix</span>
<span class="sd">            x: observer matrix</span>
<span class="sd">            burn in: obvious</span>
<span class="sd">            verbose:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if verbose:</span>
        <span class="c1">#     if nl:</span>
        <span class="c1">#         print(&quot;nonlinear!&quot;)</span>
        <span class="c1">#     else:</span>
        <span class="c1">#         print(&quot;linear&quot;)</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            
            <span class="n">non_assign_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">non_assign_keys</span><span class="p">:</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

            <span class="c1"># if random_sampling and not self.ODE_order:</span>
            <span class="c1">#     assert False, &quot;random sampling not implimented in supervised case&quot;</span>


            <span class="c1">#self.reparam = reparam_f</span>

            
            
            
            <span class="bp">self</span><span class="o">.</span><span class="n">track_in_grad</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1">#track_in_grad</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">preactivation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preactivation_noise</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">preactivation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preactivation_vanilla</span>

            <span class="c1">########################### beta arguments are currently silenced #################</span>
            <span class="c1">#self.beta = beta</span>
            <span class="c1"># if beta is None:</span>
            <span class="c1">#     self.preactivation = self.preactivation_vanilla </span>
            <span class="c1"># else:</span>
            <span class="c1">#     self.preactivation = self.preactivation_beta</span>
            <span class="c1">########################### beta arguments are currently silenced #################</span>
            

            <span class="c1">#assert len(init_conditions) == ODE_order</span>
            
            <span class="c1">#SCALE = True</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                <span class="n">SCALE</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">SCALE</span> <span class="o">=</span> <span class="kc">False</span>
            
            <span class="c1">#ensure that y is a 2-dimensional tensor with pre-specified arguments.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span> 

                <span class="bp">self</span><span class="o">.</span><span class="n">train_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_states_unsupervised</span>
                
                <span class="k">assert</span> <span class="n">init_conditions</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
                        <span class="c1">#weird randomization condition</span>
                        <span class="n">multiple_ICs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="c1">#print(&#39;randomizing init conds&#39;)</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">)):</span>
                            <span class="n">init_conditions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="n">init_conditions</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">high</span> <span class="o">=</span> <span class="n">init_conditions</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                        <span class="n">init_conditions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_conditions</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
                    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
                        <span class="n">multiple_ICs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">multiple_ICs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
                    <span class="n">multiple_ICs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">init_conditions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span>  <span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">]:</span>
                    <span class="n">multiple_ICs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;malformed ICs, either use a list, a float or an integer&#39;</span>
                <span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_conditions</span>
                
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
                    

                    <span class="n">start</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

                    <span class="k">if</span> <span class="ow">not</span> <span class="n">random_sampling</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
                        <span class="n">nsteps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">stop</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span><span class="p">)</span>
                        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">nsteps</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">sample_timepoints</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">stop</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">+</span> <span class="n">start</span>

                        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span><span class="o">.</span><span class="n">values</span>
                        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">dt</span> <span class="o">=</span> <span class="n">x1</span><span class="o">-</span><span class="n">x2</span>
                        
                        <span class="c1">#if not hasattr(self, &quot;leaking_rate_orig&quot;):</span>
                            
                        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">alpha</span><span class="p">),</span> <span class="n">alpha</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    
                        
                        
                        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">([])</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                    <span class="n">x0</span><span class="p">,</span> <span class="n">xf</span><span class="p">,</span> <span class="n">nsteps</span> <span class="o">=</span> <span class="n">X</span> <span class="c1">#6*np.pi, 100</span>
                    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">xf</span><span class="p">,</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">nsteps</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Please input start, stop, dt&quot;</span>

                <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">y</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">),</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>

                
                
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#ensure that X is a two dimensional tensor, or if X is None declare a tensor.</span>

                <span class="n">X</span> <span class="o">=</span> <span class="n">_check_x</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">,</span> <span class="n">supervised</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">X</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">_check_y</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tensor_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> 
                



                <span class="k">if</span> <span class="n">random_sampling</span><span class="p">:</span>
                    <span class="c1">#X = torch.rand(sample_timepoints + 2)</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">sample_timepoints</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                    <span class="n">idx</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

                    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

                    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>


                    <span class="bp">self</span><span class="o">.</span><span class="n">_assign_random_sampling</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="kc">False</span>

                    <span class="c1">#self.leaking_rate_orig = [new_predict_lr, 1-new_predict_lr]</span>
                    <span class="c1">#X = X[:-1]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_assign_const_leaking_rate</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                
                
                    

                <span class="bp">self</span><span class="o">.</span><span class="n">y_tr</span> <span class="o">=</span> <span class="n">y</span>
                
                <span class="k">if</span> <span class="n">SCALE</span><span class="p">:</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span><span class="p">)</span>    
                <span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span> <span class="o">=</span> <span class="n">multiple_ICs</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_states_supervised</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_in_grad</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>


            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="c1">#.clone()</span>

                <span class="k">if</span> <span class="n">SCALE</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="c1">#.clone()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_input_stds</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_input_means</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span>


            <span class="c1">##at this point you can take a random sample? now it must happen sooner.</span>


            <span class="bp">self</span><span class="o">.</span><span class="n">X_extended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extend_X</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_sampling</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span><span class="p">:</span>
                <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                
                <span class="n">y_extended</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X_extended</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">y_extended</span> <span class="o">=</span> <span class="n">y</span>

            <span class="c1"># if self.betas is None:</span>
            <span class="c1">#     self.betas = torch.ones_like(X[:,0].view(-1,1))</span>

            <span class="n">start_index</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span> <span class="k">else</span> <span class="mi">0</span> 
            <span class="n">rows</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">start_index</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> 
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;you must enter n_outputs&#39;</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">noise_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_extended</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">lastinput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            
            <span class="n">start_index</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span> <span class="k">else</span> <span class="mi">0</span> 
            <span class="n">rows</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">start_index</span>

            
            <span class="n">combined_weights</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">combined_weights</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span>  <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                    <span class="c1">#self.LinFeedback = Linear(self.n_inputs, self.n_nodes, bias = False)</span>
                    <span class="c1">#self.LinFeedback = Linear(self.n_inputs, self.n_nodes, bias = False)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span>  <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">LinFeedback</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

                <span class="c1">#self.LinIn = Linear(self.n_inputs, self.n_nodes,  bias = False)</span>
                
                <span class="c1">#self.LinIn.weight, self.LinFeedback.weight = self.set_Win()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_Win</span><span class="p">()</span>


                <span class="c1"># assert isinstance(n_inputs, int), &quot;you must enter n_inputs. This is the number of input time series (int)&quot;</span>
                <span class="c1"># assert isinstance(n_outputs, int), &quot;you must enter n_outputs. This is the number of output time series (int)&quot;</span>

            <span class="c1">#self.LinOutDE = Linear(self.n_nodes + 1, self.n_outputs)</span>

            <span class="c1">################################################################################################</span>
            <span class="c1">#+++++++++++++++++++++++++++         FORWARD PASS AND SOLVE             ++++++++++++++++++++++++</span>
            <span class="c1">################################################################################################</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">track_in_grad</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_freeze_weights</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">out_weights</span><span class="p">:</span>
                    <span class="c1">#self.SOLVE = SOLVE = False</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span>  <span class="n">Parameter</span><span class="p">(</span><span class="n">out_weights</span><span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">out_weights</span><span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span> 
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
                    <span class="k">except</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

                <span class="c1"># CUSTOM_AUTOGRAD_F = False</span>
                <span class="c1"># if CUSTOM_AUTOGRAD_F:</span>
                <span class="c1">#     #for more information on custom pytorch autograd functions see the following link:</span>
                <span class="c1">#     #https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html</span>
                <span class="c1">#     recurrence = Recurrence.apply</span>

                <span class="c1">#     self.states, self.states_dot, states_dot = recurrence(self.states, self, self.X_extended, y)</span>
                <span class="c1"># else:</span>

                

                <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">),</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>
                

                <span class="c1">#drop the first state and burned states</span>
                
                <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_states</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_extended</span><span class="p">,</span> <span class="n">y_extended</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span>
                    
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">burn_in</span><span class="p">:]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">burn_in</span><span class="p">:]</span>

                <span class="c1"># calculate hidden state derivatives</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span><span class="p">:</span>
                        <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">updates</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
                    <span class="c1"># if self.ODE_order == 2:</span>
                    <span class="c1">#     self.states_dot2 = - self.alpha * self.states_dot + self.alpha * self.act_f_prime(updates) * (self.LinIn.weight.T + self.bias + self.LinRes(self.states_dot))</span>
                    <span class="c1">#     self.states_dot2 = torch.cat((torch.zeros_like(self.X), self.states_dot2), axis = 1)</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparam_f</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span><span class="c1">#, order = self.ODE_order)</span>

                <span class="c1">#append columns for the data:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
                <span class="c1">#self.extended_states = torch.hstack((self.X, self.states))</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">laststate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span>

                <span class="c1">#add rows corresponding to bias to the states </span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sb</span> <span class="o">=</span> <span class="n">states_with_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">sb1</span> <span class="o">=</span> <span class="n">states_dot_with_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

                    <span class="c1"># do the same for the second derivatives</span>
                    <span class="c1"># if self.ODE_order == 2:</span>
                    <span class="c1">#     self.sb2 = states_dot2_with_bias = torch.cat((torch.zeros_like(self.states_dot2[:,0].view(-1,1)), self.states_dot2), axis = 1)</span>
                    <span class="n">g</span><span class="p">,</span> <span class="n">g_dot</span> <span class="o">=</span> <span class="n">G</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">g</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_conditions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">_convert_ode_coefs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ode_coefs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>



                <span class="c1">#self.laststate = self.extended_states[-1, 1:]</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">force</span> <span class="o">=</span> <span class="n">force</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">force</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

                    <span class="c1"># if not SOLVE and not self.ODE_order:</span>
                    <span class="c1">#     train_x = self.extended_states</span>
                    <span class="c1">#     ones_row = torch.ones( train_x.shape[0], 1, **self.dev)</span>
                    <span class="c1">#     train_x = torch.hstack((ones_row, train_x))</span>

                    <span class="k">if</span> <span class="n">SOLVE</span><span class="p">:</span> <span class="c1">#and not out_weights:</span>
                        <span class="c1">#print(&quot;SOLVING!&quot;)</span>

                        <span class="c1">#include everything after burn_in </span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_supervised</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_states</span><span class="p">,</span> <span class="n">SCALE</span><span class="p">)</span>

                        <span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

                        <span class="c1">#print(&quot;ridge regularizing&quot;)</span>
                        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>

                            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">multiple_ICs</span><span class="p">:</span>
                                    <span class="c1">#only implemented for first order</span>

                                    <span class="k">try</span><span class="p">:</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">As</span> <span class="o">=</span> <span class="n">As</span> <span class="o">=</span> <span class="p">[</span><span class="n">y0</span> <span class="o">*</span> <span class="n">g</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y0</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
                                    <span class="k">except</span><span class="p">:</span>
                                        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="si">}</span><span class="s1">&#39;</span>
                                    <span class="n">init_cond_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span>
                                    <span class="c1">#init_cond_list = [[y0, self.init_conds[1]] for y0 in self.init_conds[0]]</span>
                                     
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_conds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">g</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">)]</span> 


                                <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span>


                                <span class="c1"># if self.Hargun_changes:</span>
                                <span class="c1">#     H, H_dot = self.extended_states, self.states_dot #</span>
                                <span class="c1"># else:</span>
                                <span class="n">H</span><span class="p">,</span> <span class="n">H_dot</span> <span class="o">=</span> <span class="n">states_with_bias</span><span class="p">,</span> <span class="n">states_dot_with_bias</span>
                                
                                
                                <span class="bp">self</span><span class="o">.</span><span class="n">gH</span> <span class="o">=</span> <span class="n">gH</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">H</span>

                                <span class="bp">self</span><span class="o">.</span><span class="n">gH_mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gH</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">gH_dot</span> <span class="o">=</span> <span class="n">gH_dot</span> <span class="o">=</span>  <span class="n">g_dot</span> <span class="o">*</span> <span class="n">H</span> <span class="o">+</span>  <span class="n">g</span> <span class="o">*</span> <span class="n">H_dot</span>

                                <span class="k">if</span> <span class="n">eq_system</span><span class="p">:</span>
                                    
                                    <span class="n">S</span><span class="p">,</span> <span class="n">S_dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gH</span><span class="p">,</span> <span class="n">gH_dot</span>

                                    <span class="k">if</span> <span class="n">multiple_ICs</span><span class="p">:</span>
                                        <span class="n">p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                                        <span class="n">ones_vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> 
                                        <span class="n">p0_vec</span> <span class="o">=</span> <span class="n">ones_vec</span> <span class="o">*</span> <span class="n">p0</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">Cxs</span> <span class="o">=</span> <span class="n">Cxs</span> <span class="o">=</span> <span class="p">[]</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">Cps</span> <span class="o">=</span> <span class="n">Cps</span> <span class="o">=</span> <span class="p">[]</span>
                                        <span class="k">for</span> <span class="n">y0</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                                            <span class="c1">#print(&quot;y0&quot;, y0)</span>
                                            <span class="n">y0_vec</span> <span class="o">=</span> <span class="n">ones_vec</span> <span class="o">*</span> <span class="n">y0</span>
                                            <span class="n">Cx</span> <span class="o">=</span> <span class="o">-</span><span class="n">y0_vec</span> <span class="o">@</span> <span class="n">S</span> <span class="o">+</span> <span class="n">p0_vec</span> <span class="o">@</span> <span class="n">S_dot</span>
                                            <span class="n">Cp</span> <span class="o">=</span> <span class="o">-</span><span class="n">y0_vec</span> <span class="o">@</span> <span class="n">S_dot</span> <span class="o">-</span> <span class="n">p0_vec</span> <span class="o">@</span> <span class="n">S</span>
                                            <span class="n">Cxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Cx</span><span class="p">)</span>
                                            <span class="n">Cps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Cp</span><span class="p">)</span>

                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="n">y0</span><span class="p">,</span> <span class="n">p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span>
                                        <span class="n">ones_vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> 

                                        <span class="n">y0_vec</span> <span class="o">=</span> <span class="n">ones_vec</span> <span class="o">*</span> <span class="n">y0</span>
                                        <span class="n">p0_vec</span> <span class="o">=</span> <span class="n">ones_vec</span> <span class="o">*</span> <span class="n">p0</span>

                                        <span class="bp">self</span><span class="o">.</span><span class="n">Cx</span> <span class="o">=</span> <span class="n">Cx</span> <span class="o">=</span> <span class="o">-</span><span class="n">y0_vec</span> <span class="o">@</span> <span class="n">S</span> <span class="o">+</span> <span class="n">p0_vec</span> <span class="o">@</span> <span class="n">S_dot</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">Cp</span> <span class="o">=</span> <span class="n">Cp</span> <span class="o">=</span> <span class="o">-</span><span class="n">y0_vec</span> <span class="o">@</span> <span class="n">S_dot</span> <span class="o">-</span> <span class="n">p0_vec</span> <span class="o">@</span> <span class="n">S</span>

                                    <span class="n">sigma1</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S</span> <span class="o">+</span> <span class="n">S_dot</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_dot</span>

                                    <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="o">=</span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">sigma1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">sigma1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>

                                    <span class="bp">self</span><span class="o">.</span><span class="n">Delta</span><span class="o">=</span><span class="n">Delta</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_dot</span> <span class="o">-</span> <span class="n">S_dot</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S</span>

                                    <span class="c1">#try:</span>
                                    <span class="n">delta_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">Delta</span><span class="p">)</span>
                                    <span class="n">sigma_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>
                                    <span class="n">D_H</span> <span class="o">=</span> <span class="n">Sigma</span> <span class="o">@</span> <span class="n">delta_inv</span> <span class="o">+</span> <span class="n">Delta</span> <span class="o">@</span> <span class="n">sigma_inv</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">Lam</span><span class="o">=</span><span class="n">Lam</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">D_H</span><span class="p">)</span>
                                    
                                    <span class="c1">#assert False, f&#39;{Cx.shape}, {Cp.shape} {Lam.shape} {Sigma.shape} {Delta.shape}&#39;</span>
                                    <span class="k">if</span> <span class="ow">not</span> <span class="n">multiple_ICs</span><span class="p">:</span>

                                        <span class="bp">self</span><span class="o">.</span><span class="n">Wy</span> <span class="o">=</span> <span class="n">Wy</span> <span class="o">=</span>  <span class="n">Lam</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">Cx</span> <span class="o">@</span> <span class="n">delta_inv</span>  <span class="o">+</span> <span class="n">Cp</span> <span class="o">@</span>  <span class="n">sigma_inv</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">Wp</span> <span class="o">=</span> <span class="n">Wp</span> <span class="o">=</span>  <span class="n">Lam</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">Cp</span> <span class="o">@</span> <span class="n">delta_inv</span>  <span class="o">-</span> <span class="n">Cx</span> <span class="o">@</span>  <span class="n">sigma_inv</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                                        
                                        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">Wy</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">Wp</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

                                        <span class="n">bias</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                                        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span> <span class="o">=</span> <span class="p">[]</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span> <span class="o">=</span> <span class="p">[]</span>
                                        <span class="n">Wys</span><span class="p">,</span> <span class="n">Wps</span> <span class="o">=</span>  <span class="p">[],</span> <span class="p">[]</span>
                                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Cps</span><span class="p">)):</span>
                                            <span class="n">Cx</span><span class="p">,</span> <span class="n">Cp</span><span class="p">,</span> <span class="n">nX</span> <span class="o">=</span> <span class="n">Cxs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Cps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                                            <span class="n">Wy</span> <span class="o">=</span> <span class="n">Lam</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">Cx</span> <span class="o">@</span> <span class="n">delta_inv</span>  <span class="o">+</span> <span class="n">Cp</span> <span class="o">@</span>  <span class="n">sigma_inv</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="n">nX</span>
                                            <span class="n">Wp</span> <span class="o">=</span> <span class="n">Lam</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">Cp</span> <span class="o">@</span> <span class="n">delta_inv</span>  <span class="o">-</span> <span class="n">Cx</span> <span class="o">@</span>  <span class="n">sigma_inv</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="n">nX</span>
                                            <span class="n">Wys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Wy</span><span class="p">)</span>
                                            <span class="n">Wps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Wp</span><span class="p">)</span>

                                            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">Wy</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">Wp</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

                                            <span class="n">bias</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                                            <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                                            <span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
                                        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span>
                                        <span class="n">biases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span>
                                    
                                <span class="k">else</span><span class="p">:</span>

                                
                                    <span class="c1">#if self.ODE_order == 1:</span>

                                    <span class="c1">#oscillator</span>
                                    <span class="c1">#########################################################</span>
                                    
                                    <span class="c1">#########################################################</span>

                                    <span class="c1">#nonlinear:</span>
                                    <span class="c1">#########################################################</span>
                                    <span class="k">if</span> <span class="n">multiple_ICs</span><span class="p">:</span>
                                        <span class="k">if</span> <span class="n">nl</span><span class="p">:</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">y0s</span> <span class="o">=</span> <span class="n">y0s</span>  <span class="o">=</span> <span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                                            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ode_coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">D_As</span> <span class="o">=</span> <span class="n">D_As</span> <span class="o">=</span> <span class="p">[</span><span class="n">y0</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="n">y0</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span> <span class="k">for</span> <span class="n">y0</span> <span class="ow">in</span> <span class="n">y0s</span><span class="p">]</span>
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="n">D_As</span> <span class="o">=</span> <span class="p">[</span><span class="n">A</span><span class="o">*</span><span class="n">ode_coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span> <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">As</span><span class="p">]</span>
                                    <span class="k">else</span><span class="p">:</span>

                                        <span class="k">if</span> <span class="n">nl</span><span class="p">:</span>
                                            <span class="n">y0</span>  <span class="o">=</span> <span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                                            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ode_coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
                                            <span class="c1">#q=0.5</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">D_A</span> <span class="o">=</span> <span class="n">D_A</span> <span class="o">=</span>  <span class="n">y0</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="n">y0</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span>
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="c1">######x###################################################</span>
                                            <span class="c1">#population:</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">D_A</span> <span class="o">=</span> <span class="n">D_A</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ode_coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span>   
                                    
                                        
                                    <span class="c1"># D_A = A[0] * ode_coefs[0] - force(t)                                  </span>
                                    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">                                    try:</span>
<span class="sd">                                        D_A = A * ode_coefs[0] - force(t)</span>
<span class="sd">                                    except:</span>
<span class="sd">                                        assert False, f&#39;{ode_coefs[0].shape} {self.X.shape} {A[0].shape} {force(t).shape}&#39;</span>
<span class="sd">                                    &quot;&quot;&quot;</span>
                                    <span class="c1"># elif self.ODE_order == 2:</span>
                                    <span class="c1">#     w= self.ode_coefs[0]</span>
                                    <span class="c1">#     y0, v0 = init_conds[0], init_conds[1]</span>
                                    <span class="c1">#     D_A = v0 *(g_dot2 + w**2*g )+ w**2 * y0 -force(t)</span>

                                    <span class="k">if</span> <span class="n">nl</span><span class="p">:</span>                                
                                        <span class="k">if</span> <span class="ow">not</span> <span class="n">multiple_ICs</span><span class="p">:</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">DH</span> <span class="o">=</span> <span class="n">DH</span> <span class="o">=</span>  <span class="n">gH_dot</span> <span class="o">+</span> <span class="n">p</span><span class="o">*</span><span class="n">gH</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">*</span><span class="n">y0</span><span class="o">*</span><span class="n">gH</span> 
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">DHs</span> <span class="o">=</span> <span class="n">DHs</span> <span class="o">=</span> <span class="p">[</span><span class="n">gH_dot</span> <span class="o">+</span> <span class="n">p</span><span class="o">*</span><span class="n">gH</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">*</span><span class="n">y0</span><span class="o">*</span><span class="n">gH</span> <span class="k">for</span> <span class="n">y0</span> <span class="ow">in</span> <span class="n">y0s</span><span class="p">]</span>
                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="c1">#assert False</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">DH</span> <span class="o">=</span> <span class="n">DH</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">gH</span> <span class="o">+</span> <span class="n">ode_coefs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">gH_dot</span>   

                                    <span class="c1"># if self.ODE_order == 2:</span>

                                    <span class="c1">#     H_dot2 = states_dot2_with_bias</span>
                                    <span class="c1">#     DH = 2 * H *(g_dot ** 2 + g*g_dot2) + g*(4*g_dot*H_dot + g*H_dot2)  + w**2 * g**2*H</span>
                                    <span class="c1">#     term1 = 2*H*g_dot**2</span>
                                    <span class="c1">#     term2 = 2*g*(2*g_dot*H_dot + H*g_dot2)</span>
                                    <span class="c1">#     term3 = g**2*(w**2*H + H_dot2)</span>
                                    <span class="c1">#     DH = term1 +  term2 + term3</span>
                                    <span class="c1">#there will always be the same number of initial conditions as the order of the equation.</span>
                                    <span class="c1">#D_A = G[0] + torch.sum([ G[i + 1] * condition for i, condition in enumerate(initial_conditions)])</span>
                                    <span class="k">if</span> <span class="ow">not</span> <span class="n">nl</span><span class="p">:</span>
                                        <span class="c1">#################</span>
                                        <span class="n">DH</span> <span class="o">=</span> <span class="n">DH</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
                                        <span class="c1">#DH = self._center_H(inputs = DH, keep = True)</span>


                                        <span class="c1">#self.extended_states = self.extended_states - self.extended_states.mean(axis = 0)</span>
                                        <span class="c1">#################</span>
                                        <span class="n">DH1</span> <span class="o">=</span> <span class="n">DH</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">DH</span>

                                        <span class="c1">#xx, _ = self._center_data(DH, D_As[0])</span>

                                        <span class="c1">#if nl:</span>
                                        <span class="c1">#    DH1 = DH1 -2 * self.q* D_A.T * gH.T @ gH</span>
                                        
                                        <span class="bp">self</span><span class="o">.</span><span class="n">DH1</span> <span class="o">=</span> <span class="n">DH1</span> <span class="o">=</span> <span class="n">DH1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">DH1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>
                                        <span class="n">DHinv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">DH1</span><span class="p">)</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">DH2</span> <span class="o">=</span> <span class="n">DH2</span> <span class="o">=</span> <span class="n">DHinv</span> <span class="o">@</span> <span class="n">DH</span><span class="o">.</span><span class="n">T</span>
                                        <span class="k">if</span> <span class="ow">not</span> <span class="n">multiple_ICs</span><span class="p">:</span>
                                            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="o">-</span><span class="n">DH2</span><span class="p">,</span> <span class="n">D_A</span><span class="p">)</span>

                                            <span class="n">bias</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                                            <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

                                            <span class="n">DH</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_center_H</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="o">-</span><span class="n">DH</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

                                            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">D_A</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D_As</span><span class="p">):</span>


                                                <span class="n">_</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_center_H</span><span class="p">(</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">D_A</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
                                                <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="o">-</span><span class="n">DH2</span><span class="p">,</span> <span class="n">D_A</span><span class="p">)</span>
                                                <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_bias</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
                                                <span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
                                                <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

                                            <span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span> <span class="o">=</span> <span class="n">biases</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span> <span class="o">=</span> <span class="n">weights</span>

                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="c1">################################</span>
                                        <span class="n">DH1s</span> <span class="o">=</span> <span class="p">[</span><span class="n">DH</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">DH</span> <span class="k">for</span> <span class="n">DH</span> <span class="ow">in</span> <span class="n">DHs</span><span class="p">]</span>

                                        <span class="c1">#gH_sq = gH.T @ gH </span>
                                        <span class="c1">#DH1p2s = [-2 * self.q* D_A.T * gH_sq for D_A in D_As]</span>
                                        <span class="n">nl_corrections</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">*</span> <span class="n">D_A</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">gH</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">gH</span>  <span class="k">for</span> <span class="n">D_A</span> <span class="ow">in</span> <span class="n">D_As</span><span class="p">]</span>
                                        <span class="c1">#self.DH1 -2 * self.q* D_A.T * gH.T @ gH</span>

                                        <span class="bp">self</span><span class="o">.</span><span class="n">DH1s</span> <span class="o">=</span> <span class="n">DH1s</span> <span class="o">=</span> <span class="p">[</span><span class="n">DH1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">correction</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nl_corrections</span><span class="p">)]</span>
                                        
                                        <span class="c1">#DH1 = DH1 + self.regularization * torch.eye(DH1.shape[1], **self.dev)</span>

                                        <span class="n">DHinvs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">DH1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">DH1s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">))</span><span class="k">for</span> <span class="n">DH1</span> <span class="ow">in</span> <span class="n">DH1s</span><span class="p">]</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">DH2s</span> <span class="o">=</span> <span class="n">DH2s</span> <span class="o">=</span> <span class="p">[</span><span class="n">DHinv</span> <span class="o">@</span> <span class="n">DHs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">DHinv</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">DHinvs</span><span class="p">)]</span>

                                        <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>



                                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">D_As</span><span class="p">)):</span>
                                            <span class="n">DH2</span> <span class="o">=</span> <span class="n">DH2s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                                            <span class="n">D_A</span> <span class="o">=</span> <span class="n">D_As</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                                            <span class="n">init_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="o">-</span><span class="n">DH2</span><span class="p">,</span> <span class="n">D_A</span><span class="p">)</span>

                                            <span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init_weight</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                                            <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init_weight</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                                        <span class="c1">#assert False</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span> <span class="o">=</span> <span class="n">biases</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span> <span class="o">=</span> <span class="n">weights</span>

                                    
                                        


                        <span class="c1">#     elif not self.ODE_order:</span>
                        <span class="c1">#         ones_row = torch.ones( train_x.shape[0], 1, **self.dev)</span>
                            
                        <span class="c1">#         ridge_x = torch.matmul(train_x.T, train_x) + \</span>
                        <span class="c1">#                            self.regularization * torch.eye(train_x.shape[1], **self.dev)</span>

                        <span class="c1">#         ridge_y = torch.matmul(train_x.T, train_y)</span>

                        <span class="c1">#         ridge_x_inv = torch.pinverse(ridge_x)</span>
                        <span class="c1">#         weight = ridge_x_inv @ ridge_y</span>

                        <span class="c1">#         bias = weight[0]</span>
                        <span class="c1">#         weight = weight[1:]</span>

                        <span class="c1"># if not multiple_ICs:</span>
                        <span class="c1">#     self.LinOut.weight = Parameter(weight.view(self.n_outputs, -1))</span>
                        <span class="c1">#     self.LinOut.bias = Parameter(bias.view(1, self.n_outputs))</span>

            <span class="k">if</span> <span class="n">multiple_ICs</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">backprop_f</span><span class="p">:</span>
                    <span class="n">init_conds_clone</span> <span class="o">=</span> <span class="n">init_conditions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">ydots</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">last_outputs</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="c1">#init_conds_clone = init_conds.copy()</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span><span class="p">):</span>
                        <span class="c1">#print(&quot;w&quot;, i)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span>
                        <span class="k">if</span> <span class="n">SOLVE</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                        

                        <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">init_conds_clone</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>

                        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="p">)</span>
                        <span class="n">N_dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_Ndot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span><span class="p">,</span> <span class="n">cutoff</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                        <span class="n">yfit</span> <span class="o">=</span> <span class="n">g</span><span class="o">*</span><span class="n">N</span>
                        
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">eq_system</span><span class="p">:</span>
                            <span class="n">yfit</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">yfit</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                        <span class="k">else</span><span class="p">:</span>
                            
                            <span class="n">yfit</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">yfit</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">cond</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">init_conds</span><span class="p">):</span>
                                <span class="n">yfit</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">yfit</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">cond</span>
                        
                        <span class="k">if</span> <span class="n">train_score</span><span class="p">:</span>
                            <span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span>  <span class="n">g</span> <span class="o">*</span> <span class="n">N_dot</span>
                            <span class="n">ydots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ydot</span><span class="p">)</span>
                            <span class="n">score</span> <span class="o">=</span> <span class="n">ODE_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">yfit</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">ydot</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                    <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">,</span> <span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_cond_list</span><span class="p">,</span> 
                                                    <span class="n">enet_strength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span><span class="p">,</span> <span class="n">enet_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">,</span>
                                                    <span class="n">force_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span><span class="p">)</span>
                            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

                        <span class="n">last_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
                        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">yfit</span><span class="p">)</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="c1">#.clone()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_conditions</span>
                    <span class="k">if</span> <span class="n">train_score</span><span class="p">:</span>
                        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;scores&quot;</span> <span class="p">:</span> <span class="n">scores</span><span class="p">,</span> 
                                <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="n">weights</span><span class="p">,</span> 
                                <span class="s2">&quot;biases&quot;</span> <span class="p">:</span> <span class="n">biases</span><span class="p">,</span>
                                <span class="s2">&quot;ys&quot;</span>     <span class="p">:</span> <span class="n">ys</span><span class="p">,</span>
                                <span class="s2">&quot;ydots&quot;</span>  <span class="p">:</span> <span class="n">ydots</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">ys</span><span class="p">,</span> <span class="n">ydots</span>
                <span class="k">else</span><span class="p">:</span>

                    <span class="c1"># if self.parallel_backprop:</span>

                    <span class="c1"># results = ray.get([execute_objective.remote(parallel_args_id, cv_samples, parameter_lst[i], i) for i in range(num_processes)])</span>
                    <span class="c1"># else:</span>
                    <span class="n">gd_weights</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">gd_biases</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">ydots</span> <span class="o">=</span><span class="p">[]</span>
                    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">Ls</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">init_conds_clone</span> <span class="o">=</span> <span class="n">init_conditions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">SOLVE</span><span class="p">:</span>
                        <span class="n">orig_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="c1">#.clone()</span>
                        <span class="n">orig_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="c1">#.clone()</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">parallel_backprop</span> <span class="o">=</span> <span class="kc">True</span>

                    <span class="n">weight_dicts</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_backprop</span><span class="p">:</span>
                        <span class="n">new_out_W</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span>

                        <span class="k">if</span> <span class="n">SOLVE</span><span class="p">:</span>
                            <span class="n">new_out_W</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                            <span class="n">new_out_W</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">try</span><span class="p">:</span>

                                <span class="n">new_out_W</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">orig_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                                <span class="n">new_out_W</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">orig_bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                            <span class="k">except</span><span class="p">:</span>
                                <span class="n">new_out_W</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">orig_weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                                <span class="n">new_out_W</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">orig_bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>



                        <span class="n">data2save</span> <span class="o">=</span> <span class="p">{</span><span class="c1">#&quot;rc&quot; : self, </span>
                                     <span class="s2">&quot;custom_loss&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_criterion</span><span class="p">,</span> 
                                     <span class="s2">&quot;epochs&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                                     <span class="s2">&quot;New_X&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
                                     <span class="s2">&quot;states_dot&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span>
                                     <span class="c1">#&quot;orig_bias&quot; : orig_bias,</span>
                                     <span class="c1">#&quot;orig_weights&quot; : orig_weights,</span>
                                     <span class="s2">&quot;out_W&quot;</span> <span class="p">:</span> <span class="n">new_out_W</span><span class="p">,</span>
                                     <span class="s2">&quot;force_t&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span><span class="p">,</span>
                                     <span class="s2">&quot;criterion&quot;</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
                                     <span class="c1">#&quot;optimizer&quot; : optim.Adam(      self.parameters(), lr = 0.05),</span>
                                     <span class="s2">&quot;t&quot;</span>  <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span>
                                     <span class="s2">&quot;G&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span>
                                     <span class="s2">&quot;gamma&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
                                     <span class="s2">&quot;gamma_cyclic&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_cyclic</span><span class="p">,</span>
                                     <span class="c1">#&quot;parameters&quot; : self.parameters(),</span>
                                     <span class="s2">&quot;spikethreshold&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">spikethreshold</span><span class="p">,</span>
                                     <span class="s2">&quot;ode_coefs&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ode_coefs</span><span class="p">,</span>
                                     <span class="s2">&quot;enet_alpha&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">,</span>
                                     <span class="s2">&quot;enet_strength&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span><span class="p">,</span>
                                     <span class="s2">&quot;init_conds&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span>
                                     <span class="p">}</span>

                        <span class="n">self_id</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">data2save</span><span class="p">)</span>

                        <span class="n">weight_dicts</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="n">execute_backprop</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">self_id</span><span class="p">,</span> <span class="n">y0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y0</span> <span class="ow">in</span> <span class="n">init_conds_clone</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
                    
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">init_conds_clone</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                            <span class="c1">#print(&quot;w&quot;, i)</span>
                            <span class="k">if</span> <span class="n">SOLVE</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">orig_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">orig_bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">y0</span><span class="p">)</span>
                            <span class="c1">#print(self.init_conds[0])</span>
                            <span class="c1">#breakpoint()</span>
                            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                                <span class="n">weight_dict</span> <span class="o">=</span> <span class="n">backprop_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">force_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span><span class="p">,</span> <span class="n">custom_loss</span> <span class="o">=</span> <span class="n">ODE_criterion</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">)</span>
                            <span class="n">weight_dicts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight_dict</span><span class="p">)</span>
                    <span class="n">last_outputs</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">weight_dict</span> <span class="ow">in</span> <span class="n">weight_dicts</span><span class="p">:</span>
                        <span class="n">score</span><span class="o">=</span><span class="n">weight_dict</span><span class="p">[</span><span class="s2">&quot;best_score&quot;</span><span class="p">]</span>
                        <span class="n">y</span> <span class="o">=</span> <span class="n">weight_dict</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
                        <span class="n">ydot</span> <span class="o">=</span> <span class="n">weight_dict</span><span class="p">[</span><span class="s2">&quot;ydot&quot;</span><span class="p">]</span>
                        <span class="n">loss</span><span class="p">,</span> <span class="n">gd_weight</span><span class="p">,</span> <span class="n">gd_bias</span> <span class="o">=</span> <span class="n">weight_dict</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="s2">&quot;loss_history&quot;</span><span class="p">],</span> <span class="n">weight_dict</span><span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">],</span>  <span class="n">weight_dict</span><span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">]</span>
                        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
                        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                        <span class="n">ydots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ydot</span><span class="p">)</span>
                        <span class="n">gd_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gd_weight</span><span class="p">)</span>
                        <span class="n">gd_biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gd_bias</span><span class="p">)</span>
                        <span class="n">Ls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                        <span class="n">last_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_conditions</span>


                    <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span> <span class="o">=</span> <span class="n">gd_weights</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span> <span class="o">=</span> <span class="n">gd_biases</span>
                    <span class="k">if</span> <span class="n">train_score</span><span class="p">:</span>
                        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;scores&quot;</span> <span class="p">:</span> <span class="n">scores</span><span class="p">,</span> 
                                <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="n">gd_weights</span><span class="p">,</span> 
                                <span class="s2">&quot;biases&quot;</span> <span class="p">:</span> <span class="n">gd_biases</span><span class="p">,</span>
                                <span class="s2">&quot;ys&quot;</span>     <span class="p">:</span> <span class="n">ys</span><span class="p">,</span>
                                <span class="s2">&quot;ydots&quot;</span>  <span class="p">:</span> <span class="n">ydots</span><span class="p">,</span>
                                <span class="s2">&quot;losses&quot;</span> <span class="p">:</span> <span class="n">Ls</span><span class="p">}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">ys</span><span class="p">,</span> <span class="n">ydots</span>

                        <span class="c1">#{&quot;weights&quot;: best_weight, &quot;bias&quot; : best_bias, &quot;loss&quot; : backprop_args, &quot;ydot&quot; : ydot, &quot;y&quot; : y}</span>


            <span class="k">else</span><span class="p">:</span>

                <span class="c1"># self.biases_list = [bias]</span>
                <span class="c1"># self.weights_list = [weight]</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extended_states</span><span class="p">)</span>
                
                <span class="c1"># Store last y value as starting value for predictions</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1">#calc Ndot just uses the weights</span>
                    <span class="c1">#self.states_dot @ self.LinOut.weight</span>
                    <span class="c1"># if not SOLVE:</span>
                    <span class="c1">#     ones_row = torch.ones( X.shape[0], 1, **self.dev)</span>
                    <span class="c1">#     #print(f&#39;adjusting extended_states, shape before: {self.extended_states.shape}&#39;)</span>
                    <span class="c1">#     self.extended_states = torch.hstack((ones_row, self.extended_states))</span>
                    <span class="c1">#     #print(f&#39;adjusting extended_states, shape after: {self.extended_states.shape}&#39;)</span>
                        
                    <span class="c1">#     zeros_row = torch.zeros( X.shape[0], 1, **self.dev)</span>
                    <span class="c1">#     self.states_dot = torch.hstack((zeros_row, self.states_dot))</span>
                    
                    <span class="n">N_dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_Ndot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_dot</span><span class="p">,</span> <span class="n">cutoff</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                    <span class="c1">#1st order</span>
                    <span class="c1"># self.ydot = g_dot * self.N +  g * N_dot</span>
                    <span class="c1">#2nd order</span>
                    <span class="c1">#if self.ODE_order == 1:</span>
                        
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">eq_system</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span> <span class="o">=</span> <span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">g</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">+</span>  <span class="n">g</span> <span class="o">*</span> <span class="n">N_dot</span>
                    <span class="k">else</span><span class="p">:</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span> <span class="o">=</span> <span class="n">g</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                        
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cond</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">):</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">cond</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">+</span>  <span class="n">g</span> <span class="o">*</span> <span class="n">N_dot</span>

                    <span class="k">return</span> <span class="p">{</span><span class="c1">#&quot;scores&quot; : scores, </span>
                                <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                <span class="s2">&quot;bias&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                <span class="s2">&quot;y&quot;</span>     <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">,</span>
                                <span class="s2">&quot;ydot&quot;</span>  <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ydot</span><span class="p">}</span>

                    <span class="k">if</span> <span class="n">train_score</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">ODE_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ydot</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">,</span> <span class="n">init_conds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">,</span> 
                                                <span class="n">enet_strength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span><span class="p">,</span> <span class="n">enet_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">,</span>
                                                <span class="n">force_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_t</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ydot</span></div>

            <span class="c1"># if self.ODE_order &gt;= 2:</span>
            <span class="c1">#     v0 = self.init_conds[1]</span>
            <span class="c1">#     self.ydot =  g_dot*(v0+2*g*self.N) + g**2*N_dot </span>

            <span class="c1">#     #self.ydot2 = gH_dot2[:,1:] @ self.LinOut.weight</span>
            <span class="c1">#     N_dot2 = self.states_dot2 @ self.LinOut.weight.T </span>
            <span class="c1">#     term2_1 = 4*g*g_dot*N_dot</span>
            <span class="c1">#     term2_2 = v0*g_dot2 </span>
            <span class="c1">#     term2_3 = 2*self.N*(g_dot**2 + g*g_dot2)</span>
            <span class="c1">#     term2_4 = g**2*N_dot2</span>
            <span class="c1">#     self.ydot2 = term2_1 +term2_2 + term2_3 + term2_4</span>
            <span class="c1">#     self.yfit = init_conds[0] + init_conds[1] * g + g.pow(self.ODE_order) * self.N</span>
            <span class="c1">#     self.lastoutput = self.yfit[-1, :]</span>
            <span class="c1">#     if train_score:</span>
            <span class="c1">#         return ODE_criterion(X= X, </span>
            <span class="c1">#                              y = self.yfit.data, </span>
            <span class="c1">#                              ydot = self.ydot.data,</span>
            <span class="c1">#                              ydot2 = self.ydot2.data, </span>
            <span class="c1">#                              out_weights = self.LinOut.weight.data, </span>
            <span class="c1">#                              ode_coefs = ode_coefs,</span>
            <span class="c1">#                              init_conds = self.init_conds,</span>
            <span class="c1">#                              enet_strength = self.enet_strength, </span>
            <span class="c1">#                              enet_alpha = self.enet_alpha)</span>
            <span class="c1">#     return self.yfit, self.ydot, self.ydot2</span>
            

            <span class="c1"># if not ODE_order and burn_in:</span>
            <span class="c1">#     self.N = self.N[self.burn_in:,:]</span>
            <span class="c1">#     self.N = self.N.view(-1, self.n_outputs)</span>
            <span class="c1">#     self.X = self.X[self.burn_in:]</span>
            
            <span class="c1"># # Return all data for computation or visualization purposes (Note: these are normalized)</span>
            <span class="c1"># if return_states:</span>
            <span class="c1">#     return extended_states, (y[1:,:] if self.feedback else y), burn_in</span>
            <span class="c1"># else:</span>
            <span class="c1">#     self.yfit = self.LinOut(self.extended_states)</span>
            <span class="c1">#     if SCALE:   </span>
            <span class="c1">#         self.yfit = self._output_stds * self.yfit + self._output_means</span>
            <span class="c1">#     return self.yfit</span>


    <span class="c1"># def calculate_n_grads(self, X, y,  n = 2, scale = False):</span>
    <span class="c1">#     self.grads = []</span>

    <span class="c1">#     #X = X.reshape(-1, self.n_inputs)</span>

    <span class="c1">#     assert y.requires_grad, &quot;entended doesn&#39;t require grad, but you want to track_in_grad&quot;</span>
    <span class="c1">#     for i in range(n):</span>
    <span class="c1">#         print(&#39;calculating derivative&#39;, i+1)</span>
    <span class="c1">#         if not i:</span>
    <span class="c1">#             grad = _dfx(X, y)</span>
    <span class="c1">#         else:</span>
    <span class="c1">#             grad = _dfx(X, self.grads[i-1])</span>

    <span class="c1">#         self.grads.append(grad)</span>

    <span class="c1">#         if scale:</span>
    <span class="c1">#             self.grads[i] = self.grads[i]/(self._input_stds)</span>
    <span class="c1">#     with torch.no_grad():</span>
    <span class="c1">#         self.grads = [self.grads[i][self.burn_in:] for i in range(n)]</span>
                
    <span class="c1">#         #self.yfit = self.yfit[self.burn_in:]</span>
    <span class="c1">#     #assert extended_states.requires_grad, &quot;entended doesn&#39;t require grad, but you want to track_in_grad&quot;</span>
    
    <span class="k">def</span> <span class="nf">_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Normalizes array by column (along rows) and stores mean and standard devation.</span>

<span class="sd">        Set `store` to True if you want to retain means and stds for denormalization later.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            :meth:`_scale` is an internal method. As is standard, methods not designed for the user</span>
<span class="sd">            begin with the `_` character.</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : array or None</span>
<span class="sd">            Input matrix that is to be normalized</span>
<span class="sd">        outputs : array or No ne </span>
<span class="sd">        no_grads            Output column vector that is to be normalized</span>
<span class="sd">        keep : bool</span>
<span class="sd">            Stores the normalization transformation in the object to denormalize later</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        transformed : tuple or array</span>
<span class="sd">            Returns tuple of every normalized array. In case only one object is to be returned the tuple will be</span>
<span class="sd">            unpacked before returning</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh_bound_limit</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1">#.9</span>
        <span class="c1"># Checks</span>
        <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Inputs and outputs cannot both be None&#39;</span><span class="p">)</span>

        <span class="c1"># Storage for transformed variables</span>
        <span class="n">transformed</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
                    <span class="c1"># Store for denormalization</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_input_mins</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_input_ranges</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_mins</span> 

                <span class="c1"># Transform</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="c1">#normalize to between -0.5 and 0.5 for echostate purposes</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="o">==</span> <span class="s2">&quot;sin&quot;</span><span class="p">:</span>
                        <span class="n">normalized</span> <span class="o">=</span> <span class="p">(((</span><span class="n">inputs</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_mins</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_ranges</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">normalized</span> <span class="o">=</span> <span class="p">(((</span><span class="n">inputs</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_mins</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_ranges</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">tanh_bound_limit</span>

                    <span class="n">preped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f_inv</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>

                    <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">preped</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="n">keep</span><span class="p">))</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;normalization not implimented for ODEs&quot;</span>
                    <span class="c1">#transformed.append( inputs / self._input_stds)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
                    <span class="c1"># Store for destandardization</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_input_means</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_input_stds</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Transform</span>
                <span class="c1">#if not self.ODE_order:</span>
                <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">inputs</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_means</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_stds</span><span class="p">)</span>
                <span class="c1">#else: </span>
                <span class="c1">#    transformed.append( inputs / self._input_stds)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
                    <span class="c1"># Store for denormalization</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_output_mins</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_output_ranges</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_mins</span>

                <span class="c1"># Transform</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="c1">#normalize to between -0.5 and 0.5 for echostate purposes</span>
                    <span class="n">normalized</span> <span class="o">=</span> <span class="p">(((</span><span class="n">outputs</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_mins</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_ranges</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">tanh_bound_limit</span>

                    <span class="n">preped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f_inv</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>

                    <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">preped</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="n">keep</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span> 
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;normalization not implimented for ODEs&quot;</span>
                    <span class="c1">#transformed.append( inputs / self._input_stds)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
                    <span class="c1"># Store for denormalization</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_output_means</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_output_stds</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="c1">#, ddof=1)</span>

                <span class="c1"># Transform</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">outputs</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_means</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_stds</span><span class="p">)</span>
                
                <span class="bp">self</span><span class="o">.</span><span class="n">_output_means</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_means</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_output_stds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_stds</span>
        <span class="c1"># Syntactic sugar</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">transformed</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformed</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<div class="viewcode-block" id="RcNetwork.error"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.error">[docs]</a>    <span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;nmse&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the error between predictions and target values.</span>

<span class="sd">        Suggested values of alpha (see the parameter description below for an explanation):</span>

<span class="sd">        .. list-table:: n vs alpha values</span>
<span class="sd">                :widths: 25 25</span>
<span class="sd">                :header-rows: 1</span>

<span class="sd">                * - n</span>
<span class="sd">                  - alpha</span>
<span class="sd">                * - 1</span>
<span class="sd">                  - 1.6</span>
<span class="sd">                * - 2</span>
<span class="sd">                  - 2.8</span>
<span class="sd">                * - 3</span>
<span class="sd">                  - 4.0</span>
<span class="sd">                * - 4</span>
<span class="sd">                  - 5.2</span>
<span class="sd">                * - 5</span>
<span class="sd">                  - 6.4</span>
<span class="sd">                * - 6</span>
<span class="sd">                  - 7.6</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        predicted : array</span>
<span class="sd">            Predicted value</span>
<span class="sd">        target : array</span>
<span class="sd">            Target values</span>
<span class="sd">        method : {&#39;mse&#39;, &#39;tanh&#39;, &#39;rmse&#39;, &#39;nmse&#39;, &#39;nrmse&#39;, &#39;tanh-nmse&#39;, &#39;log-tanh&#39;, &#39;log&#39;}</span>
<span class="sd">            Evaluation metric. &#39;tanh&#39; takes the hyperbolic tangent of mse to bound its domain to [0, 1] to ensure</span>
<span class="sd">            continuity for unstable models. &#39;log&#39; takes the logged mse, and &#39;log-tanh&#39; takes the log of the squeezed</span>
<span class="sd">            normalized mse. The log ensures that any variance in the GP stays within bounds as errors go toward 0.</span>
<span class="sd">        alpha : float</span>
<span class="sd">            Alpha coefficient to scale the tanh error transformation: ``alpha * tanh{(1 / alpha) * error}``.</span>
<span class="sd">            This squeezes errors onto the interval [0, alpha].</span>
<span class="sd">            Default is 1. Suggestions for squeezing errors &gt; n * stddev of the original series</span>
<span class="sd">            (for tanh-nrmse, this is the point after which difference with y = x is larger than 50%,</span>
<span class="sd">            and squeezing kicks in). suggested n, alpha value pairs:</span>
<span class="sd">            </span>
<span class="sd">            </span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        error : float</span>
<span class="sd">            The error as evaluated with the metric chosen above</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="n">predicted</span> <span class="o">-</span> <span class="n">target</span>

        <span class="c1"># Adjust for NaN and np.inf in predictions (unstable solution)</span>
        <span class="c1">#if not torch.all(torch.isfinite(predicted)):</span>
        <span class="c1">#    # print(&quot;Warning: some predicted values are not finite&quot;)</span>
        <span class="c1">#    errors = torch.inf</span>
        
        <span class="k">def</span> <span class="nf">nmse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            normalized mean square error</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span><span class="p">))))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
            
        <span class="c1">#### attempt at loss function when steps ahead &gt; 2 </span>

        <span class="c1"># def step_ahead_loss(y, yhat, plot = False, decay = 0.9):</span>
        <span class="c1">#     loss = torch.zeros(1,1, device = self.device)</span>
        <span class="c1">#     losses = []</span>
        <span class="c1">#     total_length = len(y)</span>
        <span class="c1">#     for i in range(1, total_length - self.steps_ahead):</span>
        <span class="c1">#         #step ahead == i subsequences</span>
        <span class="c1">#         #columnwise</span>
        <span class="c1">#         #   yhat_sub = yhat[:(total_length - i), i - 1]</span>
        <span class="c1">#         #   y_sub = y[i:(total_length),0]</span>
        <span class="c1">#         #row-wise</span>
        <span class="c1">#         yhat_sub = yhat[i-1, :]</span>
        <span class="c1">#         y_sub = y[i:(self.steps_ahead + i),0]</span>
        <span class="c1">#         assert(len(yhat_sub) == len(y_sub)), &quot;yhat: {}, y: {}&quot;.format(yhat_sub.shape, y_sub.shape)</span>

        <span class="c1">#         loss_ = nmse(y_sub.squeeze(), yhat_sub.squeeze())</span>

        <span class="c1">#         if decay:</span>
        <span class="c1">#             loss_ *= (decay ** i)</span>

        <span class="c1">#         #if i &gt; self.burn_in:</span>
        <span class="c1">#         loss += loss_</span>
        <span class="c1">#         losses.append(loss_)</span>

        <span class="c1">#     if plot:</span>
        <span class="c1">#         plt.plot(range(1, len(losses) + 1), losses)</span>
        <span class="c1">#         plt.title(&quot;loss vs step ahead&quot;)</span>
        <span class="c1">#         plt.xlabel(&quot;steps ahead&quot;)</span>
        <span class="c1">#         plt.ylabel(&quot;avg loss&quot;)</span>
        <span class="c1">#     return loss.squeeze()</span>

        <span class="c1"># if predicted.shape[1] != 1:</span>
        <span class="c1">#     return step_ahead_loss(y = target, yhat = predicted) </span>

        <span class="c1"># Compute mean error</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">method</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="s2">&quot;custom&quot;</span><span class="p">):</span>
            <span class="c1">#assert self.custom_criterion, &quot;You need to input the argument `custom criterion` with a proper torch loss function that takes `predicted` and `target` as input&quot;</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">method</span><span class="p">(</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">predicted</span><span class="p">)</span>

            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            try:</span>
<span class="sd">                error = </span>
<span class="sd">            except:</span>
<span class="sd">                if type(method) == type(&quot;custom&quot;):</span>
<span class="sd">                    pass</span>
<span class="sd">                else:</span>
<span class="sd">                assert False, &quot;bad scoring method, please enter a string or input a valid custom loss function&quot;</span>
<span class="sd">            &quot;&quot;&quot;</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;mse&#39;</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;combined&quot;</span><span class="p">:</span>
            <span class="n">nmse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>

            <span class="n">kl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">)(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> 
                <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">))))</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">nmse</span> <span class="o">+</span> <span class="n">kl</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;nmse&#39;</span><span class="p">,</span> <span class="n">nmse</span><span class="p">,</span> <span class="s1">&#39;kl&#39;</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="s1">&#39;combined&#39;</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;trivial_penalty&quot;</span><span class="p">:</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">predicted</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">+</span> <span class="n">penalty</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">mse</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;penalty&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;combined&#39;</span><span class="p">,</span> <span class="n">error</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;smoothing_penalty&quot;</span><span class="p">:</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dydx2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">penalty</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">nmse</span><span class="p">,</span> <span class="s1">&#39;penalty&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="s1">&#39;combined&#39;</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;combined_penalties&quot;</span><span class="p">:</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
            <span class="c1">#we should include hyper-parameters here.</span>
            <span class="n">dxpenalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dydx2</span><span class="p">))</span>
            <span class="n">dxpenalty_is_positive</span> <span class="o">=</span> <span class="p">(</span><span class="n">dxpenalty</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>
            <span class="n">dxpenalty</span> <span class="o">=</span> <span class="n">dxpenalty</span> <span class="o">*</span> <span class="n">dxpenalty_is_positive</span>
            <span class="n">dxpenalty</span> <span class="o">=</span> <span class="n">dxpenalty</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">nullpenalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">predicted</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">+</span> <span class="n">dxpenalty</span> <span class="o">+</span> <span class="n">nullpenalty</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">mse</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;dydx^2_penalty&#39;</span><span class="p">,</span> <span class="n">dxpenalty</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;penalty2&quot;</span><span class="p">,</span> <span class="n">nullpenalty</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;combined&#39;</span><span class="p">,</span> <span class="n">error</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;tanh&#39;</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>  <span class="c1"># To &#39;squeeze&#39; errors onto the interval (0, 1)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;rmse&#39;</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)))</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;nmse&#39;</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">std</span><span class="p">())</span><span class="c1">#ddof=1))</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;nrmse&#39;</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)))</span> <span class="o">/</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="c1">#ddof=1)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;tanh-nrmse&#39;</span><span class="p">:</span>
            <span class="n">nrmse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)))</span> <span class="o">/</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">nrmse</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;log&#39;</span><span class="p">:</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;log-tanh&#39;</span><span class="p">:</span>
            <span class="n">nrmse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)))</span> <span class="o">/</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">((</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">nrmse</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Scoring method not recognized&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span><span class="c1">#.type(self.dtype)</span></div>
    

    <span class="c1"># def back(self, tensor_spec, retain_graph = True):</span>
    <span class="c1">#     return tensor_spec.backward(torch.ones(*tensor_spec.shape, device = tensor_spec.device), retain_graph = retain_graph)</span>

<div class="viewcode-block" id="RcNetwork.test"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.test">[docs]</a>    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">steps_ahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scoring_method</span><span class="o">=</span><span class="s1">&#39;nmse&#39;</span><span class="p">,</span> 
                  <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reparam</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">ODE_criterion</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span> <span class="c1"># beta = None</span>
        <span class="sd">&quot;&quot;&quot;Tests and scores against known output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : array</span>
<span class="sd">            Column vector of known outputs</span>
<span class="sd">        x : array or None</span>
<span class="sd">            Any inputs if required</span>
<span class="sd">        y_start : float or None</span>
<span class="sd">            Starting value from which to start testing. If None, last stored value from trainging will be used</span>
<span class="sd">        steps_ahead : int or None</span>
<span class="sd">            Computes average error on n steps ahead prediction. If `None` all steps in y will be used.</span>
<span class="sd">        scoring_method : {&#39;mse&#39;, &#39;rmse&#39;, &#39;nrmse&#39;, &#39;tanh&#39;}</span>
<span class="sd">            Evaluation metric used to calculate error</span>
<span class="sd">        alpha : float</span>
<span class="sd">            Alpha coefficient to scale the tanh error transformation: alpha * tanh{(1 / alpha) * error}</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        error : float</span>
<span class="sd">            Error between prediction and knwon outputs</span>

<span class="sd">        &quot;&quot;&quot;</span> 

        <span class="c1">########################## betas are currently silenced ################</span>
        <span class="c1">#self.beta = beta</span>
        <span class="c1">########################## betas are currently silenced ################</span>


        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>

            <span class="n">y</span> <span class="o">=</span> <span class="n">_check_y</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tensor_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">_check_x</span><span class="p">(</span><span class="n">X</span> <span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">,</span> <span class="n">supervised</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">final_t</span> <span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">steps_ahead</span> <span class="o">=</span> <span class="n">steps_ahead</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaking_rate_orig</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assign_const_leaking_rate</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            
            
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
                

                <span class="n">start</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">nsteps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">stop</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span><span class="p">)</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">nsteps</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="c1">#.to(self.device)</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">([])</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">x0</span><span class="p">,</span> <span class="n">xf</span><span class="p">,</span> <span class="n">nsteps</span> <span class="o">=</span> <span class="n">X</span> <span class="c1">#6*np.pi, 100</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">xf</span><span class="p">,</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">nsteps</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="c1">#.to(self.device)</span>
            <span class="n">final_t</span> <span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">_convert_ode_coefs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ode_coefs</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

        <span class="n">X</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">==</span> <span class="kc">False</span><span class="c1">#self.track_in_grad</span>
        
        <span class="c1"># Run prediction</span>

        <span class="k">if</span> <span class="n">steps_ahead</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y_start</span><span class="o">=</span><span class="n">y_start</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
                
                <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="n">scoring_method</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="o">.</span><span class="n">data</span> <span class="c1">#{&quot;yhat&quot;: y_predicted.data, &quot;ytest&quot;: y}, X[self.burn_in:]</span>
                <span class="k">else</span><span class="p">:</span>
                    
                    <span class="n">score</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="k">return</span> <span class="n">score</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">val_force_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">force</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">scale</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span><span class="p">:</span>

                    <span class="n">returns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y_start</span><span class="o">=</span><span class="n">y_start</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span>
                                   <span class="n">continue_force</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">y_predicted</span><span class="p">,</span> <span class="n">ydot</span> <span class="o">=</span> <span class="n">returns</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="n">y_predicted</span><span class="p">,</span> <span class="n">ydot</span><span class="p">,</span> <span class="n">ydot2</span> <span class="o">=</span> <span class="n">returns</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_predicted</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">ydot</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">,</span> 
                                                <span class="n">init_conds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">,</span>
                                                <span class="n">force_t</span> <span class="o">=</span> <span class="n">val_force_t</span><span class="p">,</span>
                                                <span class="n">enet_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">,</span> 
                                                <span class="n">enet_strength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span><span class="p">)</span> 
                    <span class="c1"># elif self.ODE_order == 2:</span>
                    <span class="c1">#     score = ODE_criterion(X, y_predicted.data, ydot.data, ydot2.data, self.LinOut.weight.data, ode_coefs = ode_coefs, init_conds = self.init_conds,</span>
                    <span class="c1">#                           enet_alpha = self.enet_alpha, enet_strength = self.enet_strength) #error(predicted = y_predicted, target = dy_dx_val, method = scoring_method, alpha=alpha)</span>
                    <span class="c1"># # else:</span>
                    <span class="c1"># #     assert False</span>
                    <span class="c1"># #     score = self.error(predicted = y_predicted, target = y, method = scoring_method, alpha=alpha)</span>
                    <span class="c1"># Return error</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span> <span class="o">=</span> <span class="n">y_predicted</span>
                    <span class="c1">#print(&quot;score&quot;, score.detach())</span>
                    <span class="k">return</span> <span class="n">score</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y_predicted</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">y_preds</span><span class="p">,</span> <span class="n">ydots</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y_start</span><span class="o">=</span><span class="n">y_start</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span>
                                   <span class="n">continue_force</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
                    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_preds</span><span class="p">):</span>
                        <span class="n">ydot</span> <span class="o">=</span> <span class="n">ydots</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">eq_system</span><span class="p">:</span>
                            
                            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pred</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">ydot</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                  <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">,</span> 
                                                  <span class="n">init_conds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">,</span>
                                                  <span class="n">enet_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">,</span> 
                                                  <span class="n">enet_strength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span><span class="p">,</span>
                                                  <span class="n">force_t</span> <span class="o">=</span> <span class="n">val_force_t</span><span class="p">)</span>
                            
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">init_conds_system</span> <span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                            <span class="n">y0</span><span class="p">,</span> <span class="n">p0</span> <span class="o">=</span> <span class="n">init_conds_system</span>
                            <span class="c1">#ham0 = </span>
                            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pred</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">ydot</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                  <span class="n">ode_coefs</span> <span class="o">=</span> <span class="n">ode_coefs</span><span class="p">,</span> 
                                                  <span class="n">init_conds</span> <span class="o">=</span> <span class="n">init_conds_system</span><span class="p">,</span>
                                                  <span class="n">enet_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_alpha</span><span class="p">,</span> 
                                                  <span class="n">enet_strength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enet_strength</span><span class="p">,</span>
                                                  <span class="c1">#ham0 = (1/2)*p0**2 - 3*y0**2 + (21/4)*y0**4,</span>
                                                  <span class="n">force_t</span> <span class="o">=</span> <span class="n">val_force_t</span><span class="p">)</span>
                        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
                        <span class="c1">#print(&quot;score&quot;, score.detach())</span>
                    
                    <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_</span>



                <span class="c1">#y_predicted, ydot = self.reparam(self.y0, X, N, N_dot)</span>
            <span class="c1">#_printc(&quot;predicting &quot;  + str(y.shape[0]) + &quot;steps&quot;, &#39;blue&#39;)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;predict_stepwise not implimented&#39;</span>
            <span class="n">y_predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_stepwise</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">steps_ahead</span><span class="o">=</span><span class="n">steps_ahead</span><span class="p">,</span> <span class="n">y_start</span><span class="o">=</span><span class="n">y_start</span><span class="p">)[:</span><span class="n">final_t</span><span class="p">,:]</span></div>

    <span class="k">def</span> <span class="nf">_supervised_val_states</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>

            <span class="c1">#assert False, f&#39;inputs[t+1, :] :, {inputs[t+1, :]}&#39;</span>
            <span class="n">input_t</span><span class="p">,</span> <span class="n">state_t</span><span class="p">,</span> <span class="n">output_prev</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">states</span><span class="p">[</span><span class="n">t</span><span class="p">,:],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">t</span><span class="p">,:]</span>

            <span class="n">state_t</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">input_t</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">state_t</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">output_prev</span><span class="p">)</span>

            <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">state_t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

            <span class="n">output_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_i</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="n">state_t</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_t</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">_unsupervised_val_states</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summary line.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        arg1 : int</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        arg2 : str</span>
<span class="sd">            Description of arg2</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Description of return value</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>

            <span class="n">input_t</span><span class="p">,</span> <span class="n">state_t</span><span class="p">,</span> <span class="n">output_prev</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">states</span><span class="p">[</span><span class="n">t</span><span class="p">,:],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">t</span><span class="p">,:]</span>

            <span class="n">state_t</span><span class="p">,</span> <span class="n">output_t</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_state</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">input_t</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">state_t</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">output_prev</span><span class="p">)</span>
            <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">states</span><span class="p">,</span> <span class="n">state_t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">)))</span>

            <span class="n">output_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_i</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="n">state_t</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_t</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span>

<div class="viewcode-block" id="RcNetwork.predict"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">continuation</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">continue_force</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predicts n values in advance.</span>

<span class="sd">        Prediction starts from the last state generated in training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_steps : int</span>
<span class="sd">            The number of steps to predict into the future (internally done in one step increments)</span>
<span class="sd">        x : numpy array or None</span>
<span class="sd">            If prediciton requires inputs, provide them here</span>
<span class="sd">        y_start : float or None</span>
<span class="sd">            Starting value from which to start prediction. If None, last stored value dfrom training will be used</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_predicted : numpy array</span>
<span class="sd">            Array of n_step predictions</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if ESN has been trained</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Error: ESN not trained yet&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_Xte</span> <span class="o">=</span> <span class="n">X</span>

        <span class="c1"># Normalize the inputs (like was done in train)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">X</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">scale</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_X</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">X_val</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unscaled_Xte</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">X_val</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y_start</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">continuation</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">noise_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dev</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>

        <span class="c1"># try:</span>
        <span class="c1">#     assert self.X_val.device == self.device, &quot;&quot;</span>
            
        <span class="c1"># except:</span>
        <span class="c1">#     self.X_val.data = self.X_val.to(self.device)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X_val_extended</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">X_val</span>
        <span class="c1">#assert False, f&#39;X mean {self.X_val.mean()} std {self.X_val.std()}&#39;</span>
        <span class="c1"># if not continue_force:</span>
        <span class="c1">#     if self.ODE_order:</span>
        <span class="c1">#         continuation = False</span>
        <span class="n">dev</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;requires_grad&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_val_extended</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># if y_start: #if not x is None:</span>
        <span class="c1">#     if scale:</span>
        <span class="c1">#         previous_y = self.scale(outputs=y_start)[0]</span>
        <span class="c1">#     else:</span>
        <span class="c1">#         previous_y = y_start[0]</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">lastinput</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="n">continuation</span><span class="p">:</span>
            <span class="c1">#if self.ODE_order &gt;=2:</span>
            <span class="c1">#    lasthdot2 = self.lasthdot2</span>
            <span class="c1">#lasthdot = self.lasthdot</span>
            <span class="n">laststate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">laststate</span>
            <span class="n">lastinput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lastinput</span>
            <span class="n">lastoutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lastoutput</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#if self.ODE_order &gt;=2:</span>
            <span class="c1">#    lasthdot2 = torch.zeros(self.n_nodes, **dev)</span>
            <span class="c1">#lasthdot = torch.zeros(self.n_nodes, **dev)</span>
            <span class="n">laststate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="o">**</span><span class="n">dev</span><span class="p">)</span>
            <span class="n">lastinput</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">dev</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_inputs</span><span class="p">)</span>
            <span class="n">lastoutput</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">**</span><span class="n">dev</span><span class="p">)</span>
        <span class="c1">#if self.ODE_order:</span>
        <span class="c1">#    lastoutput = torch.zeros(self.n_outputs, **dev)</span>


        <span class="k">if</span> <span class="ow">not</span> <span class="n">y_start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="n">lastoutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">y_start</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span><span class="p">)</span><span class="c1"># self.scale(inputs=X)</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">lastinput</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_val_extended</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_val_extended</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">),</span> <span class="o">**</span><span class="n">dev</span><span class="p">)</span>
        <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">laststate</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">lastoutput</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span>

        <span class="c1">#assert False, f&quot;last output {lastoutput}, ystart {y_start}&quot;</span>
        

        <span class="c1">#dt = inputs[1,:] - inputs[0,:]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                <span class="n">states_dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">),</span> <span class="o">**</span><span class="n">dev</span><span class="p">)</span>
                <span class="c1">#states, outputs = self.unsupervised_val_states(n_samples, inputs, states, outputs)</span>
                <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unsupervised_val_states</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#states = torch.vstack((states, states))</span>
                <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supervised_val_states</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

            <span class="c1">#drop first state and first output (final train-set datapoints which were already used)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">val_states</span> <span class="o">=</span> <span class="n">states</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

            <span class="c1">#if self.burn_in:</span>
            <span class="c1">#    states = states[self.burn_in:]</span>
            <span class="c1">#    outputs = outputs[self.burn_in:]</span>

            

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                <span class="c1"># try:</span>
                <span class="c1">#     if scale:</span>
                <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_descale</span><span class="p">(</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span> 
                <span class="c1"># except:</span>
                <span class="c1">#     yhat = outputs</span>
                <span class="k">return</span> <span class="n">yhat</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># calculate hidden state derivatives</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span><span class="p">:</span>
                    <span class="n">updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="p">,</span> <span class="n">outputs</span><span class="p">))</span>
                    <span class="n">updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinIn</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinRes</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>

                <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="p">,</span> <span class="n">states</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_val</span>
                <span class="n">states_dot</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">states</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span>
                <span class="c1"># if self.ODE_order == 2:</span>
                <span class="c1">#     states_dot2 = - self.alpha * states_dot + self.alpha * self.act_f_prime(updates) * (self.LinIn.weight.T + self.bias + self.LinRes(states_dot))</span>
                <span class="c1">#     states_dot2 = torch.cat((torch.zeros_like(self.X_val), states_dot2), axis = 1)</span>
                <span class="n">states_dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="p">),</span> <span class="n">states_dot</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">states</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">states_dot</span><span class="o">.</span><span class="n">shape</span>

                <span class="n">G</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparam_f</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="p">,</span> <span class="n">order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">)</span>
                <span class="n">g</span><span class="p">,</span> <span class="n">g_dot</span> <span class="o">=</span> <span class="n">G</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">val_states_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;s&quot;</span> <span class="p">:</span> <span class="n">states</span><span class="p">,</span> <span class="s2">&quot;s1&quot;</span> <span class="p">:</span> <span class="n">states_dot</span><span class="p">,</span> <span class="s2">&quot;G&quot;</span> <span class="p">:</span>  <span class="n">G</span><span class="p">}</span>


                <span class="c1"># if self.ODE_order == 2:</span>
                <span class="c1">#     #derivative of  g_dot * states_with_bias</span>
                <span class="c1">#     #gH_dot2_p1 =  g_dot * states_dot  + g_dot2 * states</span>

                <span class="c1">#     #derivative of  g * states_dot_with_bias</span>
                <span class="c1">#     #gH_dot2_p2 =  g * states_dot2  + g_dot * states_dot</span>
                    
                <span class="c1">#     #gH_dot2 = gH_dot2_p1 + gH_dot2_p2</span>

                <span class="c1">#     #ydot2 = self.LinOut(gH_dot2)</span>

                <span class="c1">#     N_dot2 = states_dot2 @ self.LinOut.weight.T #self.calc_Ndot(states_dot2, cutoff = False)</span>
                <span class="c1">#     ydot2 = 4*g*g_dot*N_dot + self.init_conds[1]*g_dot2</span>
                <span class="c1">#assert False, f&#39;o {outputs.shape} t {time.shape} dN {N_dot.shape}&#39;</span>
                <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reparam_f</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="s2">&quot;you must input a reparam function with ODE&quot;</span>
                
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span><span class="p">:</span>

                    <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">g</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">)</span>

                    <span class="c1">#the code may be useful for solving higher order:</span>
                    <span class="c1">##########################</span>
                    <span class="c1">#A = [ self.init_conds[i] * g.pow(i) for i in range(self.ODE_order)]</span>
                    <span class="c1"># for i in range(self.ODE_order):</span>
                    <span class="c1">#     A_i = self.init_conds[i] * g.pow(i) #self.ode_coefs[i] * </span>
                    <span class="c1">#     if not i:</span>
                    <span class="c1">#         A = A_i # + ode_coefs[1] * v0 * g.pow(1) + ... + ode_coefs[m] * accel_0 * g.pow(m)</span>
                    <span class="c1">#     else:</span>
                    <span class="c1">#         A = A + A_i</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">As</span> <span class="o">=</span> <span class="p">[</span><span class="n">y0</span> <span class="o">*</span> <span class="n">g</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y0</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_ICs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">eq_system</span><span class="p">:</span>
                        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                        <span class="n">N_dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_Ndot</span><span class="p">(</span><span class="n">states_dot</span><span class="p">,</span> <span class="n">cutoff</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

                        <span class="n">y</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">g</span> <span class="o">*</span> <span class="n">N</span>

                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cond</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
                            

                            <span class="n">y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

                        <span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span>  <span class="n">g</span> <span class="o">*</span> <span class="n">N_dot</span>
                        
                        <span class="c1">#self.N = self.N.view(-1, self.n_outputs)</span>
                        <span class="c1">#gH = g * states</span>
                        <span class="c1">#gH_dot =  g_dot * states  +  g * states_dot</span>

                        
                        <span class="c1">#elf.yfit = init_conds[0] + g.pow(1) * self.N</span>
                        <span class="c1">#self.ydot = g_dot * self.N +  g * N_dot</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">y</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                        <span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">N_dot</span> <span class="o">*</span> <span class="n">g</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cond</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">):</span>

                            <span class="n">y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">cond</span>
                    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">ydot</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">eq_system</span><span class="p">:</span>
                        <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="n">ydots</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span><span class="p">):</span>
                            <span class="c1">#print(&quot;w&quot;, i)</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span>

                            <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                            <span class="n">N_dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_Ndot</span><span class="p">(</span><span class="n">states_dot</span><span class="p">,</span> <span class="n">cutoff</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
                            <span class="n">yfit</span> <span class="o">=</span> <span class="n">g</span><span class="o">*</span><span class="n">N</span>


                            <span class="n">yfit</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">yfit</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                            
                            <span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span>  <span class="n">g</span> <span class="o">*</span> <span class="n">N_dot</span>

                            <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">yfit</span><span class="p">)</span>
                            <span class="n">ydots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ydot</span><span class="p">)</span>

                        <span class="k">return</span> <span class="n">ys</span><span class="p">,</span> <span class="n">ydots</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="n">ydots</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="c1">#loop</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_list</span><span class="p">):</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">))</span>

                            <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LinOut</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                            <span class="n">N_dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_Ndot</span><span class="p">(</span><span class="n">states_dot</span><span class="p">,</span> <span class="n">cutoff</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

                            <span class="c1">#reparameterize</span>
                            <span class="n">y</span> <span class="o">=</span> <span class="n">g</span><span class="o">*</span><span class="n">N</span>
                            <span class="n">ydot</span> <span class="o">=</span> <span class="n">g_dot</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">N_dot</span> <span class="o">*</span> <span class="n">g</span>

                            <span class="c1">#add initial conditions</span>
                            <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                                <span class="n">y</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_conds</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                            
                            <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                            <span class="n">ydots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ydot</span><span class="p">)</span>

                        <span class="k">return</span> <span class="n">ys</span><span class="p">,</span> <span class="n">ydots</span></div>

                
                <span class="c1"># if self.ODE_order == 2:</span>
                <span class="c1">#     y0, v0 = self.init_conds</span>
                <span class="c1">#     ydot =  g_dot*(v0+2*g*N) + g**2*N_dot </span>

                <span class="c1">#     #self.ydot2 = gH_dot2[:,1:] @ self.LinOut.weight</span>
                <span class="c1">#     N_dot2 = states_dot2 @ self.LinOut.weight.T </span>
                <span class="c1">#     term2_1 = 4*g*g_dot*N_dot</span>
                <span class="c1">#     term2_2 = v0*g_dot2 </span>
                <span class="c1">#     term2_3 = 2*N*(g_dot**2 + g*g_dot2)</span>
                <span class="c1">#     term2_4 = g**2*N_dot2</span>
                <span class="c1">#     ydot2 = term2_1 +term2_2 + term2_3 + term2_4</span>
                <span class="c1">#     y = y0 + v0 * g + g.pow(self.ODE_order) * N</span>
                <span class="c1">#     #y = A + g**2 * N</span>
                <span class="c1">#     return y, ydot, ydot2</span>
                
        <span class="c1">#https://towardsdatascience.com/in-place-operations-in-pytorch-f91d493e970e</span>



    <span class="c1"># def predict_stepwise(self, y, x=None, steps_ahead=1, y_start=None):</span>
    <span class="c1">#     &quot;&quot;&quot;Predicts a specified number of steps into the future for every time point in y-values array.</span>
    <span class="c1">#     E.g. if `steps_ahead` is 1 this produces a 1-step ahead prediction at every point in time.</span>
    <span class="c1">#     Parameters</span>
    <span class="c1">#     ----------</span>
    <span class="c1">#     y : numpy array</span>
    <span class="c1">#         Array with y-values. At every time point a prediction is made (excluding the current y)</span>
    <span class="c1">#     x : numpy array or None</span>
    <span class="c1">#         If prediciton requires inputs, provide them here</span>
    <span class="c1">#     steps_ahead : int (default 1)</span>
    <span class="c1">#         The number of steps to predict into the future at every time point</span>
    <span class="c1">#     y_start : float or None</span>
    <span class="c1">#         Starting value from which to start prediction. If None, last stored value from training will be used</span>
    <span class="c1">#     Returns</span>
    <span class="c1">#     -------</span>
    <span class="c1">#     y_predicted : numpy array</span>
    <span class="c1">#         Array of predictions at every time step of shape (times, steps_ahead)</span>
    <span class="c1">#     &quot;&quot;&quot;</span>

    <span class="c1">#     # Check if ESN has been trained</span>
    <span class="c1">#     if self.out_weights is None or self.y_last is None:</span>
    <span class="c1">#         raise ValueError(&#39;Error: ESN not trained yet&#39;)</span>

    <span class="c1">#     # Normalize the arguments (like was done in train)</span>
    <span class="c1">#     y = self.scale(outputs=y)</span>
    <span class="c1">#     if not x is None:</span>
    <span class="c1">#         x = self.scale(inputs=x)</span>

    <span class="c1">#     # Timesteps in y</span>
    <span class="c1">#     t_steps = y.shape[0]</span>

    <span class="c1">#     # Check input</span>
    <span class="c1">#     if not x is None and not x.shape[0] == t_steps:</span>
    <span class="c1">#         raise ValueError(&#39;x has the wrong size for prediction: x.shape[0] = {}, while y.shape[0] = {}&#39;.format(</span>
    <span class="c1">#             x.shape[0], t_steps))</span>

    <span class="c1">#     # Choose correct input</span>
    <span class="c1">#     if x is None and not self.feedback:</span>
    <span class="c1">#         #pass #raise ValueError(&quot;Error: cannot run without feedback and without x. Enable feedback or supply x&quot;)</span>
    <span class="c1">#         inputs = torch.ones((t_steps + steps_ahead, 2), **dev) </span>
    <span class="c1">#     elif not x is None:</span>
    <span class="c1">#         # Initialize input</span>
    <span class="c1">#         inputs = torch.ones((t_steps, 1), **dev)  # Add bias term</span>
    <span class="c1">#         inputs = torch.hstack((inputs, x))  # Add x inputs</span>
    <span class="c1">#     else:</span>
    <span class="c1">#         # x is None</span>
    <span class="c1">#         inputs = torch.ones((t_steps + steps_ahead, 1), **dev)  # Add bias term</span>
        
    <span class="c1">#     # Run until we have no further inputs</span>
    <span class="c1">#     time_length = t_steps if x is None else t_steps - steps_ahead + 1</span>

    <span class="c1">#     # Set parameters</span>
    <span class="c1">#     y_predicted = torch.zeros((time_length, steps_ahead), dtype=self.dtype, device=self.device)</span>

    <span class="c1">#     # Get last states</span>
    <span class="c1">#     previous_y = self.y_last</span>
    <span class="c1">#     if not y_start is None:</span>
    <span class="c1">#         previous_y = self.scale(outputs=y_start)[0]</span>

    <span class="c1">#     # Initialize state from last availble in train</span>
    <span class="c1">#     current_state = self.state[-1]</span>

    <span class="c1">#     # Predict iteratively</span>
    <span class="c1">#     with torch.no_grad():</span>
            
    <span class="c1">#         for t in range(time_length):</span>

    <span class="c1">#             # State_buffer for steps ahead prediction</span>
    <span class="c1">#             prediction_state = current_state.clone().detach()</span>
                
    <span class="c1">#             # Y buffer for step ahead prediction</span>
    <span class="c1">#             prediction_y = previous_y.clone().detach()</span>
            
    <span class="c1">#             # Predict stepwise at from current time step</span>
    <span class="c1">#             for n in range(steps_ahead):</span>
                    
    <span class="c1">#                 # Get correct input based on feedback setting</span>
    <span class="c1">#                 prediction_input = inputs[t + n] if not self.feedback else torch.hstack((inputs[t + n], prediction_y))</span>
                    
    <span class="c1">#                 # Update</span>
    <span class="c1">#                 prediction_update = self.activation_function(torch.matmul(self.in_weights, prediction_input.T) + </span>
    <span class="c1">#                                                torch.matmul(self.weights, prediction_state))</span>
                    
    <span class="c1">#                 prediction_state = self.leaking_rate * prediction_update + (1 - self.leaking_rate) * prediction_state</span>
                    
    <span class="c1">#                 # Store for next iteration of t (evolves true state)</span>
    <span class="c1">#                 if n == 0:</span>
    <span class="c1">#                     current_state = prediction_state.clone().detach()</span>
                    
    <span class="c1">#                 # Prediction. Order of concatenation is [1, inputs, y(n-1), state]</span>
    <span class="c1">#                 prediction_row = torch.hstack((prediction_input, prediction_state))</span>
    <span class="c1">#                 if not self.backprop:</span>
    <span class="c1">#                     y_predicted[t, n] = torch.matmul(prediction_row, self.out_weights)</span>
    <span class="c1">#                 else:</span>
    <span class="c1">#                     y_predicted[t, n] = self.LinOut.weight.T @ prediction_row[1:]</span>
    <span class="c1">#                 prediction_y = y_predicted[t, n]</span>

    <span class="c1">#             # Evolve true state</span>
    <span class="c1">#             previous_y = y[t]</span>

    <span class="c1">#     # Denormalize predictions</span>
    <span class="c1">#     y_predicted = self.descale(outputs=y_predicted)</span>
        
    <span class="c1">#     # Return predictions</span>
    <span class="c1">#     return y_predicted</span>

    

    <span class="k">def</span> <span class="nf">_plot_prep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gt_tr_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">gt_te_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        internal plot preparation function</span>

<span class="sd">        Assigns various plotting attributes to the RcNetwork class</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gt_tr_override : dtype</span>
<span class="sd">            If you are using noisy data this argument will allow you to plot the non-noisy ground truth</span>
<span class="sd">            on the training set</span>
<span class="sd">        gt_te_override : dtype</span>
<span class="sd">            If you are using noisy data this argument will allow you to plot the non-noisy ground truth</span>
<span class="sd">            on the test set</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#for noise predictions, ground truth over-ride</span>

        

        <span class="k">if</span> <span class="n">gt_tr_override</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gt_te</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_val</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gt_tr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_tr</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_override</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gt_te</span> <span class="o">=</span> <span class="n">gt_te_override</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gt_tr</span> <span class="o">=</span> <span class="n">gt_tr_override</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_override</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1">#self._gt_tr = gt_tr_override #self._gt_tr = fit_args[&quot;y&quot;]</span>

        <span class="c1"># if gt_te_override is None:</span>
        <span class="c1">#     self._gt_te = self.y_val</span>
        <span class="c1"># else:</span>
        <span class="c1">#     self._gt_te = gt_te_override</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_len_te</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_len_tr</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">te_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_len_tr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_te</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_tr</span><span class="p">))</span>
            

        <span class="c1">#save resids</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">te_resids</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gt_te</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tr_resids</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gt_tr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_xte</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_Xte</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_len_tr</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tr_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_len_tr</span><span class="p">))</span>
        

<div class="viewcode-block" id="RcNetwork.plot_prediction"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.plot_prediction">[docs]</a>    <span class="k">def</span> <span class="nf">plot_prediction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis_label_fontsize</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                              <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                              <span class="n">gt_tr_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                              <span class="n">gt_te_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                              <span class="n">lw_vert</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                              <span class="n">prep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
                              <span class="n">tick_fontsize</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                              <span class="n">ylabel</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;plots the RC predictions</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        gt_tr_override: if you want to calculate residuals from a non-noisy real training set,</span>
<span class="sd">            residuals will be calculated from it not say noisy inputs</span>

<span class="sd">        gt_te_override: if you want to calculate residuals from a non-noisy real validation set,</span>
<span class="sd">            residuals will be calculated from it not say noisy inputs</span>
<span class="sd">        fig: a matplotlib figure to plot on. </span>

<span class="sd">        ylabel: the users desired ylabel</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">fig</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

        <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span>
        
        <span class="k">if</span> <span class="n">prep</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_plot_prep</span><span class="p">(</span><span class="n">gt_tr_override</span><span class="p">,</span> <span class="n">gt_te_override</span><span class="p">)</span>

        <span class="c1">#do you want discrete time steps or the actual value of the input?</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1">#if you want actual value</span>
            <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_tr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_te</span>  
        <span class="k">except</span><span class="p">:</span>
            <span class="c1">#if you want the discrete indices/ don&#39;t supply the input.</span>
            <span class="c1">#assert False, &quot;fix this later&quot;</span>
            <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tr_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">te_idx</span>


        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_override</span><span class="p">:</span>
            <span class="n">pred_alpha</span> <span class="o">=</span> <span class="mf">0.4</span>
            <span class="n">gt_alpha</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">,</span>  <span class="n">alpha</span> <span class="o">=</span> <span class="n">pred_alpha</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="c1">#color = &quot;blue&quot;,</span>
                <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">)</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">pred_alpha</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="c1">#color = &quot;red&quot;, </span>
                    <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_train</span> <span class="o">+</span> <span class="n">input_test</span><span class="p">,</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_gt_tr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gt_te</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span>
                     <span class="s1">&#39;--&#39;</span><span class="p">,</span>
                     <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span>
                     <span class="n">alpha</span> <span class="o">=</span> <span class="n">gt_alpha</span><span class="p">,</span>
                     <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>

                     <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;ground truth&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred_alpha</span> <span class="o">=</span> <span class="mf">0.9</span>
            <span class="n">gt_alpha</span> <span class="o">=</span> <span class="mf">0.3</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_train</span> <span class="o">+</span> <span class="n">input_test</span><span class="p">,</span> <span class="c1">#self.tr_idx + self.te_idx,</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">y_tr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_val</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span>
                     <span class="s1">&#39;--&#39;</span><span class="p">,</span>
                     <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span>
                     <span class="n">alpha</span> <span class="o">=</span> <span class="n">gt_alpha</span><span class="p">,</span>
                     <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;ground truth&quot;</span><span class="p">)</span>

        
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yfit</span><span class="p">,</span>  <span class="n">alpha</span> <span class="o">=</span> <span class="n">pred_alpha</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw</span><span class="p">,</span> <span class="c1">#color = &quot;blue&quot;,</span>
                <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">pred_alpha</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw</span><span class="p">,</span> <span class="c1">#color = &quot;red&quot;, </span>
                    <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">input_train</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span>  <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw_vert</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">ylabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">,</span> <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="n">axis_label_fontsize</span><span class="p">})</span>
        <span class="c1">#plt.legend()</span>
        <span class="k">if</span> <span class="n">tick_fontsize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="n">tick_fontsize</span><span class="p">)</span></div>

<div class="viewcode-block" id="RcNetwork.plot_residuals"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.plot_residuals">[docs]</a>    <span class="k">def</span> <span class="nf">plot_residuals</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                       <span class="n">axis_label_fontsize</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                       <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                       <span class="n">gt_tr_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                       <span class="n">gt_te_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                       <span class="n">lw_vert</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                       <span class="n">prep</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                       <span class="n">tick_fontsize</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                       <span class="n">ylabel</span> <span class="o">=</span> <span class="kc">None</span>
                       <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Residuals plot</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis_label_fontsize : int</span>
<span class="sd">            fontsize for axis labels</span>
<span class="sd">        fig  : matplotlib.figure.Figure</span>
<span class="sd">            Figure to use to plot. If None, a Figure will be generated. </span>
<span class="sd">            matplotlib.figure.Figure: The top level container for all the plot elements.</span>
<span class="sd">        gt_tr_override : dtype</span>
<span class="sd">            Description of arg3</span>
<span class="sd">        gt_te_override : dtype</span>
<span class="sd">            Description of arg4</span>
<span class="sd">        lw_vert : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        prep : bool</span>
<span class="sd">            Desc</span>
<span class="sd">        tick_fontsize: int</span>
<span class="sd">            Desc</span>
<span class="sd">        ylabel: dtype</span>
<span class="sd">            Desc</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prep</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_plot_prep</span><span class="p">(</span><span class="n">gt_tr_override</span><span class="p">,</span> <span class="n">gt_te_override</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fig</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

        <span class="c1">#do you want discrete time steps or the actual value of the input?</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1">#if you want actual value</span>
            <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_tr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_te</span>  
        <span class="k">except</span><span class="p">:</span>
            <span class="c1">#if you want the discrete indices/ don&#39;t supply the input.</span>
            <span class="c1">#assert False, &quot;fix this later&quot;</span>
            <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tr_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">te_idx</span>

        
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tr_resids</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">te_resids</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">input_train</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span>  <span class="n">linewidth</span> <span class="o">=</span> <span class="n">lw_vert</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ylabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">,</span> <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="n">axis_label_fontsize</span><span class="p">})</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$t$&quot;</span><span class="p">,</span> <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="n">axis_label_fontsize</span><span class="p">})</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">tick_fontsize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="n">tick_fontsize</span><span class="p">)</span></div>

<div class="viewcode-block" id="RcNetwork.combined_plot"><a class="viewcode-back" href="../../Pages/api.html#rctorchprivate.rc.RcNetwork.combined_plot">[docs]</a>    <span class="k">def</span> <span class="nf">combined_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_tr</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">t_te</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">gt_tr_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">gt_te_override</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  
                            <span class="n">axis_label_fontsize</span> <span class="o">=</span> <span class="mi">29</span><span class="p">,</span>
                            <span class="n">lw_vert</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> 
                            <span class="n">tight_layout_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;w_pad&#39;</span> <span class="p">:</span> <span class="mf">0.00</span><span class="p">,</span> <span class="s1">&#39;h_pad&#39;</span> <span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span> <span class="p">:</span> <span class="mf">0.01</span> <span class="p">},</span> 
                            <span class="n">ylabel_pred</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">tick_fontsize</span><span class="o">=</span> <span class="mi">27</span><span class="p">,</span>
                            <span class="n">ylabel_resid</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$MSE$&#39;</span><span class="p">,</span>
                            <span class="n">grid_spec_x</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span>
                            <span class="n">labelsize</span>  <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
                            <span class="n">resid_blocks</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plots both residuals and the prediction.</span>

<span class="sd">        Extended description of function.</span>

<span class="sd">        tight_layout_args: must be a dictionary,</span>
<span class="sd">            for example: {&#39;pad&#39;=0.4, &#39;w_pad&#39;=0.5, &#39;h_pad&#39;=1.0}</span>
<span class="sd">            #https://matplotlib.org/stable/tutorials/intermediate/tight_layout_guide.html</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis_label_fontsize : int</span>
<span class="sd">            fontsize for axis labels</span>
<span class="sd">        fig  : matplotlib.figure.Figure</span>
<span class="sd">            Figure to use to plot. If None, a Figure will be generated. </span>
<span class="sd">            matplotlib.figure.Figure: The top level container for all the plot elements.</span>
<span class="sd">        grid_spec_x : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        gt_tr_override : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        gt_te_override  : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        labelsize : int</span>
<span class="sd">            font size for labels</span>
<span class="sd">        lw_vert  : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        resid_blocks : int</span>
<span class="sd">            Desc</span>
<span class="sd">        tick_fontsize : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        tight_layout_args  : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        t_tr : dtype</span>
<span class="sd">            Description of arg1</span>
<span class="sd">        t_te : dtype</span>
<span class="sd">            Description of arg2</span>
<span class="sd">        ylabel_pred : dtype</span>
<span class="sd">            Desc</span>
<span class="sd">        ylabel_resid : dtype</span>
<span class="sd">            Desc</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#assign the time train and time test, if they exist.</span>
        <span class="k">if</span> <span class="n">t_tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_tr</span> <span class="o">=</span> <span class="n">t_tr</span>
        <span class="k">if</span> <span class="n">t_te</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_te</span> <span class="o">=</span> <span class="n">t_te</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_plot_prep</span><span class="p">(</span><span class="n">gt_tr_override</span><span class="p">,</span> <span class="n">gt_te_override</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fig</span><span class="p">:</span>
            <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">grid_spec_x</span>
            <span class="n">gs1</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">g</span><span class="p">);</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs1</span><span class="p">[:</span><span class="n">resid_blocks</span><span class="p">,</span> <span class="p">:])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">plot_prediction</span><span class="p">(</span><span class="n">gt_tr_override</span> <span class="o">=</span> <span class="n">gt_tr_override</span><span class="p">,</span> 
                             <span class="n">gt_te_override</span> <span class="o">=</span> <span class="n">gt_te_override</span><span class="p">,</span> 
                             <span class="n">fig</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
                             <span class="n">prep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
                             <span class="n">axis_label_fontsize</span> <span class="o">=</span> <span class="n">axis_label_fontsize</span><span class="p">,</span> 
                             <span class="n">tick_fontsize</span> <span class="o">=</span> <span class="n">tick_fontsize</span><span class="p">,</span>
                             <span class="n">lw_vert</span> <span class="o">=</span> <span class="n">lw_vert</span><span class="p">,</span> 
                             <span class="n">ylabel</span> <span class="o">=</span> <span class="n">ylabel_pred</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelsize</span> <span class="o">=</span> <span class="n">tick_fontsize</span><span class="p">)</span>

        <span class="c1">#[ (plt.sca(ax[i]), plt.xticks(fontsize=tick_fontsize), plt.yticks(fontsize=tick_fontsize)) for i in range(3)]</span>

        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span> <span class="n">gs1</span><span class="p">[</span><span class="n">resid_blocks</span><span class="p">:,</span> <span class="p">:]</span> <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_residuals</span><span class="p">(</span><span class="n">fig</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="c1">#prep = False, </span>
                            <span class="n">lw_vert</span> <span class="o">=</span> <span class="n">lw_vert</span><span class="p">,</span> 
                            <span class="n">ylabel</span> <span class="o">=</span> <span class="n">ylabel_resid</span><span class="p">,</span>
                            <span class="n">axis_label_fontsize</span> <span class="o">=</span> <span class="n">axis_label_fontsize</span><span class="p">,</span> 
                            <span class="n">tick_fontsize</span> <span class="o">=</span> <span class="n">tick_fontsize</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">tight_layout_args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="o">**</span><span class="n">tight_layout_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span></div>
    
    

    <span class="k">def</span> <span class="nf">_descale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal descaling function</span>
<span class="sd">        Denormalizes array by column (along rows) using stored mean and standard deviation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : array or None</span>
<span class="sd">            Any inputs that need to be transformed back to their original scales</span>
<span class="sd">        outputs : array or None</span>
<span class="sd">            Any output that need to be transformed back to their original scales</span>
<span class="sd">        normalize: bool</span>
<span class="sd">            Desc</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        transformed : tuple or array</span>
<span class="sd">            Returns tuple of every denormalized array. In case only one object is to be returned the tuple will be</span>
<span class="sd">            unpacked before returning</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Inputs and outputs cannot both be None&#39;</span><span class="p">)</span>

        <span class="c1"># Storage for transformed variables</span>
        <span class="n">transformed</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1">#for tensor in [train_x, train_y]:</span>
        <span class="c1">#     print(&#39;device&#39;,tensor.get_device())</span>
        
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;not implimented&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39; there is no need to normalize the input, standardize instead&#39;</span>
                <span class="c1">#transformed.append(( (inputs + 0.5) * self._input_ranges) + self._input_mins)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

                <span class="n">destandardized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_descale</span><span class="p">(</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="o">==</span> <span class="s2">&quot;sin&quot;</span><span class="p">:</span>
                    <span class="n">denormalized</span> <span class="o">=</span> <span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f</span><span class="p">(</span><span class="n">destandardized</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_ranges</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_mins</span>

                <span class="k">else</span><span class="p">:</span>

                    <span class="n">denormalized</span> <span class="o">=</span> <span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_f</span><span class="p">(</span><span class="n">destandardized</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">tanh_bound_limit</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_ranges</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_mins</span>

                <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">denormalized</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># if self.ODE_order:</span>
                <span class="c1">#     transformed.append(inputs * self._input_stds)</span>
                <span class="c1"># else:</span>
                <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">inputs</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_stds</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_means</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ODE_order</span><span class="p">:</span>
                    <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">transformed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">outputs</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_stds</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_means</span><span class="p">)</span>

        <span class="c1"># Syntactic sugar</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">transformed</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformed</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">                                    #calculate F depending on the order of the ODE:</span>
<span class="sd">                                    if ODE == 1:</span>
<span class="sd">                                        #population eq</span>
<span class="sd">                                        #RHS = lam * self.y0 - f(self.X) </span>
<span class="sd">                                        #self.F =  g_dot * states_  +  g * (states_dot + lam * states_)</span>
<span class="sd">                                        </span>
<span class="sd">                                        #nl eq </span>
<span class="sd">                                        self.F =  g_dot * states_  +  g * states_dot</span>
<span class="sd">                                        if nl_f:</span>
<span class="sd">                                            #y0_nl, y0_nl_dot = nl_f(self.y0)</span>
<span class="sd">                                            self.F = self.F - 2 * self.y0 * g * states_ </span>
<span class="sd">                                            #self.F = self.F - (g * ).T @ </span>

<span class="sd">                                    elif ODE == 2:</span>
<span class="sd">                                        # without a reparameterization</span>
<span class="sd">                                        #self.F = torch.square(self.X) * states_dot2 + 4 * self.X * states_dot + 2 * states_ + (self.X ** 2) * states_</span>
<span class="sd">                                        self.G = g * states_</span>
<span class="sd">                                        assert self.G.shape == states_.shape, f&#39;{self.shape} != {self.states_.shape}&#39;</span>
<span class="sd">                                        self.Lambda = g.pow(2) * states_ </span>
<span class="sd">                                        self.k = 2 * states_ + g * (4*states_dot - self.G*states_) + g.pow(2) * (4 * states_ - 4 * states_dot + states_dot2)</span>
<span class="sd">                                        self.F = self.k + self.Lambda</span>
<span class="sd">                                    #common F derivation:</span>
<span class="sd">                                    F = self.F.T</span>
<span class="sd">                                    F1 = F.T @ F </span>
<span class="sd">                                    F1 = F1 + self.regularization * torch.eye(F1.shape[1], **self.dev)</span>
<span class="sd">                                    ##################################### non-linear adustment</span>
<span class="sd">                                    nl_adjust = False</span>
<span class="sd">                                    if nl_adjust:</span>
<span class="sd">                                        G = g * states_</span>
<span class="sd">                                        G_sq = G @ G.T</span>
<span class="sd">                                        nl_correction = -2 * self.y0 * (G_sq)</span>
<span class="sd">                                        F1 = F1 + nl_correction</span>
<span class="sd">                                    #F1_inv = torch.pinverse(F1)</span>
<span class="sd">                                    #F2 = torch.matmul(F1_inv, F.T)</span>
<span class="sd">                                    #####################################</span>
<span class="sd">                                    #First Order equation</span>
<span class="sd">                                    if self.ODE_order == 1:</span>
<span class="sd">                                        self.y0I = (self.y0 ** 2) * torch.ones_like(self.X)</span>
<span class="sd">                                        #self.y0I = self.y0I.squeeze().unsqueeze(0)</span>
<span class="sd">                                        #RHS = lam*self.y0I.T - f(self.X) </span>

<span class="sd">                                        #REPARAM population</span>
<span class="sd">                                        #RHS = lam * self.y0 - f(self.X) </span>

<span class="sd">                                        RHS = self.y0I</span>

<span class="sd">                                        #weight = torch.matmul(-F2.T, RHS)</span>

<span class="sd">                                        weight = torch.matmul(F2.T, RHS)</span>
<span class="sd">                                        #assert False, weight.shape</span>

<span class="sd">                                    #Second Order equation</span>
<span class="sd">                                    elif self.ODE_order == 2:</span>
<span class="sd">                                        </span>
<span class="sd">                                        #self.y0I = y0[0] * torch.ones_like(self.X)</span>
<span class="sd">                                        #self.y0I = self.y0I.squeeze().unsqueeze(0)</span>

<span class="sd">                                        #RHS = self.y0I.T + self.X * y0[1]</span>
<span class="sd">                                        RHS = self.y0 + f_t * y0[1]</span>
<span class="sd">                                        </span>
<span class="sd">                                        #t = self.X</span>
<span class="sd">                                        #A0 = y0 + g * v0</span>
<span class="sd">                                        #RHS = A0 + (g - 1)*v0 - f(t)</span>
<span class="sd">                                        weight = torch.matmul(-F2.T, D_A)</span>
<span class="sd">                                    weight = torch.matmul(D_W, D_A)</span>

<span class="sd">                                    #y = y0[0] + self.X * y0[1] + self.X</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        if self.ODE_order == 1:</span>
<span class="sd">            return self.reparam(t = self.X, init_conditions = self.y0, N = self.yfit, N_dot = N_dot)</span>
<span class="sd">        elif self.ODE_order == 2:</span>
<span class="sd">            N_dot2 = self.calc_hdot(states_dot2[:,1:], cutoff = False)</span>
<span class="sd">            return self.reparam(t = self.X, init_conditions = [y0, v0], </span>
<span class="sd">                N = self.yfit, N_dot = [N_dot, N_dot2], esn = self, </span>
<span class="sd">                states = states_[:,1:], states_dot = states_dot[:,1:], states_dot2 = states_dot2[:,1:])</span>
<span class="sd">        &quot;&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    #assert weight.requires_grad, &quot;weight doesn&#39;t req grad&quot;</span>
<span class="sd">                            #torch.solve solves AX = B. Here X is beta_hat, A is ridge_x, and B is ridge_y</span>
<span class="sd">                            #weight = torch.solve(ridge_y, ridge_x).solution</span>
<span class="sd">                        # elif self.l2_prop == 1:</span>
<span class="sd">                        # else: #+++++++++++++++++++++++         This section is elastic net         +++++++++++++++++++++++++++++++</span>

<span class="sd">                        #     gram_matrix = torch.matmul(train_x.T, train_x) </span>

<span class="sd">                        #     regr = ElasticNet(random_state=0, </span>
<span class="sd">                        #                           alpha = self.regularization, </span>
<span class="sd">                        #                           l1_ratio = 1-self.l2_prop,</span>
<span class="sd">                        #                           selection = &quot;random&quot;,</span>
<span class="sd">                        #                           max_iter = 3000,</span>
<span class="sd">                        #                           tol = 1e-3,</span>
<span class="sd">                        #                           #precompute = gram_matrix.numpy(),</span>
<span class="sd">                        #                           fit_intercept = True</span>
<span class="sd">                        #                           )</span>
<span class="sd">                        #     print(&quot;train_x&quot;, train_x.shape, &quot;_____________ train_y&quot;, train_y.shape)</span>
<span class="sd">                        #     regr.fit(train_x.numpy(), train_y.numpy())</span>

<span class="sd">                        #     weight = torch.tensor(regr.coef_, device = self.device, **self.dev)</span>
<span class="sd">                        #     bias =  torch.tensor(regr.intercept_, device =self.device, **self.dev)</span>


<span class="sd">#if not preloaded_states_dict:</span>
<span class="sd">                # else:</span>
<span class="sd">                #     sd = preloaded_states_dict</span>
<span class="sd">                #     self.states, self.states_dot, G, self.extended_states = sd[&quot;s&quot;], sd[&quot;s1&quot;], sd[&quot;G&quot;], sd[&quot;ex&quot;]</span>
<span class="sd">                #     states_with_bias, states_dot_with_bias = sd[&quot;sb&quot;], sd[&quot;sb1&quot;]</span>
<span class="sd">                #     # if self.ODE_order == 2:</span>
<span class="sd">                #     #     self.states_dot2 = sd[&quot;s2&quot;]</span>
<span class="sd">                #     #     states_dot2_with_bias = sd[&quot;sb2&quot;]</span>
<span class="sd">                #     g, g_dot = G</span>
<span class="sd">                #     self.g = gdef</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1">#EchoStateNetwork = RcNetwork</span>


<span class="c1"># class Recurrence(Function):</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     Summary line.</span>

<span class="c1">#     Extended description of function.</span>

<span class="c1">#     Parameters</span>
<span class="c1">#     ----------</span>
<span class="c1">#     arg1 : int</span>
<span class="c1">#         Description of arg1</span>
<span class="c1">#     arg2 : str</span>
<span class="c1">#         Description of arg2</span>

<span class="c1">#     Returns</span>
<span class="c1">#     -------</span>
<span class="c1">#     int</span>
<span class="c1">#         Description of return value</span>

<span class="c1">#     &quot;&quot;&quot;</span>

<span class="c1">#     @staticmethod</span>
<span class="c1">#     def forward(ctx, states, esn, X, y, weights):</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         Summary line.</span>

<span class="c1">#         Extended description of function.</span>

<span class="c1">#         Parameters</span>
<span class="c1">#         ----------</span>
<span class="c1">#         ctx : dtype</span>
<span class="c1">#             Description of arg1</span>
<span class="c1">#         states : dtype</span>
<span class="c1">#             Description of arg2</span>
<span class="c1">#         esn : RcNetwork</span>
<span class="c1">#             the echo-state network object</span>
<span class="c1">#         X : pytorch.tensor or numpy.array</span>
<span class="c1">#             observers</span>
<span class="c1">#         y : pytorch.tensor or numpy.array</span>
<span class="c1">#             response</span>
<span class="c1">#         weights : ... </span>
<span class="c1">#             Desc</span>
<span class="c1">#         Returns</span>
<span class="c1">#         -------</span>
<span class="c1">#         states, states_dot : pytorch.tensor, pytorch.tensor</span>
<span class="c1">#             The hidden states and the derivative of the hidden states</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         states, states_dot = esn.train_states(X, y, states)</span>
<span class="c1">#         ctx.states = states</span>
<span class="c1">#         ctx.states_dot = states_dot</span>
<span class="c1">#         return states, states_dot</span>
<span class="c1">#     @staticmethod</span>
<span class="c1">#     def backward(ctx, grad_output, weights):</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         Summary line.</span>
<span class="c1">#         Extended description of function.</span>
<span class="c1">#         Parameters</span>
<span class="c1">#         ----------</span>
<span class="c1">#         ctx : int</span>
<span class="c1">#             Description of arg1</span>
<span class="c1">#         grad_output : str</span>
<span class="c1">#             Description of arg2</span>
<span class="c1">#         weights : pytorch.tensor?</span>
<span class="c1">#             Desc</span>
<span class="c1">#         Returns</span>
<span class="c1">#         -------</span>
<span class="c1">#         int</span>
<span class="c1">#             Description of return value</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         if grad_output is None:</span>
<span class="c1">#             return None, None</span>
<span class="c1">#         output = torch.matmul(ctx.states_dot, weights.T)</span>
<span class="c1">#         return output, None, None, None, None</span>
                                    
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">RcTorch</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Pages/api.html">RcTorch API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Pages/tutorials/forced_pendulum.html">RcTorch Tutorial: Forced Pendulum Example</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Hayden Joy.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>