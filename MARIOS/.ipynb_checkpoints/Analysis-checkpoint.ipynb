{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Analysis Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It’s publication time.\n",
    "\n",
    "## Top priority tasks\n",
    "\n",
    "0) Execute some pure predictions on the cluster (medium dataset).\n",
    "\n",
    "    SORT OUT EXPONENTIAL VS UNIFORM (only run one, try breaking apart noise from model, independent regularization term).\n",
    "    \n",
    "0b) Try to run Cyclic Exponential tests.\n",
    "\n",
    "1) Edit the article.\n",
    "\n",
    "2) prepare for meeting with Marios, prepare pure prediction, multiple kinds.\n",
    "\n",
    "3) Fix \"ip: nearest\", change to label \"interpolation\"\n",
    "\n",
    "## Computational complexity tasks\n",
    "1) save the repaired dictionaries\n",
    "\n",
    "2) delete old crappy results (clean the experiment_results directory)\n",
    "\n",
    "3) Fix tqdm notebook software bug.\n",
    "\n",
    "4) Delete unnecessary python kernels.\n",
    "\n",
    "5) Edit notebook, delete cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from platform import python_version\n",
    "import copy\n",
    "import glob\n",
    "import matplotlib\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(python_version())\n",
    "\n",
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "\n",
    "# to stop print output:\n",
    "#def blockPrint():\n",
    "#    sys.stdout = open(os.devnull, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_freq(experiment):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_spec_lst = np.array(experiment[\"f\"])[experiment[\"resp_idx\"]]\n",
    "    target_freq_spec = np.round(np.mean(freq_spec_lst),1)\n",
    "    return(target_freq_spec)\n",
    "\n",
    "def IdxMatch(experiment, \n",
    "             resp_idx_range = range(272, 301), \n",
    "             n_obs = 28\n",
    "             ):\n",
    "    \"\"\"\n",
    "    obs3:\n",
    "    obs4: resp_idx_range = range(272, 301) n_obs = 28\n",
    "    obs5:\n",
    "    \"\"\"\n",
    "    #if obs_idx_lst == experiment[\"obs_idx\"]:\n",
    "    #    if resp_idx_lst == experiment[\"resp_idx\"]:\n",
    "    #        return True\n",
    "    resp_idx_list = list(resp_idx_range)\n",
    "    if resp_idx_list == experiment[\"resp_idx\"]:\n",
    "        if n_obs == len(experiment[\"obs_idx\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def freq_plot(analysis_obj, experiment_num = 0, title = \"Observer 4 experiment\",  save = None,\n",
    "             custom_colors = True, mean = False):\n",
    "    \"\"\" \n",
    "    This function takes an EchoStateAnalysis object and creates a frequency loss plot.\n",
    "    \n",
    "    Arguments:\n",
    "        analysis_obj: the EchoStateAnalysis object\n",
    "        experiment_num: the index number of the EchoStateExperiment.\n",
    "        title: the title of the experiment.\n",
    "        save: the path to which to save the image.\n",
    "    \"\"\"\n",
    "    freq_rDF = analysis_obj.rDF.copy()\n",
    "    freq_loss_df =  freq_rDF[freq_rDF[\"experiment #\"] == experiment_num]\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "    modelz = np.unique(freq_loss_df.model)\n",
    "    if custom_colors: #len(modelz) == 4 and \n",
    "        palette_ = dict(zip([\"cyclic\", \"uniform\", \"ip: linear\", \"exponential\", \"random_exponential\",\n",
    "                            \"random_uniform\", \"ip: nearest\"], \n",
    "                            sns.color_palette(\"tab10\")[0:7] ))\n",
    "    else:\n",
    "        palette_ = dict(zip(modelz, sns.color_palette(\"tab10\")[0:4]))\n",
    "    palette_[\"ip: nearest\"] = sns.color_palette(\"tab10\")[1]\n",
    "    for i, model in enumerate(modelz):\n",
    "        \n",
    "        freq_rDF_spec = freq_loss_df[freq_loss_df.model == model]\n",
    "        mean_freq_rDF_spec = freq_rDF_spec.groupby(by = \"freq\").mean()\n",
    "        mean_freq_rDF_spec[\"model\"] = model\n",
    "        \n",
    "        #multi-model df concatenation\n",
    "        if not i:\n",
    "            mean_freq_rDF = mean_freq_rDF_spec \n",
    "        else: \n",
    "            mean_freq_rDF = pd.concat([mean_freq_rDF, mean_freq_rDF_spec], axis = 0)\n",
    "        #model mean lines \n",
    "        plt.axhline(np.mean(freq_rDF_spec.L2), color = palette_[model], linestyle = \"dotted\")\n",
    "    if mean:\n",
    "        mean_freq_rDF[\"freq\"] = list(mean_freq_rDF.index)\n",
    "        \n",
    "        sns.lineplot(x = \"freq\", y = \"L2\", data = mean_freq_rDF, alpha = 0.9, hue = \"model\", palette = palette_)\n",
    "    else:\n",
    "        sns.lineplot(x = \"freq\", y = \"L2\", data = freq_loss_df, alpha = 0.9, hue = \"model\", palette = palette_)\n",
    "    \n",
    "    plt.title(title, fontsize = 16)\n",
    "    plt.xlabel(\"Frequency (Hz)\", fontsize = 15)\n",
    "    plt.ylabel(\"L2 Loss\", fontsize = 15)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "\n",
    "\n",
    "def prediction_plot(analysis_obj, experiment_num = 0, title = \"Observer 4 experiment\", \n",
    "                    rolling = None, save = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "        \n",
    "    \n",
    "#pickle_A.experiment_lst[0] = pickle_zhizhuo.experiment_lst[0] \n",
    "def quick_plot(n, analysis, outfile = None, mean = False, rolling = 150, force_build = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(analysis) != list:\n",
    "        analysis = [analysis]\n",
    "    \n",
    "    #only save the plot if requested.\n",
    "    if outfile:\n",
    "        save_path = \"fig/\" + outfile \n",
    "        save_path_time = save_path + \"_time\"\n",
    "        save_path_freq = save_path + \"_freq\"\n",
    "    else:\n",
    "        save_path_time, save_path_freq  = None, None\n",
    "        outfile = \"\"\n",
    "    \n",
    "    for i, analysis_ in enumerate(analysis):\n",
    "        models = list(analysis_.experiment_lst[n][\"prediction\"].keys())\n",
    "        if not hasattr(analysis_, 'rDF'):\n",
    "            analysis_.build_loss_df(group_by = \"freq\", models = models)\n",
    "        if force_build:\n",
    "            analysis_.build_loss_df(group_by = \"freq\", models = models)\n",
    "        freq_plot(analysis_, n, title = outfile + \" Avg. L2 Loss vs Frequency\", \n",
    "                  save = save_path_freq, mean = mean)\n",
    "        time_plot(analysis_, experiment_num = n, rolling = rolling, save = save_path_time,\n",
    "              title = outfile + \" Avg. L2 Loss vs Time (rolling average)\", mean = mean)\n",
    "        #plt.show()\n",
    "    return analysis[0].rDF\n",
    "\n",
    "def show_images(analysis_, exper_num = 0, aspect = 10, sigma = 1, method = \"heatmap\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    exper_ = analysis_.experiment_lst[exper_num]\n",
    "    Train, Test = exper_[\"xTr\"], exper_[\"xTe\"]\n",
    "    predictions = exper_[\"prediction\"].copy()\n",
    "\n",
    "    for i in predictions.values():\n",
    "        a = np.array(i)\n",
    "        #print(a.shape)\n",
    "        \n",
    "    nrmses      = exper_[\"nrmse\"].copy()\n",
    "    predictions[\"ground_Truth\"]  = Test\n",
    "    nrmses[\"ground_Truth\"]       = 0\n",
    "    #predictions[\"ground_Truth_smooth\"]  = gaussian_filter(Test, sigma=sigma)\n",
    "    #nrmses[\"ground_Truth_smooth\"]       = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(5,1, figsize = (12, 24))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, (key, value) in enumerate(predictions.items()):\n",
    "        print(i)\n",
    "        arr = np.array(value)\n",
    "        full_arr = np.concatenate((Train, arr), axis = 0)\n",
    "        \n",
    "        if method != \"heatmap\":\n",
    "            plt.imshow(arr.T, aspect = aspect)\n",
    "            plt.title(key +\" R: \" + str(nrmses[key]))\n",
    "            plt.subplot(2,3,i)\n",
    "            plt.show()       \n",
    "        else:\n",
    "            sns.heatmap(full_arr.T, ax = ax[i])\n",
    "            ax[i].set_title(key +\" R: \" + str(nrmses[key]))\n",
    "    plt.tight_layout()\n",
    "\n",
    "def load_analysis(path, model = \"uniform\", bp = \"experiment_results/publish/split_0.5/\", ip_use_obs = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert model in [\"uniform\", \"cyclic\"]\n",
    "    # path = 'experiment_results/publish/split_0.5/block_target*.pickle'\n",
    "    path_list = glob.glob(bp + path)\n",
    "    print(path_list)\n",
    "    expers = EchoStateAnalysis(path_list, model = model, ip_use_observers = ip_use_obs, ip_method = \"linear\")\n",
    "    return(expers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot(analysis_obj, experiment_num = 0, title = \"Observer 4 experiment\", rolling = None, save = None,\n",
    "             mean = False, custom_colors = True):\n",
    "    \"\"\"\n",
    "    This function takes an EchoStateAnalysis object and creates a time loss plot.\n",
    "    \n",
    "    Arguments:\n",
    "        analysis_obj: the EchoStateAnalysis object\n",
    "        experiment_num: the index number of the EchoStateExperiment.\n",
    "        title: the title of the experiment.\n",
    "        save: the path to which to save the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    time_rDF = analysis_obj.rDF.copy()\n",
    "    time_loss_df =  time_rDF[time_rDF[\"experiment #\"] == experiment_num].copy()\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "    rollings = {}\n",
    "    if custom_colors:\n",
    "        palette_ = dict(zip([\"cyclic\", \"uniform\", \"ip: linear\", \"exponential\", \"random_uniform\",\n",
    "                            \"random_exponential\", \"ip: nearest\"], sns.color_palette(\"tab10\")[0:7]))\n",
    "    else:\n",
    "        palette_ = dict(zip(modelz, sns.color_palette(\"tab10\")[0:4]))\n",
    "    palette_[\"ip: nearest\"] = sns.color_palette(\"tab10\")[1]\n",
    "    if rolling:\n",
    "        #print(\"ROLLING\", rolling)\n",
    "        mean_ = []\n",
    "        for i, model in enumerate(np.unique(time_loss_df.model)):\n",
    "            filterr = time_loss_df[\"model\"] == model\n",
    "            time_rDF_spec = time_loss_df[filterr]\n",
    "            time_rDF_spec[\"rolling_L2\"] = -1\n",
    "            \n",
    "            #multi-model df concatenation\n",
    "            if mean:\n",
    "                time_sub_df = time_rDF_spec.groupby(by = \"time\").mean()\n",
    "                time_sub_df[\"model\"] = model\n",
    "\n",
    "                time_sub_df.rolling_L2 = time_sub_df.L2.rolling(rolling).mean()\n",
    "\n",
    "                if not i:\n",
    "                    mean_time_rDF = time_sub_df\n",
    "                    \n",
    "                else: \n",
    "                    mean_time_rDF = pd.concat([mean_time_rDF, time_sub_df], axis = 0)\n",
    "                    \n",
    "                mean_time_rDF[\"time\"] = list(mean_time_rDF.index)\n",
    "                \n",
    "            else:\n",
    "                rollings[model] = time_rDF_spec.L2.rolling(rolling).mean()\n",
    "            #display(rollings)\n",
    "            plt.axhline(np.mean(time_rDF_spec.L2.copy()), linewidth = 1.5,\n",
    "                        color = palette_[model], linestyle = \"dotted\")\n",
    "        \n",
    "        \n",
    "        modelzz = np.unique(time_loss_df.model)\n",
    "\n",
    "        for model in modelzz:\n",
    "            if mean:\n",
    "                pass\n",
    "                #mean_time_rDF.rolling_L2[mean_time_rDF.model == model] = rollings[model]\n",
    "                \n",
    "            else:\n",
    "                time_loss_df.rolling_L2[time_loss_df.model == model] = rollings[model]\n",
    "        if mean:\n",
    "  \n",
    "            sns.lineplot(x = \"time\", y = \"rolling_L2\", data = mean_time_rDF, palette = palette_, \n",
    "                         alpha = 0.75, linewidth = 2.5, hue = \"model\") \n",
    "        else:\n",
    "            sns.lineplot(x = \"time\", y = \"rolling_L2\", data = time_loss_df, palette = palette_, \n",
    "                         alpha = 0.85,linewidth = 1.5, hue = \"model\") \n",
    "    else:\n",
    "        print(\"NAH\", rolling)\n",
    "        sns.lineplot(x = \"time\", y = \"L2\", data = time_loss_df,  alpha = 0.9, hue = \"model\")\n",
    "    lb = 0\n",
    "    ub = np.percentile(time_loss_df.L2, 95)\n",
    "    title = title\n",
    "    plt.title(title, fontsize = 25)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize = 20)\n",
    "    plt.ylabel(\"L2 Loss $(y-\\hat{y})^2$\", fontsize = 20)\n",
    "    plt.legend(prop={\"size\":17})\n",
    "    sns.despine()\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiment_results/medium/split_0.5/column_cyclic_unif_W_N_Targidx_1103N_Obsidx_0.pickle']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d653cc4caa432b98565b2307825735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='experiment list, loading data...'), FloatProgress(value=0.0, max=1.0), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['prediction', 'nrmse', 'best arguments', 'obs_idx', 'resp_idx', 'input_weight_type', 'Train', 'Test', 'xTe', 'xTr', 'experiment_inputs', 'get_observer_inputs'])\n",
      "xTe shape (100, 1103)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55beae8fba4f43b4af21efd95c77c0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='experiment list, fixing interpolation...'), FloatProgress(value=0.0, max=1.0), HTML…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLIC DISABLED\n",
      "{'noise': 0.1584675971825879, 'connectivity': 0.0005452050474823753, 'spectral_radius': 0.5017982447147369, 'regularization': 1793.3769622864643, 'leaking_rate': 0.7393609389662743, 'n_nodes': 1000, 'feedback': True}\n",
      "uniform\n",
      "COLUMN GOLUMN!\n",
      "OVERWRITING f\n",
      "OVERWRITING T\n",
      "OVERWRITING A\n",
      "\n",
      "[]\n",
      "[]\n",
      "total experiments completed: 1\n",
      "total experiments half complete: 0\n",
      "total experiments not yet run: 0\n",
      "Percentage of tests completed: 100.0%\n",
      "CPU times: user 27.8 s, sys: 634 ms, total: 28.4 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#rand_path = 'final_results/*random.pickle'\n",
    "\n",
    "#column_expers = load_analysis(path = \"*\", bp = \"pure_prediction/\", \n",
    "#                              model = \"cyclic\", ip_use_obs = False)\n",
    "\n",
    "rand_path = 'final_results/*random.pickle'\n",
    "random_path_list = glob.glob(rand_path)\n",
    "\n",
    "random_path_list = [#'block_N_Targidx_115N_Obsidx_74.pickle', \n",
    "                    #'experiment_results/publish/split_0.5/block_N_Targidx_115N_Obsidx_143.pickle',\n",
    "                    'experiment_results/medium/split_0.5/column_cyclic_unif_W_N_Targidx_1103N_Obsidx_0.pickle']\n",
    "                   #'experiment_results/medium/split_0.5/column_cyclic_unif_W_N_Targidx_1103N_Obsidx_0.pickle']\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "COMBINED_ANALYSIS_NOT_LOADED = True\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    \n",
    "    print(random_path_list)\n",
    "    random_expers = EchoStateAnalysis(random_path_list, model = \"uniform\",\n",
    "                                  ip_use_observers = True, ip_method = \"nearest\",\n",
    "                                     force_random_expers = True, pure_prediction = True)\n",
    "    \n",
    "    #random_expers = EchoStateAnalysis(random_path_list, model = \"uniform\",\n",
    "    #                              ip_use_observers = True, ip_method = \"linear\")\n",
    "    \n",
    "    #dl_path_list = glob.glob('final_results/*cyclic.pickle')\n",
    "\n",
    "    #dl_path_list = [\"final_results/500_900Hz_cyclic.pickle\",\n",
    "    #                \"final_results/150_500Hz_cyclic.pickle\",\n",
    "    #               \"experiment_results/publish/split_0.5/block_cyclictargetHz_ctr:_999targetKhz:_0.1__obskHz:_0.25.pickle\",\n",
    "    #               \"experiment_results/publish/split_0.5/block_cyclictargetHz_ctr:_999targetKhz:_0.5__obskHz:_1.0.pickle\"]#\"experiment_results/publish/split_0.5/block_cyclicN_Targidx_40N_Obsidx_26.pickle\"]\n",
    "    #dl_expers = EchoStateAnalysis(dl_path_list, model = \"cyclic\",\n",
    "    #                              ip_use_observers = True, ip_method = \"linear\")\n",
    "\n",
    "#end = time()\n",
    "#end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time_plot(random_expers, experiment_num = 0,mean = True, rolling = 1)\n",
    "type(np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1848d69efda94a009f81b2fec251ad8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='processing path list...'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 512 is out of bounds for axis 0 with size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_expers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquick_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_expers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrolling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#show_images(column_expers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/analysis.py\u001b[0m in \u001b[0;36mquick_plot\u001b[0;34m(n, analysis, outfile, mean, rolling, force_build)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rDF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0manalysis_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_by\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"freq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0manalysis_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_by\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"freq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/analysis.py\u001b[0m in \u001b[0;36mbuild_loss_df\u001b[0;34m(self, group_by, columnwise, relative, rolling, models, silent, hybrid)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mtime_lst_one_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0mfreq_lst_one_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0;31m###################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 512 is out of bounds for axis 0 with size 512"
     ]
    }
   ],
   "source": [
    "for i in range(len(random_expers.experiment_lst)):\n",
    "    quick_plot(i, random_expers, mean = True, rolling = 3)\n",
    "    #show_images(column_expers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_idx = random_expers.experiment_lst[0][\"obs_idx\"]\n",
    "resp_idx = random_expers.experiment_lst[0][\"resp_idx\"]\n",
    "dual_lambda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = np.random.uniform(size = (10,100))\n",
    "hi2 = np.random.uniform(size = (10,100))\n",
    "hi.shape\n",
    "ridge_x = hi.T @ hi + 0.01 * np.eye(hi.shape[1])\n",
    "ridge_y = hi.T @ hi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "np.linalg.solve(ridge_x, ridge_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unq_dict_lst( lst1, lst2, key1 = \"start_indices\", key2 = \"random_seed\"):\n",
    "        dict_lst = []\n",
    "        for i in range(len(lst1)):\n",
    "            for j in range(len(lst2)):\n",
    "                dictt = {}\n",
    "                dictt[key1] =  lst1[i]\n",
    "                dictt[key2] =  lst2[j]\n",
    "                dict_lst.append(dictt)\n",
    "        return dict_lst\n",
    "\n",
    "build_unq_dict_lst([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "print(bcolors.FAIL + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "scipy.linalg.solve(ridge_x, ridge_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def build_distance_matrix()\n",
    "    def calculate_distance_matrix(obs_idx, resp_idx):\n",
    "        obs_idxx_arr = np.array(obs_idx)\n",
    "        for i, resp_seq in enumerate(resp_idx):\n",
    "            DistsToTarg = abs(resp_seq - obs_idxx_arr).reshape(1, -1)\n",
    "            if i == 0:\n",
    "                distance_np_ = DistsToTarg\n",
    "            else:\n",
    "                distance_np_ = np.concatenate([distance_np_, DistsToTarg], axis = 0)\n",
    "        #if verbose == True:\n",
    "        #    display(pd.DataFrame(distance_np))\n",
    "        distance_np_ = distance_np_\n",
    "        return(distance_np_)\n",
    "\n",
    "    if not dual_lambda:\n",
    "\n",
    "        distance_np = calculate_distance_matrix(obs_idx, resp_idx)\n",
    "\n",
    "    else:\n",
    "\n",
    "        def split_lst(lst, scnd_lst):\n",
    "\n",
    "            lst = np.array(lst)\n",
    "            breaka = np.mean(scnd_lst)\n",
    "            scnd_arr = np.array(scnd_lst)\n",
    "            lst1, lst2 = lst[lst < scnd_arr.min()], lst[lst > scnd_arr.max()]\n",
    "\n",
    "            return list(lst1), list(lst2)\n",
    "\n",
    "        obs_lsts = split_lst(obs_idx, resp_idx) #good!\n",
    "        self.distance_np = [calculate_distance_matrix(obs_lst) for obs_lst in obs_lsts]\n",
    "        \n",
    "\n",
    "def get_exp_weights():\n",
    "    \"\"\"\n",
    "    #TODO: description\n",
    "    change the automatic var assignments\n",
    "    \"\"\"\n",
    "    build_distance_matrix()\n",
    "    random_state = np.random.RandomState(self.seed)\n",
    "    self.exp_w(random_state)\n",
    "    n_temp = len(self.exp_weights)\n",
    "    sign = random_state.choice([-1, 1], self.exp_weights.shape)\n",
    "\n",
    "    exp_weights *= sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def calculate_distance_matrix(obs_idx):\n",
    "    obs_idxx_arr = np.array(obs_idx)\n",
    "    for i, resp_seq in enumerate(self.resp_idx):\n",
    "        DistsToTarg = abs(resp_seq - obs_idxx_arr).reshape(1, -1)\n",
    "        if i == 0:\n",
    "            distance_np_ = DistsToTarg\n",
    "        else:\n",
    "            distance_np_ = np.concatenate([distance_np_, DistsToTarg], axis = 0)\n",
    "    #if verbose == True:\n",
    "    #    display(pd.DataFrame(distance_np))\n",
    "    distance_np_ = distance_np_\n",
    "    if verbose == True:\n",
    "        print(\"distance_matrix completed \" + str(self.distance_np_.shape))\n",
    "        display(self.distance_np_)\n",
    "    return(distance_np_)\n",
    "\n",
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_expers.experiment_lst[0][\"best arguments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_path = 'final_results/*random.pickle'\n",
    "random_path_list = glob.glob(rand_path)\n",
    "\n",
    "\n",
    "COMBINED_ANALYSIS_NOT_LOADED = True\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    \n",
    "    print(random_path_list)\n",
    "    random_expers2 = EchoStateAnalysis(random_path_list, model = \"uniform\",\n",
    "                                  ip_use_observers = True, ip_method = \"linear\",\n",
    "                                     force_random_expers = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(123)\n",
    "random_state.choice([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_other_exper_W(analysis, w_dest, w_source,  models = [\"exponential\"], random_seed = 155,\n",
    "                     models2avg = 100):\n",
    "    \n",
    "    scores = {}\n",
    "    for model in models:\n",
    "        vals = w_source[\"best arguments\"][model]\n",
    "        my_esn = EchoStateNetwork(**vals, model_type = model)\n",
    "\n",
    "        obs_idx_temp = w_dest[\"obs_idx\"].copy()\n",
    "        resp_idx_temp = w_dest[\"resp_idx\"].copy()\n",
    "        A_temp = w_dest[\"A\"].copy()\n",
    "        tr_timesteps = np.array(w_dest[\"xTr\"]).shape[0]\n",
    "        Train = A_temp[:tr_timesteps ,obs_idx_temp]\n",
    "        Test = A_temp[tr_timesteps :,obs_idx_temp]\n",
    "        xTr = w_dest[\"xTr\"].copy()\n",
    "        xTe = w_dest[\"xTe\"].copy()\n",
    "        print(\"train shape\", Train.shape)\n",
    "\n",
    "        for i in range(models2avg):\n",
    "            \n",
    "                \n",
    "            vals[\"random_seed\"] = random_seed + i\n",
    "            my_esn = EchoStateNetwork(**vals, model_type = model, obs_idx = obs_idx_temp, resp_idx = resp_idx_temp)\n",
    "            my_esn.train(x = Train, y = xTr)\n",
    "            pred_spec = my_esn.predict(n_steps = Test.shape[0], x = Test)\n",
    "            score= (np.mean((pred_spec - xTe)**2))\n",
    "            \n",
    "            if not i:\n",
    "                pred = pred_spec.copy()\n",
    "                \n",
    "                scores[model] = [score]\n",
    "            else: \n",
    "                pred += pred_spec\n",
    "                scores[model].append(score)\n",
    "        w_dest[\"prediction\"][model] = pred/models2avg\n",
    "    models = list(w_dest[\"prediction\"].keys())\n",
    "    analysis.build_loss_df(group_by = \"freq\", models = models,  columnwise = False)\n",
    "    for i in range(len(analysis.experiment_lst)):\n",
    "        quick_plot(i, analysis, mean = True, rolling = 150)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_expers.experiment_lst[0][\"prediction\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_other_exper_W(random_expers, random_expers.experiment_lst[0], \n",
    "                  random_expers.experiment_lst[0], models = [\"random_exponential\"], models2avg = 100,\n",
    "                  random_seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(random_expers.experiment_lst)):\n",
    "#        quick_plot(i, random_expers, mean = False, rolling = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_dict = try_other_exper_W(random_expers2, random_expers2.experiment_lst[1], \n",
    "                  random_expers2.experiment_lst[1], models = [\"uniform\"], models2avg = 100,\n",
    "                  random_seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_expers2.experiment_lst)):\n",
    "        quick_plot(i, random_expers2, mean = True, rolling = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(trial = 1):\n",
    "    \"\"\"\n",
    "    get frequency lists\n",
    "    \"\"\"\n",
    "    if trial == 1:\n",
    "          lb_targ, ub_targ, obs_hz  = 210, 560, int(320 / 2)\n",
    "    elif trial == 2:\n",
    "          lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    elif trial == 3:\n",
    "          lb_targ, ub_targ, obs_hz  = 340, 350, 20#40\n",
    "    elif trial == 4:\n",
    "          lb_targ, ub_targ, obs_hz  = 60, 350, 40\n",
    "    elif trial == 5:\n",
    "          lb_targ, ub_targ, obs_hz  = 50, 200, 40\n",
    "    if trial == 6:\n",
    "          lb_targ, ub_targ, obs_hz  = 130, 530, 130\n",
    "    if trial == 7:\n",
    "          lb_targ, ub_targ, obs_hz  = 500, 900, 250\n",
    "    obs_list =  list( range( lb_targ - obs_hz, lb_targ))\n",
    "    obs_list += list( range( ub_targ, ub_targ + obs_hz))\n",
    "    resp_list = list( range( lb_targ, ub_targ))\n",
    "    return obs_list, resp_list\n",
    "get_frequencies(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_idx = random_expers.experiment_lst[1][\"resp_idx\"]\n",
    "print(\"resp_idx\",obs_idx)\n",
    "f = np.round(np.array(random_expers.experiment_lst[1][\"f\"]), 2)\n",
    "print(\"frequencies\", f[obs_idx])\n",
    "\n",
    "#print(\"frequencies\", f[range(143, 257)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_expers.experiment_lst[1][\"xTe\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_plot(1, random_expers, mean = True, outfile = \"150_500Hz_mean\")\n",
    "#quick_plot(1, random_expers, mean = False, outfile = \"150_500Hz\")\n",
    "#quick_plot(0, random_expers, mean = True, outfile = \"500_900Hz_mean\")\n",
    "#quick_plot(3, random_expers, mean = True, outfile = \"1000Hz_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_expers.experiment_lst)):\n",
    "    quick_plot(i, random_expers, mean = True, outfile = \"500_900Hz_mean\")\n",
    "    show_images(random_expers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quick_plot(3, random_expers, mean = False, force_build = True ,outfile = \"1000Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_expers.experiment_lst)):\n",
    "    \n",
    "    quick_plot(i, random_expers, mean = True, rolling = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dl_expers.experiment_lst)):\n",
    "    quick_plot(i, dl_expers, mean = True)\n",
    "    #show_images(dl_expers, exper_num = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dl_expers.hyper_parameter_plot()\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    #check out whats what\n",
    "    n = 1\n",
    "    hi = [dl_expers, random_expers]\n",
    "    quick_plot(n, hi, mean = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "def recursive_dict_combine(exper1, exper2):\n",
    "    for key1, value1 in exper1.items():\n",
    "        if type(exper1[key1]) == dict:\n",
    "            #print(key1)\n",
    "            exper1[key1] = {**exper1[key1], **exper2[key1]}\n",
    "            #print(exper1[key1])\n",
    "    return exper1\n",
    "def analysis_combine(analysis1_, analysis2_, n):\n",
    "    analysis1 = copy(analysis1_)\n",
    "    analysis2 = copy(analysis2_)\n",
    "    for i, exper in enumerate(analysis1.experiment_lst):\n",
    "        if i >= 2: \n",
    "            break\n",
    "        analysis1.experiment_lst[i] = recursive_dict_combine(dl_expers.experiment_lst[i], \n",
    "                                                             random_expers.experiment_lst[i])\n",
    "    return(analysis1)\n",
    "#if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "combined_analysis = analysis_combine(random_expers, dl_expers, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in combined_analysis.experiment_lst[1][\"prediction\"].items():\n",
    "    print(key)\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#save the combined analysis:\n",
    "#import pickle\n",
    "\n",
    "a = {'combined_analysis': combined_analysis}\n",
    "\n",
    "with open('final_results/combined_analysis.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "if not COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    with open('final_results/combined_analysis.pickle', 'rb') as handle:\n",
    "        combined_analysis = pickle.load(handle)[\"combined_analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_analysis = random_expers\n",
    "quick_plot(1, combined_analysis, outfile = \"150_500Hz\", mean = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quick_plot(1, combined_analysis, outfile = \"150_500Hz\", mean = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_analysis.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls experiment_results/medium/split_0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#show_images(random_expers, 1)\n",
    "\n",
    "g_truth = combined_analysis.experiment_lst[1][\"xTe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "xx, yy = combined_analysis.experiment_lst[1][\"xTe\"].shape\n",
    "i_coords, j_coords = np.meshgrid(range(xx), range(yy), indexing='ij')\n",
    "k_coords = (combined_analysis.experiment_lst[1][\"prediction\"][\"uniform\"] - g_truth) **2\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(i_coords, j_coords,  k_coords, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "xx, yy = combined_analysis.experiment_lst[1][\"xTe\"].shape\n",
    "i_coords, j_coords = np.meshgrid(range(xx), range(yy), indexing='ij')\n",
    "k_coords = combined_analysis.experiment_lst[1][\"prediction\"][\"ip: linear\"]# - g_truth) **2\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(i_coords, j_coords,  k_coords, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "xx, yy = combined_analysis.experiment_lst[1][\"xTe\"].shape\n",
    "i_coords, j_coords = np.meshgrid(range(xx), range(yy), indexing='ij')\n",
    "k_coords = combined_analysis.experiment_lst[1][\"xTe\"]\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(i_coords, j_coords,  k_coords, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "xx, yy = combined_analysis.experiment_lst[1][\"prediction\"][\"cyclic\"].shape\n",
    "i_coords, j_coords = np.meshgrid(range(xx), range(yy), indexing='ij')\n",
    "k_coords = combined_analysis.experiment_lst[1][\"prediction\"][\"cyclic\"]\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(i_coords, j_coords,  k_coords, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "residual = combined_analysis.experiment_lst[0][\"prediction\"][\"cyclic\"] - \n",
    "\n",
    "xx, yy = combined_analysis.experiment_lst[0][\"prediction\"][\"cyclic\"].shape\n",
    "i_coords, j_coords = np.meshgrid(range(xx), range(yy), indexing='ij')\n",
    "k_coords = combined_analysis.experiment_lst[0][\"prediction\"][\"cyclic\"]\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(i_coords, j_coords,  k_coords, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(i_coords, j_coords,  k_coords, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "Axes3D(fig = ax, X = i_coords, Y = j_coords, Z = k_coords)\n",
    "#plot_wireframe()\n",
    "##*args, **kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Prediction 2.0\n",
    "The key to the pure prediction is to break it down. We are going to take in a parameter k, which is the maximum time-series to include in a block prediction. Then we will break the target matrix A into component sections.\n",
    "\n",
    "This will simply require an appropriate series. The next step is to develop an appropriate method for combining the different matrices.\n",
    "\n",
    "There can be two options: overlapping and not overlapping. If there are over-lapping predictions we take a simple average. Otherwise we simply combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enablePrint()\n",
    "pickle_list = glob.glob('experiment_results/publish/*/*.pickle')\n",
    "pickle_list = pickle_list[1:]\n",
    "print(pickle_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if path list has duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pickle_list) == len(list(np.unique(pickle_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_A = EchoStateAnalysis(pickle_list, model = \"uniform\", ip_use_observers = True, ip_method = \"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following helper functions are assisting me to accomplish getting the final figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_A = dl_expers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_freqs = []\n",
    "for i, experiment in enumerate(pickle_A.experiment_lst):\n",
    "    tf = get_target_freq(experiment)\n",
    "    target_freqs.append(tf)\n",
    "    #print(experiment[\"resp_idx\"])\n",
    "    if IdxMatch(experiment):\n",
    "        freq_plot(pickle_A, experiment_num = i)\n",
    "    if IdxMatch(experiment, n_obs = 96):\n",
    "        freq_plot(pickle_A, experiment_num = i)\n",
    "target_freqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Zhizhuo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_idx = pickle_A.experiment_lst[1][\"resp_idx\"]\n",
    "print(resp_idx)\n",
    "f = np.array(pickle_A.experiment_lst[1][\"f\"])\n",
    "f[resp_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_plot(pickle_A, 2, title = \"Rena experiment: Avg. L2 Loss vs Frequency\", save = \"Rena_freq\")\n",
    "#time_plot(pickle_A, experiment_num = 2, rolling = 150, save = \"Rena_time\",\n",
    "#          title = \"Rena experiment: Avg. L2 Loss vs Time (rolling average)\")\n",
    "quick_plot(2, pickle_A, mean = True, rolling = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_A.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pickle_A.experiment_lst)):\n",
    "    freq_plot(pickle_A, i, mean = True)\n",
    "    time_plot(pickle_A, experiment_num = i, rolling = 150, mean = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_A.make_R_barplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle_rDF = pickle_A.rDF_time\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.5]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"L2_loss\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")\n",
    "\n",
    "pickle_rDF = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.7]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"R\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")\n",
    "\n",
    "pickle_rDF = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.9]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"R\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_pretty_pics(experiment_number = 0, \n",
    "                     modelz = [\"ip: linear\", \"uniform\",  \"exponential\"], #\"zhizhuo\",\n",
    "                    show_images = False, show_residuals = False\n",
    "                    ):\n",
    "    #blockPrint()\n",
    "    if experiment_number == 0:\n",
    "        zhizhuo_label = \"obs4\"\n",
    "    elif experiment_number == 4:\n",
    "        zhizhuo_label = \"obs5\"\n",
    "    else:\n",
    "        zhizhuo_label = \"experiment \" + str(experiment_number)\n",
    "        \n",
    "    if experiment_number == 0:\n",
    "        zhizhuo_label = \"low frequency\"\n",
    "    \n",
    "    spec = pickle_A.experiment_lst[experiment_number]\n",
    "    f =  spec[\"f\"]\n",
    "    freqs_dict = { idx : f[idx] for idx in spec[\"obs_idx\"]}\n",
    "    freqs_ = [f[idx] for idx in spec[\"resp_idx\"]]\n",
    "    \n",
    "    truth = spec[\"xTe\"]\n",
    "    if show_images:\n",
    "        plt.imshow(truth, aspect = 0.01)\n",
    "        plt.title(\"Ground truth\")\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(2,2, figsize = (12, 6))\n",
    "        ax = ax.flatten()\n",
    "        for i, model in enumerate(modelz):\n",
    "\n",
    "            ax[i].imshow(spec[\"prediction\"][model], aspect = 0.01)\n",
    "            ax[i].set_title(model)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    nrmses = []\n",
    "    residuals = []\n",
    "    for i, model in enumerate(modelz):\n",
    "        pred_ = spec[\"prediction\"][model]\n",
    "        #if model == \"zhizhuo\":\n",
    "        #    pred_ = np.flip(pred_, axis = 1)\n",
    "        nrmse_spec = nrmse(pred_, truth)\n",
    "        nrmses.append({model : nrmse_spec})\n",
    "        residuals.append(np.abs(truth - pred_))\n",
    "        \n",
    "    if show_residuals:\n",
    "        fig, ax = plt.subplots(2,2, figsize = (12, 6))\n",
    "        ax = ax.flatten()\n",
    "        for i, model in enumerate(modelz):\n",
    "            sns.heatmap(residuals[i], ax = ax[i])\n",
    "            ax[i].set_title(model + \" residuals^2, R: \" + str(round(nrmse_spec, 5)))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    palette_ = dict(zip(modelz, sns.color_palette(\"tab10\")[0:4]))\n",
    "    # = {\"uniform\": \"C0\", \"best interpolation\": \"C1\", \"zhizhuo\": \"C2\", \"expoenential\": \"k\"}\n",
    "    \n",
    "    nrmse_df = pd.DataFrame(nrmses)\n",
    "    nrmse_df = nrmse_df.melt()\n",
    "    nrmse_df.columns = [\"model\", \"R\"]\n",
    "    nrmse_df = nrmse_df.sort_values(by='R', ascending=True)\n",
    "    \n",
    "    modelz_ord = list(nrmse_df.model.values)\n",
    "    \n",
    "    #barplot\n",
    "    #fig, ax = plt.subplots(1,1, figsize = (12, 6.5))\n",
    "    display(nrmse_df)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    barplot = sns.barplot(x = \"model\", y = \"R\", data = nrmse_df, palette = palette_)\n",
    "    #pal.as_hex()\n",
    "    plt.title(\"RMSE for \" + zhizhuo_label)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.savefig('obs5_R.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "make_pretty_pics(0, show_residuals = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pickle_A.experiment_lst:\n",
    "    spec = i[\"resp_idx\"]\n",
    "    \n",
    "    if len(spec) > 1:\n",
    "        exp_resp_lst = (spec)\n",
    "        exp_f = i.keys()\n",
    "        print(exp_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "new_f = sio.loadmat(\"/Users/hayden/Desktop/f_new.mat\")\n",
    "new_f = new_f[\"f\"]\n",
    "\n",
    "freq_imp = [list(new_f[idx])[0] for idx in exp_resp_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f[exp_resp_lst[-1] +13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_results_df = pickle_A.R_results_df\n",
    "display(R_results_df)\n",
    "R_results_df_rel = pickle_A.R_results_df_rel\n",
    "R_results_df[\"experiment\"] = list(range(9)) * 3\n",
    "R_results_df_rel = R_results_df_rel[R_results_df_rel[\"model\"] != \"interpolation\"]\n",
    "R_results_df_rel[\"experiment\"] = list(range(9)) * 3\n",
    "display(R_results_df_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = \"model\", y = \"R\", data = R_results_df_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9, 2, figsize = (14, 25))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    R_df_spec = R_results_df[R_results_df.experiment == i]\n",
    "    R_df_rel_spec = R_results_df_rel[R_results_df_rel.experiment == i]\n",
    "    \n",
    "    #sns.barplot(x = \"model\", y = \"R\", data = R_df_spec)#, ax=ax[0])\n",
    "    R_df_spec =R_df_spec.drop(columns = \"experiment\")\n",
    "    R_df_rel_spec =R_df_rel_spec.drop(columns = \"experiment\")\n",
    "    \n",
    "    #sns.violinplot(x = \"model\", y = \"R\", data = self.R_results_df_rel, ax=ax[1])\n",
    "    sns.barplot(x = \"model\", y = \"R\", data = R_df_spec, ci = None, ax=ax[2*i])\n",
    "    \n",
    "    sns.barplot(x = \"model\", y = \"R\", data = R_df_rel_spec, ci = None, ax=ax[2*i+1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle_rDF = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.9]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"R\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining new pickle results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\".pickle\" in 'experiment_results/publish/split_0.5/block_N_Targidx_1N_Obsidx_4.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_A.experiment_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, glob the path lists from experiment results\n",
    "\n",
    "Why not try something asymmetric? Asymmetric exponential weights? Otherwise we will totally collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "\n",
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "medium_path_list = glob.glob('experiment_results/medium/*/*.txt')\n",
    "publish_path_list = glob.glob('experiment_results/publish/*/*.txt')\n",
    "publish_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## September 18th Task List:\n",
    "\n",
    "\n",
    "0) Continue to clean up analysis notebook\n",
    "1) Work to finish grading\n",
    "2) code asymmetric exponential weights\n",
    "3) Work more on biological kaggle problem\n",
    "4) Work on the paper (Rena parts)\n",
    "\n",
    "## Monday Tasks\n",
    "1) fix figure (Cycles 4 and 5) <br> \n",
    "2) check out the new biological kaggle problem (Cycle 3) <br> \n",
    "3) Select and extract (adjusted) indexes for block tests for Zhizhuo (Cycle 2) <br>\n",
    "4) Do the parts of the paper which Rena requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_f(path = '/Users/hayden/Downloads/f_3000.mat'):\n",
    "    zhiF = loadmat(path)\n",
    "    \n",
    "    \n",
    "    ff = list(zhiF[\"f\"].reshape(-1,))\n",
    "    print(ff[272])\n",
    "#\"/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/spectrogram_data/publish/f_new.mat\")\n",
    "get_f()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment session: relative R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rolling_rel_plot(n, rolling = 100, difference = False):\n",
    "    dictLst = []\n",
    "    hi = publish_sightIp.rDF\n",
    "    #display(set(hi.model))\n",
    "    #print(\"hi\")\n",
    "\n",
    "    sub_hi = hi[hi[\"experiment #\"] == n]\n",
    "    sub_hi_unif = sub_hi.R[sub_hi.model == \"uniform\"].values\n",
    "    sub_hi_ip = sub_hi.R[sub_hi.model == \"ip: linear\"].values\n",
    "    #we want to normalize, for the sake of comparison, the r values across the different models.\n",
    "    # divide by the sum of the ip\n",
    "    lenn = len(sub_hi_ip)\n",
    "\n",
    "    denominator = np.sum(sub_hi_ip) / lenn\n",
    "\n",
    "    sub_hi_unif = pd.Series(sub_hi_unif / denominator)\n",
    "    sub_hi_ip   = pd.Series(sub_hi_ip / denominator)\n",
    "    diff = sub_hi_unif - sub_hi_ip \n",
    "\n",
    "    dict_ = {\"uniform\" :  sub_hi_unif, \"ip\" : sub_hi_ip}\n",
    "\n",
    "    dictLst.append(dict_)\n",
    "    rolling_ip = sub_hi_ip.rolling(rolling).mean()\n",
    "    rolling_unif = sub_hi_unif.rolling(rolling).mean()\n",
    "    #print(np.mean(diff))\n",
    "    diff_roll = diff.rolling(rolling).mean()\n",
    "    xx = range(len(rolling_ip))\n",
    "    if difference:\n",
    "        color_ = \"green\" if np.mean(diff)<0 else \"red\"\n",
    "        sns.scatterplot(x = xx, y = diff,  color = color_, alpha = 0.01)\n",
    "        sns.lineplot(x = xx, y = diff_roll,  color = color_, alpha = 0.3)\n",
    "        plt.ylim(-5,5)\n",
    "        plt.title(\"Difference: rel unif - interpolation: > 0 -> rc doing better\")\n",
    "        return((np.mean(diff) < 0), len(diff))\n",
    "    else:\n",
    "        sns.lineplot(x = xx, y = rolling_ip,  color = \"red\", alpha = 0.3) #label = \"interpolation\",\n",
    "        sns.lineplot(x = xx, y = rolling_unif,  color = \"blue\", alpha = 0.3) #label = \"Uniform Random RC\",\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 5))\n",
    "better_90 = []\n",
    "better_50 = []\n",
    "for i in range(29):\n",
    "    try:\n",
    "        bet = rolling_rel_plot(i, rolling = 150, difference = True)\n",
    "        if bet[1] > 400:\n",
    "            better_50.append(bet[0])\n",
    "        else:\n",
    "            better_90.append(bet[0])\n",
    "    except:\n",
    "        print(i)\n",
    "def quality(better):\n",
    "    return(str(np.sum(better)/len(better)))\n",
    "print(quality(better_50))\n",
    "print(quality(better_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the nan interpolation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.kde_plots()\n",
    "publish_sightIp.experiment_lst[1].keys()\n",
    "\n",
    "hi = publish_sightIp.get_experiment(publish_sightIp.experiment_lst[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_div(exper_number, col_wise = False, plot = True, col_wise_method = \"freq\"):\n",
    "    \"\"\" Calculates KL Divergence #https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810\n",
    "    Assuming experiment_lst[0]\n",
    "    \"\"\"\n",
    "    xTe = np.array(publish_sightIp.experiment_lst[exper_number][\"xTe\"])\n",
    "    pred_test = publish_sightIp.experiment_lst[exper_number][\"prediction\"]\n",
    "   \n",
    "    #print(publish_sightIp.experiment_lst[0][\"nrmse\"])\n",
    "    def get_empirical_pdf_data(obj, plot = plot):\n",
    "        obj = obj.flatten()\n",
    "        nparam_density = stats.kde.gaussian_kde(obj)\n",
    "        x = np.linspace(-4, 3, 200)\n",
    "        nparam_density = nparam_density(x)\n",
    "        #ax.plot(x, nparam_density, 'r-', label='non-parametric density (smoothed by Gaussian kernel)')\n",
    "        if plot:\n",
    "            plt.hist(np.array(pred_test[\"uniform\"]).ravel(),  normed=True)\n",
    "            plt.plot(x, nparam_density, 'k--', label='non-parametric density')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return(nparam_density)\n",
    "\n",
    "    def kl_divergence(p, q):\n",
    "        return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "    if not col_wise: #col_number is related to frequency.\n",
    "        uniform_rc_epdf = get_empirical_pdf_data(obj = np.array(pred_test[\"uniform\"]))\n",
    "        linear_ip_epdf = get_empirical_pdf_data(obj = pred_test[\"ip: linear\"])\n",
    "        ground_truth_epdf = get_empirical_pdf_data(obj = xTe)\n",
    "        \n",
    "        kl_divergence_dict = {}\n",
    "        for key in pred_test.keys():\n",
    "            epdf_spec = get_empirical_pdf_data(np.array(pred_test[key]))\n",
    "            kl_divergence_dict[key] = kl_divergence(epdf_spec, ground_truth_epdf)\n",
    "        return(kl_divergence_dict)\n",
    "    else:\n",
    "        which_axis = 1 if col_wise_method == \"freq\" else 0\n",
    "        \n",
    "        kl_divergence_dict = {}\n",
    "        for key in pred_test.keys():\n",
    "            kl_divs_spec = []\n",
    "            for i in range(xTe.shape[which_axis]):\n",
    "                if col_wise_method == \"freq\":\n",
    "                    epdf_spec_i = get_empirical_pdf_data(np.array(pred_test[key])[:, i])\n",
    "                    ground_truth_epdf_i = get_empirical_pdf_data(obj = xTe[:, i])\n",
    "                else:\n",
    "                    epdf_spec_i = get_empirical_pdf_data(np.array(pred_test[key])[i, :])\n",
    "                    ground_truth_epdf_i = get_empirical_pdf_data(obj = xTe[i, :])\n",
    "                kl_spec_i = kl_divergence(epdf_spec_i, ground_truth_epdf_i)\n",
    "                kl_divs_spec.append(kl_spec_i)\n",
    "            kl_divergence_dict[key]=kl_divs_spec\n",
    "            \n",
    "        kl_divergence_df = pd.DataFrame(kl_divergence_dict)\n",
    "        kl_divergence_df = kl_divergence_df #.drop(columns = 'ip: linear')\n",
    "        \n",
    "        #rolling\n",
    "        if col_wise_method == \"time\":\n",
    "            print(\"rolling\")\n",
    "            for col in list(kl_divergence_df.columns):\n",
    "                print(col)\n",
    "                print(kl_divergence_df[col])\n",
    "                mean_ = kl_divergence_df[col].rolling(5).median()\n",
    "                kl_divergence_df[col] = mean_\n",
    "        \n",
    "        len_df = len(kl_divergence_df)\n",
    "        kl_divergence_df = kl_divergence_df.melt()\n",
    "        kl_divergence_df.columns = [\"model\", \"kl divergence\"]\n",
    "        kl_divergence_df[\"freq_idx\"] = list(range(len_df))*len(kl_divergence_df.model.unique())\n",
    "        sns.lineplot(x = \"freq_idx\", y = \"kl divergence\", data = kl_divergence_df, hue = \"model\")\n",
    "        plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    kl_spec = get_kl_div(i, col_wise = True, plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad kl-divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_lst= []\n",
    "for i in trange(2):\n",
    "    kl_spec = get_kl_div(i, plot = False)\n",
    "    print(kl_spec)\n",
    "    kl_div_lst.append(kl_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_pd = pd.DataFrame(kl_div_lst)\n",
    "kl_div_pd = kl_div_pd#.drop(columns = [\"best interpolation\", \"zhizhuo\"])\n",
    "kl_div_pd = kl_div_pd.melt()\n",
    "kl_div_pd.columns = [\"model\", \"kl divergence\"]\n",
    "sns.swarmplot(x = \"model\", y = \"kl divergence\", data = kl_div_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(len(hi.f))\n",
    "\n",
    "\n",
    "new_T = np.arange(min(hi.T),max(hi.T), step = 1/2751.5)\n",
    "assert len(new_T) == len(hi.f)\n",
    "T_dict = {\"T\": new_T}\n",
    "savemat(\"new_T.mat\", T_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = sns.color_palette().as_hex()\n",
    "list(np.array(o)[[2,3,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs4 = publish_sightIp.experiment_lst[0]\n",
    "obs4_dictt = {\"experiment\": obs4}\n",
    "\n",
    "with open('obs4.pickle', 'wb') as handle:\n",
    "    pickle.dump(obs4_dictt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pretty_pics(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arr1 = np.array(publish_sightIp.experiment_lst[0][\"prediction\"][\"uniform\"])\n",
    "arr1.shape\n",
    "arr2 = np.array(publish_sightIp.experiment_lst[4][\"prediction\"][\"uniform\"])\n",
    "\n",
    "rc1dict = {\"rc_pred\" : arr1,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[0][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[0][\"xTe\"]) }\n",
    "rc2dict = {\"rc_pred\" : arr2,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[1][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[1][\"xTe\"]) }\n",
    "savemat(\"rc1.mat\", rc1dict)\n",
    "savemat(\"rc2.mat\", rc2dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zhizhuo_freqs(nn = 1):\n",
    "    for n in range(nn):\n",
    "    \n",
    "        #print(publish_sightIp.experiment_lst[n][\"nrmse\"])\n",
    "        split_ = publish_sightIp.experiment_lst[n][\"get_observer_inputs\"][\"split\"]\n",
    "        if split_ == 0.5:\n",
    "            nrmse_dict = publish_sightIp.experiment_lst[n][\"nrmse\"]\n",
    "            for key in nrmse_dict.keys():\n",
    "                nrmse_dict[key] = np.round(nrmse_dict[key], 3)\n",
    "\n",
    "            print(n)\n",
    "            print(nrmse_dict)\n",
    "\n",
    "\n",
    "            resp_idx_spec = publish_sightIp.experiment_lst[n][\"resp_idx\"]\n",
    "            xTe = publish_sightIp.experiment_lst[n][\"xTe\"]\n",
    "            missing_frequencies = [np.round(hi.f[idx],1) for idx in resp_idx_spec]\n",
    "            print(missing_frequencies[0])\n",
    "            print(missing_frequencies[-1])\n",
    "\n",
    "            matlab_resp_idxs = [idx + 1 for idx in resp_idx_spec]\n",
    "            \n",
    "            print(\"response indices: \" + str(matlab_resp_idxs[0]) + \" \" + str(matlab_resp_idxs[-1]))\n",
    "            print(\"number of f missing lines: \" + str(len(matlab_resp_idxs)))\n",
    "\n",
    "            print(\"T's: (\" + str(hi.xTe.shape[0]) + \", \" + str(hi.A.shape[0]) + \")\")\n",
    "\n",
    "            obs_idx_spec = publish_sightIp.experiment_lst[n][\"obs_idx\"] \n",
    "            matlab_obs_idxs = [idx + 1 for idx in obs_idx_spec]\n",
    "            print(\"total observers: \" + str(len(matlab_obs_idxs)))\n",
    "    \n",
    "    #return(matlab_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_zhizhuo_freqs()\n",
    "get_zhizhuo_freqs(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array(publish_sightIp.experiment_lst[0][\"prediction\"][\"uniform\"])\n",
    "arr1.shape\n",
    "arr2 = np.array(publish_sightIp.experiment_lst[13][\"prediction\"][\"uniform\"])\n",
    "arr2.shape\n",
    "publish_sightIp.experiment_lst[13][\"nrmse\"]\n",
    "\n",
    "rc1dict = {\"rc_pred\" : arr1,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[0][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[0][\"xTe\"]) }\n",
    "rc2dict = {\"rc_pred\" : arr2,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[13][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[13][\"xTe\"]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "arr1 = np.array(publish_sightIp.experiment_lst[0][\"prediction\"][\"uniform\"])\n",
    "arr1.shape\n",
    "arr2 = np.array(publish_sightIp.experiment_lst[13][\"prediction\"][\"uniform\"])\n",
    "arr2.shape\n",
    "publish_sightIp.experiment_lst[13][\"nrmse\"]\n",
    "\n",
    "rc1dict = {\"rc_pred\" : arr1,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[0][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[0][\"xTe\"]) }\n",
    "rc2dict = {\"rc_pred\" : arr2,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[13][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[13][\"xTe\"]) }\n",
    "savemat(\"rc1.mat\", rc1dict)\n",
    "savemat(\"rc2.mat\", rc2dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(publish_sightIp.experiment_lst[0][\"xTe\"]), aspect = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#publish_sightIp.loss_plot(split = 0.5, rolling = 40)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_50_50_split.R.rolling(50).mean(),\n",
    "             data = blindIP_loss_df_50_50_split, hue = \"model\", alpha = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blindIP_loss_df = publish_sightIp.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_90_10_split = publish_sightIp.rDF[publish_sightIp.rDF.split == 0.9]\n",
    "#blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"exponential\"] = blindIP_loss_df_50_50_split\n",
    "sns.scatterplot(x = \"time\", y = \"R\", data = blindIP_loss_df_90_10_split, hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_90_10_split.R.rolling(50).mean(),\n",
    "             data = blindIP_loss_df_90_10_split, hue = \"model\", alpha = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhizhuo results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "files2import = glob.glob('/Users/hayden/Desktop/zhizhuo_block_results/*.mat')\n",
    "files2import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = []\n",
    "for i in files2import:\n",
    "    data_lst.append(loadmat(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real51, y_hat101, y_real101, y_hat51 = data_lst\n",
    "truth_51 = y_real51[list(y_real51.keys())[3]]\n",
    "pred_51  = y_hat51[list(y_hat51.keys())[3]]\n",
    "print(nrmse(pred_51, truth_51))\n",
    "plt.imshow(truth_51, aspect = 4)\n",
    "plt.show()\n",
    "plt.imshow(pred_51, aspect = 4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_freqs_51 = list(range(1740, 2240 + 10, 10)) \n",
    "obs_freqs51a = list(range(1740  - (26*10), 1740, 10))\n",
    "obs_freqs51b = list(range(2240 + 10, 2240 + (26*10) + 10, 10))\n",
    "assert targ_freqs_51[-1] + 10 == obs_freqs51b[0]\n",
    "assert targ_freqs_51[0] - 10 == obs_freqs51a[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "truth_101 = y_real101[list(y_real101.keys())[3]]\n",
    "pred_101  = y_hat101[list(y_hat101.keys())[3]]\n",
    "print(nrmse(pred_101, truth_101))\n",
    "plt.imshow(truth_101, aspect = 4)\n",
    "plt.show()\n",
    "plt.imshow(pred_101, aspect = 4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "zhizhuo51 = EchoStateExperiment(\"medium\", obs_freqs = obs_freqs51a + obs_freqs51b, target_freqs = targ_freqs_51 )\n",
    "zhizhuo51.f[zhizhuo51.resp_idx[0]]\n",
    "#print(zhizhuo51.A.shape)\n",
    "#for i in zhizhuo51.A.shape[1]\n",
    "hi = \"\"\"\n",
    "for i in range(0,1000):\n",
    "    his = truth_51.T[0:]\n",
    "    mine = zhizhuo51.A[:, i ] #511:\n",
    "    assert his.shape == mine.shape, str(his.shape) + str(mine.shape)\n",
    "    if np.array_equal(his,mine):\n",
    "        print(i)\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_zhizhuo_series(series_idx):\n",
    "    mine = zhizhuo51.A[ 511:, 153 ]\n",
    "    his = truth_51[series_idx,:]\n",
    "    nrmse_lst = []\n",
    "    for i in range(0, 1000):\n",
    "        mine = zhizhuo51.A.T[ i , 511: ]\n",
    "        nrmse_spec = nrmse(his,mine)\n",
    "\n",
    "        nrmse_lst.append(nrmse_spec)\n",
    "    nrmse_series = pd.Series(nrmse_lst)\n",
    "    candidate_idx = nrmse_series.idxmin()\n",
    "    candidate = zhizhuo51.A.T[candidate_idx , 511: ]\n",
    "    plt.plot(candidate, \"--\", linewidth = 5, alpha = 0.6, label = \"actual data\") \n",
    "    plt.plot(his, \":r\", label = \"zhizhuo testset\", linewidth = 3, alpha = 0.5)\n",
    "    titl = str(candidate_idx) + \" idx: \" +  str(zhizhuo51.f[candidate_idx]) + \" Hz\"\n",
    "    plt.title(titl)\n",
    "    plt.legend()\n",
    "    print(truth_51.shape)\n",
    "retrieve_zhizhuo_series(0)\n",
    "plt.show()\n",
    "retrieve_zhizhuo_series(-1)\n",
    "plt.show()\n",
    "plt.plot(zhizhuo51.A[511:, 249])\n",
    "\n",
    "#plt.plot(zhizhuo51.A[174, :])\n",
    "#zhizhuo51.get_observers(split = 0.4995, method = \"exact\", plot_split = True)\n",
    "#zhizhuo51.runInterpolation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert zhizhuo51.xTe.T.shape == truth_51.shape, str(zhizhuo51.xTe.T.shape) + \" != \" + str(truth_51.shape)\n",
    "sns.heatmap(truth_51)\n",
    "plt.show()\n",
    "sns.heatmap(zhizhuo51.xTe.T)\n",
    "plt.show() \n",
    "plt.imshow(pred_51, aspect = 4)\n",
    "plt.show() \n",
    "\n",
    "plt.imshow(zhizhuo51.ip_res[\"prediction\"].T, aspect = 4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(zhizhuo51.xTe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Zhizhuo block results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Step 2: store hyper-parameter-results: Let's get some nice hyper-parameter plots.\n",
    "#TODO: Step 1: check if observers are correct:\n",
    "#TODO: fix\n",
    "\n",
    "\n",
    "def check_shape_obs(file = \"default\"):\n",
    "    \"\"\"\n",
    "    Check the shape\n",
    "    \"\"\"\n",
    "    if file == \"default\":\n",
    "        nf = get_new_filename(exp = exp, current = True)\n",
    "    else:\n",
    "        nf = file\n",
    "    with open(nf) as json_file: # 'non_exp_w.txt'\n",
    "        datt = json.load(json_file)\n",
    "    #datt = non_exp_best_args[\"dat\"]\n",
    "    #datt[\"obs_tr\"], datt[\"obs_te\"]   = np.array(datt[\"obs_tr\"]), np.array(datt[\"obs_te\"])\n",
    "    #datt[\"resp_tr\"], datt[\"resp_te\"] = np.array(datt[\"resp_tr\"]), np.array(datt[\"resp_te\"])\n",
    "    return(datt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#experiment.save_json(exp = False)\n",
    "#fp = bp + 'targetKhz:_0.01__obskHz:_0.01.txt'\n",
    "#fp = bp + 'targetKhz:_0.02__obskHz:_0.01.txt'\n",
    "def topline(spec_path, \n",
    "            base_path = \"/Users/hayden/Desktop/experiment_results/2k/medium/\",\n",
    "            #base_path = #\"./experiment_results/...\"\n",
    "            verbose = False,\n",
    "            print_filestructure = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(base_path)\n",
    "    fp = base_path + spec_path\n",
    "    \n",
    "    hi = load_data(file = fp)\n",
    "    if print_filestructure == True:\n",
    "        for i in hi.keys():\n",
    "            print(i + \"/\")\n",
    "\n",
    "            if type(hi[i]) == dict:\n",
    "\n",
    "                for j in hi[i].keys():\n",
    "                    print(\"    \" + j)\n",
    "                    \n",
    "    if verbose == True:\n",
    "        print(\"DATA STRUCTURE: (it's a dict)\")\n",
    "        print(\"/n inputs:\")\n",
    "        print(hi[\"experiment_inputs\"])\n",
    "        print(hi[\"get_observer_inputs\"])\n",
    "\n",
    "        print(\"/n key saved values:\")\n",
    "        print(hi[\"best arguments\"])\n",
    "        print(hi[\"nrmse\"])\n",
    "    return(hi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifdel(dictt, key):\n",
    "    \"\"\" If a key is in a dictionary delete it. Return [modified] dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del dictt[key]\n",
    "        return(dictt)\n",
    "    except:\n",
    "        return(dictt)\n",
    "ifdel({\"a\":1, \"b\" : 2}, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(check_for_duplicates(complete_experiment_path_lst) != True), \"duplicates found\"\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "bp_ = \"\"# \"./experiment_results/\"\n",
    "\n",
    "\n",
    "\n",
    "# fix nrmse calculation AND interpolation\n",
    "for i in trange(len(experiment_lst), desc='experiment list, fixing interpolation...'): \n",
    "    exper_ = experiment_lst[i]\n",
    "    exper_obj = get_experiment(exper_)\n",
    "    \n",
    "    train_set, test_set = exper_obj.xTr, exper_obj.xTe\n",
    "    models_spec = list(exper_[\"prediction\"].keys())\n",
    "    \n",
    "    for model_ in models_spec:\n",
    "        pred_ = exper_[\"prediction\"][model_]\n",
    "        corrected_nrmse = nrmse(pred_, test_set)\n",
    "        exper_[\"nrmse\"][model_] = corrected_nrmse\n",
    "        \n",
    "    experiment_lst[i] = fix_interpolation(exper_, method = \"linear\") \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_combination(ip, exp, Test, n = 10, optimize = True):\n",
    "    \n",
    "    if optimize == True:\n",
    "        nrmses = []\n",
    "        predictions = []\n",
    "        #(np.array(Test) + np.array(predictions[\"exponential\"])) / 2\n",
    "        vals =[]\n",
    "        for i in range(n):\n",
    "            a = i / n\n",
    "            b = 1 - a\n",
    "            hybrid_pred = ((1-a) * ip + a * exp)\n",
    "            predictions += [hybrid_pred]\n",
    "            nrmses += [nrmse(hybrid_pred , Test) ]\n",
    "            vals += [a]\n",
    "            #nrmse(predictions[\"hybrid\"], Test) \n",
    "        idx = np.argmin(nrmses) \n",
    "        print(nrmses)\n",
    "        print(\"A!! \" + str(vals[idx]))\n",
    "        best_prediction = predictions[idx]\n",
    "        best_nrmse      = nrmses[idx]\n",
    "        return(best_prediction, best_nrmse)\n",
    "    else:\n",
    "        hybrid_pred  = (0.5 * ip) + (0.5 * exp)\n",
    "        hybrid_nrmse = nrmse(hybrid_pred , Test)\n",
    "        return(hybrid_pred, hybrid_nrmse)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add hybrids\n",
    "for i in list(range(len(experiment_lst))):\n",
    "    experiment_lst[i]\n",
    "    predictions_= experiment_lst[i][\"prediction\"]\n",
    "    Train, Test = recover_test_set(experiment_lst[i])\n",
    "    #hybrid_pred_, hybrid_R = optimize_combination(np.array(predictions_[\"interpolation\"]),\n",
    "    #                                                                  np.array(predictions_[\"exponential\"]),\n",
    "    #                                                                  Test, optimize = False)\n",
    "    experiment_lst[i][\"nrmse\"][\"hybrid\"]      = hybrid_R\n",
    "    experiment_lst[i][\"prediction\"][\"hybrid\"] = hybrid_pred_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df():\n",
    "    IGNORE_IP = False\n",
    "\n",
    "    def quick_dirty_convert(lst):\n",
    "        if IGNORE_IP == True:\n",
    "            lst *= 2\n",
    "        else:\n",
    "            lst *= 4\n",
    "        pd_ = pd.DataFrame(np.array(lst).reshape(-1,1))\n",
    "        return(pd_)\n",
    "\n",
    "\n",
    "    idx_lst = list(range(len(experiment_lst)))\n",
    "    #idx_lst *= 3\n",
    "    #idx_lst = pd.DataFrame(np.array(idx_lst).reshape(-1,1))\n",
    "\n",
    "    idx_lst = quick_dirty_convert(idx_lst)\n",
    "\n",
    "    obs_hz_lst, targ_hz_lst, targ_freq_lst = [], [], []\n",
    "\n",
    "    for i, experiment in enumerate(experiment_lst):\n",
    "        #print(experiment['experiment_inputs'].keys())\n",
    "        targ_hz = experiment[\"experiment_inputs\"][\"target_hz\"]\n",
    "        obs_hz  = experiment[\"experiment_inputs\"][\"obs_hz\"]\n",
    "        targ_freq = experiment[\"experiment_inputs\"]['target_frequency']\n",
    "\n",
    "        if experiment[\"experiment_inputs\"][\"target_hz\"] < 1:\n",
    "            targ_hz *= 1000*1000\n",
    "            obs_hz  *= 1000*1000\n",
    "        obs_hz_lst  += [obs_hz]\n",
    "        targ_hz_lst += [targ_hz]\n",
    "        targ_freq_lst += [targ_freq]\n",
    "\n",
    "\n",
    "        hz_line = {\"target hz\" : targ_hz }\n",
    "        hz_line = Merge(hz_line , {\"obs hz\" : obs_hz })\n",
    "\n",
    "        #print(hz_line)\n",
    "        df_spec= experiment[\"nrmse\"]\n",
    "\n",
    "        #df_spec = Merge(experiment[\"nrmse\"], {\"target hz\": targ_hz})\n",
    "        df_spec = pd.DataFrame(df_spec, index = [0])\n",
    "\n",
    "        df_spec_rel = df_spec.copy()\n",
    "        #/df_spec_diff[\"uniform\"]\n",
    "        #df_spec_diff[\"rc_diff\"]\n",
    "\n",
    "        if IGNORE_IP == True:\n",
    "            df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"uniform\"]#\n",
    "        else:\n",
    "            df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"interpolation\"]\n",
    "\n",
    "\n",
    "\n",
    "        #print( df_spec_rel)\n",
    "        #print(experiment[\"experiment_inputs\"].keys())\n",
    "        if i == 0:\n",
    "            df      = df_spec\n",
    "            df_rel  = df_spec_rel\n",
    "\n",
    "\n",
    "        else:\n",
    "            df = pd.concat([df, df_spec])\n",
    "            df_rel = pd.concat([df_rel, df_spec_rel])\n",
    "\n",
    "\n",
    "    df_net = df_rel.copy()\n",
    "\n",
    "    obs_hz_lst, targ_hz_lst = quick_dirty_convert(obs_hz_lst), quick_dirty_convert(targ_hz_lst)\n",
    "    targ_freq_lst = quick_dirty_convert(targ_freq_lst)\n",
    "    #display(df)\n",
    "    if IGNORE_IP == True:\n",
    "        df_rel = df_rel.drop(columns = [\"interpolation\"])\n",
    "        df  = df.drop(columns = [\"interpolation\"])\n",
    "    #df_rel  = df_rel.drop(columns = [\"hybrid\"])\n",
    "    #df      = df.drop(    columns = [\"hybrid\"])\n",
    "\n",
    "    df, df_rel = pd.melt(df), pd.melt(df_rel)\n",
    "    df  = pd.concat( [idx_lst, df,  obs_hz_lst, targ_hz_lst, targ_freq_lst] ,axis = 1)\n",
    "\n",
    "    df_rel = pd.concat( [idx_lst, df_rel,  obs_hz_lst, targ_hz_lst, targ_freq_lst], axis = 1)\n",
    "\n",
    "    #df_diff = pd.concat( [idx_lst, df_diff,  obs_hz_lst, targ_hz_lst, targ_freq_lst], axis = 1)\n",
    "\n",
    "    col_names = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\", \"target freq\" ]\n",
    "    df.columns, df_rel.columns    = col_names, col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df_diff = df[df[\"model\"] == \"uniform\"]\n",
    "df_diff.model = \"diff\"\n",
    "nrmse_ = (df[df[\"model\"] == \"exponential\"][\"nrmse\"].values - df_diff[\"nrmse\"].values) * 100\n",
    "df_diff.nrmse = nrmse_\n",
    "def plot_loss_reduction():\n",
    "\n",
    "    df_diff = df[df[\"model\"] == \"uniform\"]\n",
    "    df_diff.model = \"diff\"\n",
    "    #df_diff[\"nrmse\"] = df_diff[\"nrmse\"] - df[df[\"model\"] == \"exponential\"][\"nrmse\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    #df[df[\"model\"] == \"exponential\"] \n",
    "\n",
    "    nrmse_ = (df[df[\"model\"] == \"exponential\"][\"nrmse\"].values - df_diff[\"nrmse\"].values) * 100\n",
    "    df_diff.nrmse = nrmse_\n",
    "    pct = round(np.mean(nrmse_ < 0) * 100,2)\n",
    "    print(\"odds of loss reduction with exponential weights vs uniform weights: \" + str(pct) + \"%\")\n",
    "    print(\"mean % loss change: \" + str(round(np.mean(nrmse_))) + \"%\")\n",
    "\n",
    "    #sns.catplot(x = \"model\", y = \"nrmse\", data = df_diff)\n",
    "    fig, ax = plt.subplots(1,1,figsize = (10, 6))\n",
    "    plt.xlabel(\"%change in loss\")\n",
    "    plt.ylabel(\"density\")\n",
    "    sns.kdeplot(df_diff[\"nrmse\"], shade = True)\n",
    "    plt.axvline(x=0, color = \"black\", label = \"zero\")\n",
    "    plt.axvline(x=np.mean(df_diff[\"nrmse\"]), color = \"red\", label = \"mean loss reduction\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_loss_reduction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_reduction2d(xx = \"target hz\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (6,6))\n",
    "    sns.kdeplot(df_diff[xx], df_diff[\"nrmse\"],\n",
    "                     cmap=\"Blues\", shade=True, shade_lowest=False, ax = ax)#, alpha = 0.5)\n",
    "    #plt.ayvline(y=0, color = \"black\", label = \"zero\")\n",
    "    sns.scatterplot(x = xx, y = \"nrmse\", data = df_diff, ax = ax,  linewidth=0, color = \"black\", alpha = 0.4)\n",
    "    plt.title(\"2d kde plot: nrmse vs target hz\")\n",
    "    plt.axhline(y=0.5, color='black', linestyle='-')\n",
    "    ax.set_ylabel(\"pct loss exp vs unif RC\")\n",
    "plot_loss_reduction2d()\n",
    "plot_loss_reduction2d(xx = \"obs hz\")\n",
    "plot_loss_reduction2d(xx = \"target freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nrmse_kde_2d(xx = \"target hz\", \n",
    "                      log = True, \n",
    "                      alph = 1, \n",
    "                      black_pnts = True, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"},\n",
    "                      enforce_bounds = False,\n",
    "                      target_freq = None):\n",
    "    \"\"\"\n",
    "    #todo description\n",
    "    \"\"\"\n",
    "    if target_freq != None:\n",
    "        df_spec = df[df[\"target freq\"] == target_freq]\n",
    "    else:\n",
    "        df_spec = df.copy()\n",
    "            \n",
    "    \n",
    "    def plot_(model_, colorr, alph = alph,  black_pnts =  black_pnts):\n",
    "        if colorr == \"Blues\":\n",
    "            color_ = \"blue\"\n",
    "        elif colorr == \"Reds\":\n",
    "            color_ = \"red\"\n",
    "        elif colorr == \"Greens\":\n",
    "            color_ = \"green\"\n",
    "            \n",
    "        df_ = df_spec[df_spec.model == model_] #df_ip  = df[df.model == \"interpolation\"]\n",
    "        \n",
    "        #display(df_)\n",
    "            \n",
    "        \n",
    "        hi = df_[\"nrmse\"]\n",
    "        cap = 1\n",
    "        if log == True:\n",
    "            hi = np.log(hi)/ np.log(10)\n",
    "            cap = np.log(cap) / np.log(10)\n",
    "        \n",
    "        \n",
    "        sns.kdeplot(df_[xx], hi, cmap= colorr, \n",
    "                    shade=True, shade_lowest=False, ax = ax, label = model_, alpha = alph)#, alpha = 0.5)\n",
    "        \n",
    "        if  black_pnts == True:\n",
    "            col_scatter = \"black\"\n",
    "        else:\n",
    "            col_scatter = color_\n",
    "        \n",
    "        sns.scatterplot(x = xx, y = hi, data = df_,  linewidth=0, \n",
    "                        color = col_scatter, alpha = 0.4, ax = ax)\n",
    "        \n",
    "        plt.title(\"2d kde plot: nrmse vs \" + xx)\n",
    "        \n",
    "        plt.axhline(y=cap, color=color_, linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "        sns.lineplot(y = hi, x = xx, data = df_ , color = color_)#, alpha = 0.2)\n",
    "        if enforce_bounds == True:\n",
    "            ax.set_ylim(0,1)\n",
    "        if log == True:\n",
    "            ax.set_ylabel(\"log( NRMSE) \")\n",
    "        else: \n",
    "            ax.set_ylabel(\"NRMSE\")\n",
    "            \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12,6))\n",
    "    for model in list(models.keys()):\n",
    "        print(model)\n",
    "        plot_(model, models[model], alph = alph)\n",
    "    #plot_(\"interpolation\", \"Blues\")\n",
    "    #plot_(\"exponential\", \"Reds\", alph = alph)\n",
    "    \n",
    "def kde_plots( target_freq = None, \n",
    "               log = False, \n",
    "               model = \"uniform\", \n",
    "               models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"},\n",
    "               enforce_bounds = True,\n",
    "               split = None):\n",
    "    \"\"\"\n",
    "    HEATMAP EXAMPLE:\n",
    "                     enforce_bounds = True)\n",
    "    flights = flights.pivot(\"month\", \"year\", \"passengers\") #y, x, z\n",
    "    ax = sns.heatmap(flights)\n",
    "    plot_nrmse_kde_2d(**additional_arguments, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"})\n",
    "    \n",
    "    plot_nrmse_kde_2d(xx = \"obs hz\", **additional_arguments, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"})\n",
    "    \"\"\"\n",
    "    \n",
    "    additional_arguments ={ \"black_pnts\" : False, \n",
    "                           \"alph\" : 0.3, \n",
    "                           \"target_freq\" : target_freq}    \n",
    "    \n",
    "    cmap = \"coolwarm\"\n",
    "    \n",
    "   \n",
    "    def add_noise(np_array, log = log):\n",
    "        sizee = len(np_array)\n",
    "        x =  np.random.randint(100, size = sizee) + np_array \n",
    "        \n",
    "        return(x)\n",
    "    \n",
    "    nrmse_dict = {}\n",
    "    \n",
    "    for i, model in enumerate([\"uniform\", \"exponential\", \"interpolation\"]):\n",
    "        df_ = df[df.model == model ]\n",
    "        \n",
    "        xx, yy = add_noise(df_[\"target hz\"]), add_noise(df_[\"obs hz\"])\n",
    "\n",
    "        nrmse= df_[\"nrmse\"]\n",
    "        if log == True:\n",
    "            print(\"hawabunga\")\n",
    "            nrmse = np.log(nrmse)\n",
    "        nrmse_dict[model] = nrmse\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    nrmse_diff = nrmse_dict[\"exponential\"].values.reshape(-1,)  - nrmse_dict[\"uniform\"].values.reshape(-1,) \n",
    "    print(\"(+): \" + str(np.sum((nrmse_diff > 0)*1)))\n",
    "    \n",
    "    print(\"(-): \" + str(np.sum((nrmse_diff < 0)*1)))\n",
    "    \n",
    "    \n",
    "    display(nrmse_diff)\n",
    "    xx, yy = add_noise(df_[\"target hz\"]), add_noise(df_[\"obs hz\"])\n",
    "    #sns.distplot(nrmse_diff, ax = ax[2])\n",
    "    sns.scatterplot(x = xx, y = yy, data = df_, ax = ax[2], palette=cmap, alpha = 0.9, s = 50, hue = nrmse_diff) #size = nrmse,\n",
    "    ax[2].set_title(\" diff: exponential - uniform\" )\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_nrmse_kde_2d(**additional_arguments, log = False, \n",
    "                      models = models, #{\"exponential\" : \"Reds\", \"uniform\" : \"Blues\", \"interpolation\" : \"Greens\"},\n",
    "                     enforce_bounds = True)\n",
    "    \n",
    "    \n",
    "    plot_nrmse_kde_2d(xx = \"obs hz\", **additional_arguments, log = False, \n",
    "                       models = models, #{\"exponential\" : \"Reds\", \"uniform\" : \"Blues\", \"interpolation\" : \"Greens\"},\n",
    "                       enforce_bounds = True)\n",
    "    \n",
    "               \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "kde_plots(models = {\"interpolation\" : \"Greens\", \"hybrid\" : \"Reds\"})#, \"uniform\" : \"Blues\"},)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plots(models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Blues\", \"uniform\" : \"Reds\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    " \n",
    "def optimize_combination(ip, exp, Test, n = 10, optimize = True):\n",
    "    \n",
    "    if optimize == True:\n",
    "        nrmses = []\n",
    "        predictions = []\n",
    "        #(np.array(Test) + np.array(predictions[\"exponential\"])) / 2\n",
    "        vals =[]\n",
    "        for i in range(n):\n",
    "            a = i / n\n",
    "            b = 1 - a\n",
    "            hybrid_pred = ((1-a) * ip + a * exp)\n",
    "            predictions += [hybrid_pred]\n",
    "            nrmses += [nrmse(hybrid_pred , Test) ]\n",
    "            vals += [a]\n",
    "            #nrmse(predictions[\"hybrid\"], Test) \n",
    "        idx = np.argmin(nrmses) \n",
    "        print(nrmses)\n",
    "        print(\"A!! \" + str(vals[idx]))\n",
    "        best_prediction = predictions[idx]\n",
    "        best_nrmse      = nrmses[idx]\n",
    "        return(best_prediction, best_nrmse)\n",
    "    else:\n",
    "        hybrid_pred  = (0.5 * ip) + (0.5 * exp)\n",
    "        hybrid_nrmse = nrmse(hybrid_pred , Test)\n",
    "        return(hybrid_pred, hybrid_nrmse)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[1])\n",
    "ax[0].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[1].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[0].set_ylabel(\"NRMSE\"); ax[1].set_ylabel(\"NRMSE\")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax[0].set_title(\"Relative NRMSE vs Interpolation model across different RC's\")\n",
    "ax[1].set_title(\"Relavite NRMSE vs Interpolation model across different RC's\")\n",
    "ax[0].set_ylabel(\"Relative NRMSE\"); ax[1].set_ylabel(\"Relative NRMSE\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (7,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_diff, ax = ax)\n",
    "#sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax.set_title(\"Relative NRMSE: ([exp nrmse] -  [unif nrmse])/[unif_nrmse] * 100\")\n",
    "ax.set_ylabel(\"Relative NRMSE\"); ax.set_ylabel(\"Relative NRMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = df[\"nrmse\"] #df[\"nrmse\"]np.log(df[\"nrmse\"])\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "cap = np.log(1.0)/np.log(10)\n",
    "plt.axhline(y=cap, color=\"black\", linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "plt.ylabel(\"log( NRMSE)\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "cap = np.log(1.0)/np.log(10)\n",
    "plt.axhline(y=cap, color=\"black\", linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "plt.ylabel(\"log( NRMSE)\")\n",
    "plt.ylim((-1.5,cap))\n",
    "\n",
    "\n",
    "\n",
    "hi = df[\"nrmse\"]\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "plt.ylabel(\"NRMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = np.log(df[\"nrmse\"])\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"obs hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "sns.lineplot( y = hi, x = \"obs hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "ax.set_ylabel(\"Log ( NRMSE )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target freq\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "plt.ylim((0,1.5))\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target freq\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, experiment in enumerate(experiment_lst):\n",
    "    #print(experiment['get_observer_inputs'].keys())\n",
    "    split = experiment['get_observer_inputs'][\"split\"]\n",
    "    targ_hz = experiment['experiment_inputs'][\"target_hz\"]\n",
    "    targ_idx_LB, targ_idx_UB = experiment[\"resp_idx\"][0], experiment[\"resp_idx\"][-1]\n",
    "    obs_hz = experiment['experiment_inputs'][\"obs_hz\"]\n",
    "    f = np.array(experiment_8_obj.f)\n",
    "    obs_idx = experiment[\"obs_idx\"] \n",
    "\n",
    "    obs_idx  = [int(j) for j in experiment[\"obs_idx\"] ]\n",
    "    obs_freq = [max(f) - f[j] for j in obs_idx]\n",
    "    \n",
    "    \n",
    "    print(\"\\nexperiment: \" + str(i) + \", target hz: \" + str(targ_hz) + \", obs hz: \" + str(obs_hz) +\n",
    "         \", split: \" + str(split))\n",
    "\n",
    "    \n",
    "    print(\"target idx: [\" + str(targ_idx_LB) + \", \" + str(targ_idx_UB) + \"]\")\n",
    "    print(\"target freq: [\" + str(max(f) - f[targ_idx_LB]) + \", \" + str(max(f) - f[targ_idx_UB]) + \"]\")\n",
    "    print(\"obs idx: \" + str(obs_idx))\n",
    "    print(\"obs freq: \" + str(obs_freq))\n",
    "    print(experiment_8_obj.A.shape[0] - np.array(experiment[\"prediction\"][\"interpolation\"]).shape[0])\n",
    "    print(experiment_8_obj.A.shape[0])\n",
    "    #print(experiment[\"resp_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_exp_weights(json_obj, llambda = None):\n",
    "    print(json_obj.keys())\n",
    "    esn_ = EchoStateNetwork(**json_obj[\"best arguments\"][\"exponential\"], plot = True)\n",
    "    esn_.obs_idx  = json_obj[\"obs_idx\"]\n",
    "    esn_.resp_idx = json_obj[\"resp_idx\"]\n",
    "    if llambda != None:\n",
    "        esn_.llambda = llambda\n",
    "    esn_.get_exp_weights()\n",
    "\n",
    "\n",
    "for i in experiment_lst:\n",
    "    show_exp_weights(i)  \n",
    "#show_exp_weights(experiment_2)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_exp_weights(i, llambda = 10**-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**-2 \n",
    "np.log(10**-4)/np.log(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_unif_exp(fp_unif, fp_exp):\n",
    "    exp_dat = load_data(fp_exp)\n",
    "    unif_dat = load_data(fp_unif)\n",
    "    assert exp_dat[\"prediction\"][\"interpolation\"] == unif_dat[\"prediction\"][\"interpolation\"], \"something is wrong!\"\n",
    "    joint_dat = unif_dat.copy()\n",
    "    for i in [\"prediction\", \"nrmse\", \"best arguments\"]:\n",
    "        exp_dict = {\"exponential\" : exp_dat[i][\"exponential\"]}\n",
    "        joint_dat[i] = Merge(joint_dat[i], exp_dict)\n",
    "    print(joint_dat[\"best arguments\"])\n",
    "        \n",
    "     \n",
    "    return(joint_dat)\n",
    "#0.5_1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "uniform_ = df_rel[df_rel.model == \"uniform\"]\n",
    "exp_ = df_rel[df_rel.model == \"exponential\"]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,6))\n",
    "sns.kdeplot(uniform_[\"target hz\"], uniform_[\"nrmse\"],\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, ax = ax[0])#, alpha = 0.5)\n",
    "sns.kdeplot(exp_[\"target hz\"], exp_[\"nrmse\"],\n",
    "                 cmap=\"Blues\", shade=True, shade_lowest=False, ax = ax[1])#, alpha = 0.5)\n",
    "ax[0].set_ylim(0,0.3)\n",
    "ax[1].set_ylim(0,0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTH\n",
    "#experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/2k/medium/split_0.5/targetKhz:_0.02__obskHz:_0.01.txt')\n",
    "#                         #bp = '/Users/hayden/Desktop/')\n",
    "#experiment_5_obj = get_experiment(experiment_5)\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "experiment_ = experiment_5.copy()\n",
    "for key, prediction in experiment_[\"prediction\"].items():\n",
    "    prediction = np.array(prediction)\n",
    "    Train, Test = recover_test_set(experiment_)\n",
    "    \n",
    "    experiment_[\"nrmse\"][key] = nrmse(prediction, Test)\n",
    "get_experiment(experiment_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTH\n",
    "experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/2k/medium/split_0.5/targetKhz:_0.02__obskHz:_0.01.txt')\n",
    "                         #bp = '/Users/hayden/Desktop/')\n",
    "experiment_ = experiment_5.copy()\n",
    "for key, prediction in experiment_[\"prediction\"].items():\n",
    "    prediction = np.array(prediction)\n",
    "    Train, Test = recover_test_set(experiment_)\n",
    "    \n",
    "    experiment_[\"nrmse\"][key] = nrmse(prediction, Test)\n",
    "experiment_5_obj = get_experiment(experiment_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd\n",
    "def Audio(transform):\n",
    "    obj = ipd.Audio(data = transform[\"signal\"], rate = transform[\"sr\"])\n",
    "    return(obj)\n",
    "\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "\n",
    "pth_high = \"/Users/hayden/Desktop/18th_century_high.m4a\"\n",
    "\n",
    "#pth = \"/Users/hayden/Desktop/19th_century_high.wav\"#\"/Users/hayden/Desktop/get_free.wav\"\n",
    "\n",
    "def partition_song(array, x_start, x_len,  y_start, y_stop):\n",
    "    print(array.shape)\n",
    "    x_stop = x_start + 3000\n",
    "    partitioned_array = array[y_start:y_stop, x_start:x_stop].copy()\n",
    "    print(partitioned_array.shape)\n",
    "    return(partitioned_array)\n",
    "\n",
    "def get_transform(trans_type,\n",
    "                  hop_length_mult = 1, pth = \"/Users/hayden/Desktop/18th_century_dense.m4a\",\n",
    "                  n_bins_mult = 2, bins_per_octave =12, sr = None, y_axis = \"hz\", filter_scale = 1,\n",
    "                  norm = 1, method = \"db\", label = None, partition = False, save_path = None\n",
    "                  ):\n",
    "    fmin   = librosa.note_to_hz('C0')\n",
    "    if trans_type == 'cqt':\n",
    "        \n",
    "        y_axis = \"cqt_note\"\n",
    "    \n",
    "    assert label, \"enter a label so that you can save a file\"\n",
    "    assert trans_type in [\"stft\", \"cqt\", \"hybrid_cqt\", \"pseudo_cqt\", \"vqt\"]\n",
    "    \n",
    "    x, sr = librosa.load(pth, sr=None)\n",
    "    #C = np.abs(librosa.cqt(x, sr=None))\n",
    "    \n",
    "    #print(\"transform_type: \" + trans_type)\n",
    "    # Librosa transform functions dictionary:\n",
    "    trans_dict = { \"stft\" : librosa.stft,\n",
    "                   \"cqt\" : librosa.cqt,\n",
    "                   \"hybrid_cqt\": librosa.hybrid_cqt,\n",
    "                   \"pseudo_cqt\": librosa.pseudo_cqt,\n",
    "                   \"vqt\" : librosa.vqt\n",
    "    }\n",
    "    #n_bins = int(84*n_bins_mult)\n",
    "    n_bins = n_bins_mult\n",
    "    default_args = {\n",
    "         \"stft\" : {\n",
    "              \"n_fft\" : 512\n",
    "              #\"sr\" : sr\n",
    "         },\n",
    "         \"cqt\" : {\n",
    "                  \"fmin\" : fmin,\n",
    "                  'pad_mode': 'wrap',\n",
    "                  #\"fmin\": FMIN, , \n",
    "                  \"sr\" : sr,\n",
    "                  #\"n_bins\": 84,\n",
    "                  #\"hop_length\" : 2**6 * hop_length_mult\n",
    "                  #\"n_bins\" : n_bins #\"norm\": norm,\n",
    "                  }, #, \"hop_length\" : 64**hop_length_exp \"fmin\"  #\"filter_scale\": filter_scale : 1 #\"res_type\": \"fft\", #\"sparsity\": 0.1\n",
    "         \"hybrid_cqt\": {},\n",
    "         \"pseudo_cqt\": {},\n",
    "         \"vqt\" : {}\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #x = librosa.resample(x, orig_sr = sr, target_sr = sr*5) #NOPE\n",
    "\n",
    "    N_FFT = len(x)\n",
    "    N_FFT_exp = 4\n",
    "    f = trans_dict[trans_type]\n",
    "    X = f(x, **default_args[trans_type]) #stft #, n_fft =int(N_FFT/np.exp(N_FFT_exp)\n",
    "\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    Xpow = np.log10( librosa.db_to_power(Xdb))\n",
    "    matrix2plot = Xpow if method == \"pow\" else Xdb\n",
    "    \n",
    "    if partition:\n",
    "        matrix2plot = partition_song(matrix2plot, partition, 15000, 0, 800)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(7, 5)) \n",
    "    im1 = librosa.display.specshow(matrix2plot, x_axis='time', y_axis=y_axis, ax = ax)\n",
    "    plt.colorbar(im1, format='%+2.0f dB')\n",
    "    \n",
    "  \n",
    "    #Librosa frequency functions \n",
    "    f_dict = { \"stft\": librosa.fft_frequencies,\n",
    "               \"cqt\" : librosa.cqt_frequencies,\n",
    "               \"hybrid_cqt\": None,\n",
    "               \"pseudo_cqt\": None,\n",
    "               \"vqt\" : None}\n",
    "    \n",
    "    \n",
    "    f_default_args_stft = {\"stft\": {\"sr\" : sr, \"n_fft\" : 512}}            \n",
    "    \n",
    "    try:\n",
    "        f_default_args_cqt = {\"cqt\" : {\"fmin\" : fmin, \"n_bins\" : Xpow.shape[0]}}\n",
    "        f_default_args     = Merge(f_default_args_stft, f_default_args_cqt)\n",
    "        \n",
    "    except NameError:   \n",
    "        f_default_args = f_default_args_stft\n",
    "        \n",
    "    freq_fun = f_dict[trans_type]\n",
    "    freqs    = freq_fun(**f_default_args[trans_type])\n",
    "    \n",
    "    #\"type\": \n",
    "    #consider putting all transform types in this position.\n",
    "    transform = {\"signal\": x,\n",
    "                 \"sr\" : sr,\n",
    "                 \"transform\": {\n",
    "                               \"Xdb\"  : Xdb,\n",
    "                               \"Xpow\" : Xpow,\n",
    "                               \"f\"  :  freqs\n",
    "                               }\n",
    "                }\n",
    "    \n",
    "    #assertion to avoid incorrect frequency length.\n",
    "    err_msg = str(len(freqs.tolist())) + \"   \" + str(Xpow.shape[0])\n",
    "    assert len(freqs) == Xpow.shape[0], err_msg\n",
    "    \n",
    "    if save_path:\n",
    "        save_path = save_path + \"_\" + trans_type\n",
    "        print(\"saving at: \" + str(save_path))\n",
    "        save_pickle(save_path, transform)\n",
    "    \n",
    "    audio = Audio(transform)\n",
    "    plt.title(label)\n",
    "    \n",
    "    print_msg =  \"\\x1b[31m\\\"\"+ 'pickle_load(' + save_path + ')' + \"\\\"\\x1b[0m\"\n",
    "    print(\" to load this transform type \" + print_msg)\n",
    "    \n",
    "    \n",
    "    #print(plt.get_xydata() )\n",
    "    #print(labels)\n",
    "    #if trans_type == \"cqt\":\n",
    "    #    print(\"C1 = \" + str(librosa.note_to_hz('C0')))\n",
    "    return(audio) #transform, \n",
    "\n",
    "\n",
    "def save_pickle(path, transform):\n",
    "    save_path = \"./pickle_files/spectrogram_files/\" + path +\".pickle\"\n",
    "    with open(save_path, 'wb') as handle:\n",
    "        pickle.dump(transform, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path):\n",
    "    path = \"./pickle_files/spectrogram_files/\" + path +\".pickle\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return(b)\n",
    "#self.librosa_outfile = librosa_outfile\n",
    "#self.spectogram_path = spectogram_path\n",
    "\n",
    "#'./pickle_files/cqt_low_pitch.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century male voice\",\n",
    "              pth = \"/Users/hayden/Desktop/DL_lab/wav_files/computer_male.mp3\",\n",
    "              save_path = \"19th_century_male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century female voice\", \n",
    "              pth = \"/Users/hayden/Desktop/DL_lab/wav_files/computer_female.mp3\", save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"19th century male cqt transform\", \n",
    "              pth = \"/Users/hayden/Desktop/DL_lab/wav_files/computer_male.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_f\", pth = \"/Users/hayden/Desktop/computer_female.mp3\", save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_f\", pth = \"/Users/hayden/Desktop/DL_lab/computer_female.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_high\",  pth = pth_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_free = \"/Users/hayden/Desktop/get_free.wav\"\n",
    "get_transform(\"cqt\", label = \"CQT: Get Free\",  pth = pth_free, partition = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_free = \"/Users/hayden/Desktop/get_free.wav\"\n",
    "#get_transform(\"stft\", label = \"FT: Get Free\",  pth = pth_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cqt = get_transform(\"cqt\", n_bins_mult = 150, y_axis = \"hz\", norm = 1, method = \"pow\" ) #filter_scale = 0.2\n",
    "NBINS = 100\n",
    "\n",
    "#plt.subplot(1,2,1)\n",
    "\n",
    "                              #hop_length_mult = 1)\n",
    "#plt.title(\"dense\")\n",
    "#save_pickle(\"18th_cqt_low\", cqt_low_pitch)\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "cqt_high_pitch  = get_transform(\"cqt\", \n",
    "                                pth = pth_high,  \n",
    "                                n_bins_mult = NBINS,\n",
    "                                label = \"18th_cqt_high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment log(power) and default db setting appear the same. consider custom functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "fourier = get_transform(\"stft\", n_bins_mult = 150, method = \"pow\") # y_axis = \"log\",)\n",
    "save_pickle(\"fourier_power_low\", fourier_db_low)\n",
    "\n",
    "pth_high = \"/Users/hayden/Desktop/18th_century_high.m4a\"\n",
    "plt.title(\"dense\")\n",
    "fourier = get_transform(\"stft\", n_bins_mult = 150, # y_axis = \"log\",\n",
    "                       pth = pth_high, method = \"pow\")\n",
    "plt.title(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "exper_ = get_experiment(experiment_lst[0], plot_split = False, compare_ = False)\n",
    "print(exper_.A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "librosa.display.specshow(exper_.A.T, y_axis='cqt_note', x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dB to Pascals: (Pa)\n",
    "\n",
    "https://www.translatorscafe.com/unit-converter/en-US/sound-pressure-level/2-9/pascal-sound%20pressure%20level%20in%20decibels/#:~:text=Sound%20pressure%20level%20Lp,20%20%CE%BCPa%20or%200.00002%20Pa)\n",
    "\n",
    "\"Sound pressure level (SPL) is a logarithmic (decibel) measure of the sound pressure relative to the reference value of 20 μPa threshold of hearing. The threshold of hearing is the quietest sound that most young healthy people can hear. Sound pressure level Lp is measured in decibels (dB) and is calculated as follows:\"\n",
    "\n",
    "$L_p = 20*log_{10} (p/p_0)$\n",
    "\n",
    "Thus:\n",
    "$(p/p_0) = 10^{L_p/20}$\n",
    "\n",
    "usually $p_0 = 20 \\mu $ Pa or 0.00002 Pa\n",
    "\n",
    "\"The sound pressure level is an absolute value because it is referenced to another absolute value — the threshold of hearing. Therefore, the sound pressure in linear values like pascals can be converted into the sound pressure level in decibels and vice versa if the reference sound pressure is known.\"\n",
    "\n",
    "So p = 0.00002 * 10^{L_p/20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dB to Intensity of sound: \n",
    "\n",
    "https://www.omnicalculator.com/physics/db#sound-intensity-level-sil\n",
    "\n",
    "Sound intensity is defined as the sound wave power per unit area. It is a special quantity that allows us to measure the energy of sound (or, to be more precise, the energy per second per one squared meter).\n",
    "\n",
    "SIL = $10*log_{10}\\left(\\frac{I}{I_{ref}}\\right)$\n",
    "\n",
    "where:\n",
    "SIL is the sound intensity level in dB;\n",
    "I is the sound intensity in watts per squared meter;\n",
    "Iref is the reference value if sound intensity. Typically, it is assumed to be equal to 1×10⁻¹² W/m².\n",
    "\n",
    "$$I = 10^{\\frac{SIL}{10}-12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The threshold of hearing:\n",
    "The threshold of hearing is generally reported as the RMS sound pressure of 20 micropascals, i.e. 0 dB SPL, corresponding to a sound intensity of 0.98 pW/m2 at 1 atmosphere and 25 °C.[3] It is approximately the quietest sound a young human with undamaged hearing can detect at 1,000 Hz.[4] The threshold of hearing is frequency-dependent and it has been shown that the ear's sensitivity is best at frequencies between 2 kHz and 5 kHz,[5] where the threshold reaches as low as −9 dB SPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = get_experiment(experiment_lst[0])\n",
    "dat = hi.A\n",
    "sns.distplot(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_parameters(array):\n",
    "    log_array    = np.log(array)\n",
    "    sample_mu, sample_sigma   = np.mean(log_array), np.std(log_array)\n",
    "    sample_variance = sample_sigma**2\n",
    "    estimated_mean = np.exp(sample_mu + 0.5*sample_variance)\n",
    "    estimated_variance = estimated_mean**2 * (np.exp(sample_variance) - 1) #np.exp\n",
    "    \n",
    "    est_params = {\"mean\" : estimated_mean,\n",
    "                  \"sd\" :   np.sqrt(estimated_variance)}\n",
    "    return(est_params)\n",
    "\n",
    "def dB2Pa(db_np, normalize = False, drop_silent = False, relative = True):\n",
    "    \"\"\"\n",
    "    converts a decibel level to pascals, for numpy arrays\n",
    "    \n",
    "    if normalize is set to true it normalizes the data assuming a log-normal distribution.\n",
    "    \n",
    "    if drop_silent is true, the function will flatten sounds not hearable by the human ear ( less that 20 * 10-6 pascals)\n",
    "    \"\"\"\n",
    "    p0 = 0.00002\n",
    "    pa_np = 10**(db_np/20)* p0 \n",
    "    \n",
    "    hearing_threshold = 20 * 10 **(-6)\n",
    "    faint_sounds = pa_np < hearing_threshold\n",
    "    \"\"\"\n",
    "    if normalize: #pa_np has a log-normal distribution: https://stats.stackexchange.com/questions/173715/calculate-variance-and-standard-deviation-for-log-normal-distribution\n",
    "        pa_np = np.log(pa_np) # it has a log-normal distribution roughly, so we transform, normalize, then transform back\n",
    "        pa_np = (pa_np - np.mean(pa_np))/np.std(pa_np)\n",
    "        pa_np = np.exp(pa_np)\n",
    "        \n",
    "     \n",
    "    \"\"\"\n",
    "    if normalize: #pa_np has a log-normal distribution: https://stats.stackexchange.com/questions/173715/calculate-variance-and-standard-deviation-for-log-normal-distribution\n",
    "        params = log_normal_parameters(pa_np)\n",
    "        print(params)\n",
    "        \n",
    "        mn, sig = params[\"mean\"], params[\"sd\"]\n",
    "        #hearing_threshold = (hearing_threshold - mn)/sig\n",
    "        pa_np = ((pa_np - mn)/sig)\n",
    "        #pa_np = pa_np  - np.min(pa_np)\n",
    "    \n",
    "    if relative:\n",
    "        pa_np = pa_np/ hearing_threshold\n",
    "    \n",
    "    #Flatten the sounds which the human ear cannot hear. rounding to 5 places preserved the lower bound of human hearing.\n",
    "    if drop_silent:\n",
    "        pa_np[faint_sounds] = np.round(pa_np[faint_sounds], 6)\n",
    "        #new lower bound to avoid 0 values:\n",
    "        pa_np_threshold = 0.000002 #20 * 10 ** (-7)\n",
    "        pa_np[pa_np < pa_np_threshold] = pa_np_threshold\n",
    "    return(pa_np)\n",
    "\n",
    "\n",
    "def dB2Intensity(db_np):\n",
    "    \"\"\"\n",
    "    converts a decibel level to pascals, for numpy arrays\n",
    "    *\n",
    "    \"\"\"\n",
    "    power = db_np/10 -12\n",
    "    Intensity = np.power(10, power)\n",
    "    return(Intensity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xpow =  dB2Pa(dat, normalize =  True)\n",
    "\n",
    "sns.distplot(Xpow)#Log- Normal!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xdb_normal = (Xdb - np.mean(Xdb))/np.std(Xdb)\n",
    "Xpow =  dB2Pa(Xdb, normalize = True, drop_silent = False)\n",
    "sns.distplot(np.log10(Xpow)) \n",
    "min_sound_human_hearing = 20 * 10**(-6)\n",
    "plt.xlabel(\"log(Pascals)\")\n",
    "plt.title(\"ke plot of Pascal values\")\n",
    "plt.ylabel(\"\")\n",
    "plt.axvline(x=np.log10(min_sound_human_hearing) , color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 micro Pascals\n",
    "The softest sound a normal human ear can detect has a pressure variation of 20 micro Pascals, abbreviated as µPa, which is 20 x 10-6 Pa (\"20 millionth of a Pascal\") and is called the Threshold of Hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_log_comparison(dataset, \n",
    "                          method = \"propto-dB\", \n",
    "                          propto = True, \n",
    "                          normalized = True,\n",
    "                          log = False,\n",
    "                          drop_silent = True):\n",
    "\n",
    "    # we are only proportional because we have normalized the data.\n",
    "    assert method in [\"propto-dB\", \"propto-SIL\", \"propto-Pa\"], \"choose decibels or sound energy\"\n",
    "    \n",
    "    if method == \"propto-SIL\":\n",
    "        plot_value_label = \"SIL \"\n",
    "        dataset = dB2Intensity(dataset)\n",
    "        if log:\n",
    "            dataset = np.log(dataset)/np.log(10)\n",
    "        ylab = 'SIL (Sound Intensity Level)'  #sound wave power per unit area')\n",
    "            #dataset = (dataset - np.mean(dataset))/np.std(dataset) <-- gets you  what you started with\n",
    "    elif method == \"propto-Pa\":\n",
    "        \n",
    "        plot_value_label = \" (Pa)\" #sound pressure\n",
    "        ylab = \"Pascal\"\n",
    "        \n",
    "        dataset = dB2Pa(dataset, normalize = normalized, drop_silent = False)\n",
    "        #dataset = np.abs(dataset)\n",
    "        #dataset = np.log(dataset) #/np.std(dataset)\n",
    "        #dataset = (dataset - np.mean(dataset))/ np.std(dataset)\n",
    "        \n",
    "    else:\n",
    "        plot_value_label = \" (dB)\" # decibels\n",
    "        ylab = \"Hz\"\n",
    "        #lower_db_limit = -20\n",
    "        \n",
    "        if normalized:\n",
    "            dataset = (dataset - np.mean(dataset))/np.std(dataset)\n",
    "            #lower_db_limit = (lower_db_limit - np.mean(dataset))/np.std(dataset) \n",
    "        #if drop_silent:\n",
    "        #    dataset[dataset < lower_db_limit] = lower_db_limit\n",
    "            \n",
    "    \n",
    "    plot_title = \" spectrogram of '18th century'\" + plot_value_label\n",
    "    \n",
    "    plt.figure(figsize = (12,8))\n",
    "    \n",
    "    # Linear spectogram Plot\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    librosa.display.specshow(dataset, y_axis='linear', x_axis='time')\n",
    "    \n",
    "    #display(quadmesh_.get_axes()) # get the first line, there might be more\n",
    "\n",
    "    #print(ax1.get_axes())#.get_xdata())\n",
    "    #print(plt.get_xdata())\n",
    "    plt.title('Linear ' + plot_title)\n",
    "    add_experiment_regions(ax1)\n",
    "    plt.ylabel(\"Hz\")\n",
    "    \n",
    "    #legend\n",
    "    legend_elements = [Patch(facecolor='pink', edgecolor='red',     label='3500 to 4500 Hz'),\n",
    "                       Patch(facecolor='lightblue', edgecolor='blue',   label='1500 to 2500 Hz'),\n",
    "                       Patch(facecolor='palegreen', edgecolor='green', label='250 to 12250 Hz')]\n",
    "    plt.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "    # Log spectogram Plot\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    librosa.display.specshow(dataset, y_axis='log', x_axis='time')\n",
    "    plt.title('Log ' + plot_title)\n",
    "    add_experiment_regions(ax2)\n",
    "    plt.ylabel(\"Hz\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if method == \"propto-dB\":\n",
    "        propto_str = '%+2.0f' + str(r'$\\propto$') if propto else '%+2.0f'\n",
    "        colorbar_label = propto_str +\" dB\" #+\n",
    "    elif method == \"propto-SIL\":\n",
    "        colorbar_label = \"%+2.0f e-12 SIL\"  # + propto_str + \n",
    "    elif method == \"propto-Pa\":\n",
    "        colorbar_label = \"%.1e Pa\"  # propto_str +\n",
    "\n",
    "    plt.colorbar(format= colorbar_label)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    sns.distplot(dataset)\n",
    "    return(dataset)\n",
    "\n",
    "def add_experiment_regions(ax, plot = True):\n",
    "    def fill_region(lb, ub, color_):\n",
    "        ax.axhline(y=lb, color = color_, linestyle='-')\n",
    "        ax.axhline(y=ub, color = color_, linestyle='-')\n",
    "        x  = np.arange(0.0, 27, 0.1)\n",
    "        y1 = lb + 0 * x\n",
    "        y2 = ub + 0 * x\n",
    "        ax.fill_between(x, y2, y1, alpha = 0.2, color = color_)\n",
    "    trial = 2\n",
    "    if trial == 1:\n",
    "        lb_targ, ub_targ, obs_hz  = 210, 560, 320 / 2\n",
    "\n",
    "    elif trial == 2:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    if plot:\n",
    "        fill_region(lb_targ-obs_hz, lb_targ, \"b\")  #150 hz\n",
    "        fill_region(lb_targ, ub_targ, \"g\") #250 hz\n",
    "        fill_region(ub_targ, ub_targ + obs_hz, \"b\") #150 hz\n",
    "    else:\n",
    "        obs_list = list(range(lb_targ-obs_hz, lb_targ, 10))\n",
    "        obs_list += list(range(ub_targ, ub_targ + obs_hz, 10))\n",
    "        resp_list = list(range(lb_targ, ub_targ, 10))\n",
    "        obs_resp = {\"target\": resp_list, \"obs\": obs_list}\n",
    "        return obs_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decibel spectogram (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exper_ = experiment_lst[0]\n",
    "exper_obj = get_experiment(exper_)\n",
    "dat = exper_obj.A.T\n",
    "#hi = (hi - np.mean(hi))/np.std(hi)\n",
    "dataset_Pow = linear_log_comparison(dat, \n",
    "                      propto = False, \n",
    "                      normalized = False,\n",
    "                      drop_silent = False,\n",
    "                      method = \"propto-Pa\") \n",
    "\n",
    "dataset_db = linear_log_comparison(dat, \n",
    "                                    propto = False, \n",
    "                                    normalized = False,\n",
    "                                    drop_silent = True,\n",
    "                                    method = \"propto-dB\") \n",
    "\n",
    "custom_transform = {\"transform\": {\n",
    "                        \"Xdb\"  : dataset_db,\n",
    "                        \"Xpow\" : dataset_Pow,\n",
    "                        \"f\"  :   exper_obj.f}\n",
    "                   }\n",
    "save_pickle(\"custom\",custom_transform)\n",
    "#load_pickle(\"custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE THIS NORMALIZED Pa DATASET!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decibel spectogram, unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, \n",
    "                      propto = False, \n",
    "                      normalized = False,\n",
    "                      drop_silent = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal spectogram, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, method = \"propto-Pa\", normalized = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal spectogram, unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, method = \"propto-Pa\", normalized = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the average sound pressure per log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dB2Pa(exper_.A_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(inputt = Xdb, method = \"propto-Pa\", drop_silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(exper_.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_spectogram(dataset = exper_.A, f_arr = exper_.f):\n",
    "    \n",
    "    T = exper_.T\n",
    "    plt.imshow(dataset)\n",
    "    \n",
    "    f = np.array(f_arr)[1:] # humans hearing ranges from 20 db to 20k db so lets drop 0 to avoid -infty.\n",
    "    dataset = dataset[:,1:]\n",
    "    \n",
    "    f = np.log(f)/np.log(2) # humans experience sound logarithmically\n",
    "    \n",
    "    sns.distplot(dataset)\n",
    "    plt.show()\n",
    "    n_timesteps, n_frequencies  = dataset.shape\n",
    "\n",
    "    for i, time_step in enumerate(range(n_timesteps)):\n",
    "        if not i:\n",
    "            dictt_lst = []\n",
    "        this_timestep = T[time_step][0]\n",
    "        \n",
    "        #assert len(f) == len(this_timestep), \"error: \" + str(len(f)) + \" != \" + str(len(this_timestep))\n",
    "        for i, frequency_spec in enumerate(f):              \n",
    "            dictt_lst += [{\"frequency\" : frequency_spec , \n",
    "                           \"time\"      : this_timestep,\n",
    "                           \"amplitude\" : dataset[time_step, i]\n",
    "                            }]\n",
    "        #display(pd.DataFrame(dictt_lst))\n",
    "    \n",
    "    log_frequency_df = pd.DataFrame(dictt_lst)\n",
    "    log_frequency_df = log_frequency_df.pivot(\"frequency\", \"time\", \"amplitude\")\n",
    "    \n",
    "    sns.heatmap(log_frequency_df)\n",
    "    \n",
    "create_log_spectogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(Xdb, y_axis='log', x_axis='time')\n",
    "plt.title('log Power spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Xdb, aspect = 10)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_ = f_[1], f_[15] #_ denotes temporary variable, for testing or within a function.\n",
    "\n",
    "lb_, ub_ = bounds_\n",
    "\n",
    "def retrieve_freqs_btwn(bounds, f_):\n",
    "    f = np.array(f_)\n",
    "    lb, ub = bounds\n",
    "    display(bounds_)\n",
    "    lb_bool_vec, ub_bool_vec = (f > lb_), (f < ub_)\n",
    "    and_vector = ub_bool_vec* lb_bool_vec\n",
    "\n",
    "    freqs = f[and_vector]            #frequencies between bounds\n",
    "    freq_idxs = np.where(and_vector)[0] #indices between bounds\n",
    "\n",
    "    return(freq_idxs.tolist())\n",
    "    \n",
    "retrieve_freqs_btwn(bounds_, f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def log_amplitude(exper_):\n",
    "    A = np.array(exper_.A)\n",
    "    print(np.min(A))\n",
    "    print(np.max(A))\n",
    "    orig_shape = A.shape\n",
    "    \n",
    "    signs = A.copy().reshape(-1,) < 0\n",
    "    signs = signs * 2 - 1\n",
    "    print(np.unique(signs))\n",
    "    signs = signs.reshape(orig_shape)\n",
    "    #plt.imshow(signs)\n",
    "    A_new = np.log(np.abs(A)) * signs\n",
    "    \n",
    "    #A_new = (A_new - np.mean(A_new))/ np.std(A_new)\n",
    "    #print(np.min(A_new))\n",
    "    #print(np.max(A_new))\n",
    "    #plt.imshow(A_new)\n",
    "    return(A_new)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle('./pickle_files/results/18th_cqt_high/db/untouched/split_0.5/tf_250__obsHz_0.1__targHz_0.02.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle(\"custom\")\n",
    "\n",
    "sns.heatmap(hi[\"Xpow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle(\"18th_cqt_high\")\n",
    "print(hi[\"transform\"][\"Xdb\"])\n",
    "sns.heatmap(hi[\"transform\"][\"Xdb\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "def get_frequencies(trial = 1):\n",
    "    \"\"\"\n",
    "    get frequency lists\n",
    "    \"\"\"\n",
    "    if trial == 1:\n",
    "        lb_targ, ub_targ, obs_hz  = 210, 560, int(320 / 2)\n",
    "\n",
    "    elif trial == 2:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    elif trial == 3:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 350, 20\n",
    "\n",
    "\n",
    "    obs_list = list(range(lb_targ-obs_hz, lb_targ, 10))\n",
    "    obs_list += list(range(ub_targ, ub_targ + obs_hz, 10))\n",
    "    resp_list = list(range(lb_targ, ub_targ, 10))\n",
    "    return obs_list, resp_list\n",
    "\n",
    "obs_freqs, resp_freqs = get_frequencies(1)\n",
    "librosa_args = { \"spectrogram_path\": \"custom\",#\"cqt_high_pitch\",\n",
    "                         \"librosa\": True}\n",
    "#inputs = {'obs_freq_lst' :, \"targ_freq_lst\": , \"split\": 0.5}\n",
    "                       \n",
    "additional_Echo_inputs = {\n",
    "            \"obs_freq_lst\":  obs_freqs,\n",
    "            \"targ_freq_lst\" : resp_freqs\n",
    "            }\n",
    "Echo_inputs = {\n",
    "        \"size\" : \"medium\",\n",
    "        \"verbose\" : False,\n",
    "        \"prediction_type\" : \"block\"}\n",
    "Echo_inputs = Merge(Echo_inputs, additional_Echo_inputs)\n",
    "experiment = EchoStateExperiment( **Echo_inputs, **librosa_args)\n",
    "experiment.get_observers(method = \"exact\", split = 0.5, aspect = 0.9, plot_split = False)\n",
    "experiment.obs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree pickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = load_pickle(\"./pickle_files/spectrogram_files/18th_cqt_high.pickle\")\n",
    "g_truth = dat[\"transform\"][\"Xdb\"]\n",
    "g_truth = (g_truth - np.mean(g_truth))/np.std(g_truth)\n",
    "line = g_truth[35][513:]\n",
    "fig, ax = plt.subplots(1,1,figsize = (10,4))\n",
    "sns.lineplot(x = range(len(bye)), y = bye, label = \"unif\")\n",
    "sns.lineplot(x = range(len(ip)), y = ip, label = \"ip\")\n",
    "sns.lineplot(x = range(len(line)), y =line, label = \"gtruth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_shape_0 = 1000\n",
    "def get_obs_eq(k):\n",
    "    hi = A_shape_0//k\n",
    "    viable_start = np.random.randint(hi)\n",
    "    observers = [k*i + viable_start for i, idx in  enumerate(range(viable_start, A_shape_0, k))]\n",
    "    print(observers)\n",
    "        \n",
    "get_obs_eq(25)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Zhizhuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(test1.ip_res)\n",
    "#https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "def pure_prediction_ip_generator(missing_data, end_idx):\n",
    "    test_idx = list(range(end_idx))[-missing_data:] #553, 712, 942\n",
    "    print(test_idx)\n",
    "    train_range_input = end_idx - missing_data\n",
    "    train_idx = list(range(train_range_input))\n",
    "    print(train_range_input)\n",
    "\n",
    "    experiment_inputs1 =  {'size': 'medium', \n",
    "                           'target_frequency': None, \n",
    "                           'verbose': False, \n",
    "                           'prediction_type': 'column', \n",
    "                           \"interpolation_method\" : \"griddata-nearest\",\n",
    "                           'train_time_idx': train_idx,\n",
    "                           'test_time_idx' : test_idx}#[514, 515, 516, 517, 518, 519, 520, 521, 522, 523]}#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], 'test_time_idx': [250, 251, 252, 253, 254, 255, 256, 257, 258, 259]}\n",
    "\n",
    "    test1 = EchoStateExperiment(**experiment_inputs1)\n",
    "    obs_inputs1 =  {'split': 0.5, 'aspect': 0.9, 'plot_split': False, 'method': 'exact'}\n",
    "    test1.get_observers(**obs_inputs1)\n",
    "\n",
    "\n",
    "    import math\n",
    "    def f(x):\n",
    "        \"\"\"\n",
    "        check if x is nan\n",
    "        x = float('nan')\n",
    "        math.isnan(x)\n",
    "        \"\"\"\n",
    "        return math.isnan(x)\n",
    "    def array_map(x, f):\n",
    "        x_shape= x.shape\n",
    "        print(\"x_shape\" + str(x.shape))\n",
    "        x  = x.flatten().tolist()\n",
    "        hi = np.array(list(map(f,x)))\n",
    "        print(hi)\n",
    "\n",
    "        return np.array(hi).reshape(x_shape)\n",
    "\n",
    "    test1_ip_pred = test1.ip_res[\"prediction\"]\n",
    "\n",
    "    plt.imshow(array_map(test1_ip_pred, f))\n",
    "    test1_ip_pred\n",
    "\n",
    "    my_dict = {\n",
    "        \"interpolation_prediction\": test1.ip_res[\"prediction\"],\n",
    "        \"ground_truth_test\"  : test1.xTe,\n",
    "        \"ground_truth_train\" : test1.xTr,\n",
    "        \"interpolation_MSE\"  : test1.ip_res[\"nrmse\"]\n",
    "    }\n",
    "\n",
    "    from scipy.io import savemat\n",
    "\n",
    "    save_path = \"zhizhuo/testindex_\" + str(test_idx[0]) + \"_\" + str(test_idx[-1]) +\".mat\"\n",
    "\n",
    "    print(save_path)\n",
    "    savemat(save_path, my_dict) #\"zhizhuo/testindex_514_523.mat\"\n",
    "    plt.imshow(test1.ip_res[\"prediction\"], aspect = 0.1)\n",
    "    return(test1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = [{\"missing_data\" : 40, \"end_idx\" : 289},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 553},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 712},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 942}]\n",
    "test_results = []\n",
    "for prediction in test_lst:\n",
    "    pred_ = pure_prediction_ip_generator(**prediction)\n",
    "    test_results.append(pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rez = [ test_result.ip_res[\"prediction\"] for test_result in test_results]\n",
    "test_ground = [ test_result.xTe for test_result in test_results]\n",
    "count = 1\n",
    "for i, rez in enumerate(test_rez):\n",
    "    plt.imshow(rez, aspect = 10)\n",
    "    plt.title(\"prediction\" + str(count))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.imshow(test_ground[i], aspect = 10)\n",
    "    plt.title(\"truth\" + str(count))\n",
    "    plt.show()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_pickle('19th_century_male_stft')\n",
    "plt.imshow(X['transform']['Xdb'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century male voice\", pth = \"/Users/hayden/Desktop/computer_male.mp3\", save_path = \"19th_century_male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when it comes time to run a lot of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "def quick_write_path(freq, split, targHz, obsHz, size = \"/medium\"):\n",
    "    if freq == 2000:\n",
    "        freqStr = \"2k\"\n",
    "    elif freq == 4000:\n",
    "        freqStr = \"4k\"\n",
    "    splitStr = \"/split_\" + str(split)\n",
    "    targHz, obsHz = str(targHz/1000) , str(obsHz/1000)\n",
    "    HzStr = \"/targetKhz:_\" + targHz + \"__obskHz:_\" +  obsHz \n",
    "    newPath = freqStr + size + splitStr + HzStr +\".txt\"\n",
    "    return([newPath])\n",
    "\n",
    "def quick_write_dict(freq, split, targHz, obsHz):\n",
    "    dict_tmp = {'target_freq': freq, 'split': split, 'obs_hz': obsHz, 'target_hz': targHz}\n",
    "    return([dict_tmp])\n",
    "\n",
    "\n",
    "path_lst = []\n",
    "dict_lst = []\n",
    "for targ_freq in [2000, 4000]:\n",
    "    for split in [0.5, 0.9]:\n",
    "        for targ in list(range(500, 2001, 250)):\n",
    "            for obs in list(range(500, 2001, 250)):\n",
    "                path_lst += quick_write_path(freq = targ_freq, split = split, targHz = targ, obsHz = obs)\n",
    "                dict_lst += quick_write_dict(freq = targ_freq, split = split, targHz = targ, obsHz = obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out low frequency results\n",
    "exper_lst = []\n",
    "bp_ = \"/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/pickle_files/results/custom/power/untouched/\"\n",
    "\n",
    "new_exper_path_lsts = [\n",
    "    \"split_0.5/tf_485.0__obsNIdx_56__targNIdx_30.pickle\",\n",
    "    \"split_0.9/tf_380.0__obsNIdx_32__targNIdx_35.pickle\"\n",
    "    #tf_485.0__obsNIdx_56__targNIdx_30.pickle\n",
    "]\n",
    "\n",
    "\n",
    "for i in new_exper_path_lsts:\n",
    "    exper_ = load_p_result(i, bp = bp_)\n",
    "    exper_lst += [exper_]\n",
    "    \n",
    "xpow = load_pickle(\"custom\")[\"transform\"][\"Xpow\"]\n",
    "\n",
    "this_experiment = exper_lst[0]\n",
    "resp_idx_ = this_experiment[\"resp_idx\"]\n",
    "print(resp_idx_)\n",
    "resp_ = xpow[resp_idx_]\n",
    "sns.heatmap(resp_)\n",
    "plt.show()\n",
    "sns.heatmap(resp_[:,512:])\n",
    "plt.show()\n",
    "sns.heatmap(np.array(exper_lst[0][\"prediction\"][\"exponential\"]).T)\n",
    "plt.show()\n",
    "sns.heatmap(np.array(exper_lst[0][\"prediction\"][\"interpolation\"]).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
